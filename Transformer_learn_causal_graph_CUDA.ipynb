{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from text file\n",
    "Status = np.loadtxt(\"Status_2024_05_06.txt\", dtype=int)  # Specify dtype if known, for efficiency\n",
    "Target = np.loadtxt(\"Target_2024_05_06.txt\", dtype=float).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=Status.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train=Status[:int(N/2),-1]\n",
    "# Y_test=Status[int(N/2):int(0.75*N),-1]\n",
    "# Y_val=Status[int(0.75*N):,-1]\n",
    "# print(Y_train.shape,Y_test.shape,Y_val.shape)\n",
    "\n",
    "# Target_train=torch.tensor(Target[:int(N/2),:]).float()\n",
    "# Target_test=torch.tensor(Target[int(N/2):int(0.75*N),:]).float()\n",
    "# Target_val=torch.tensor(Target[int(0.75*N):,:]).float()\n",
    "# print(Target_train.shape,Target_test.shape,Target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,) (250,) (99250,)\n",
      "torch.Size([1000, 10]) torch.Size([1000, 10]) torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "# Y_train=Status[:int(N/2),-1]\n",
    "# Y_test=Status[int(N/2):int(0.75*N),-1]\n",
    "# Y_val=Status[int(0.75*N):,-1]\n",
    "# print(Y_train.shape,Y_test.shape,Y_val.shape)\n",
    "\n",
    "\n",
    "Target_train=torch.tensor(Target[:1000,:]).float()\n",
    "Target_test=torch.tensor(Target[1000:2000,:]).float()\n",
    "Target_val=torch.tensor(Target[2000:3000,:]).float()\n",
    "print(Target_train.shape,Target_test.shape,Target_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "1. Each row of above X matrice is the vector of status of node 1 to node T\n",
    "2. Each row of above Y is the status of node T+1\n",
    "\n",
    "Then we need to construct word embedding and positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tilde{X}^T=\\begin{bmatrix}\n",
    "    e_{s_1}& e_{s_2}&\\cdots&e_{s_T}\\\\\n",
    "    e_1&e_2&\\cdots&e_T\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "    \\mathcal{E}_{s_{1:T}}\\\\\n",
    "    \\mathbb{I}_{T}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Generate $\\mathbb{I}_{T}$ and $\\mathcal{E}_{s_{1:T}}$\n",
    "\n",
    "The input of Transformer should be $\\tilde{X}^T$ and the output is $\\pi(s'|s_T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S=10\n",
    "# T=20\n",
    "\n",
    "# #number of random sequence\n",
    "\n",
    "\n",
    "# I_S=np.identity(S)\n",
    "# I_t=np.identity(T)\n",
    "\n",
    "# #(iter,S+T,T)\n",
    "# X_tilde_T=np.empty((N,S+T,T))\n",
    "\n",
    "\n",
    "# for i in range(N):\n",
    "\n",
    "#     epsilon_1_T=np.zeros((S,T))\n",
    "\n",
    "#     #print(T)\n",
    "\n",
    "#     for k in range(T):\n",
    "#         epsilon_1_T[:,int(k)]=I_S[:,int(Status[i,int(k)])]\n",
    "\n",
    "#     #print(Status[0])\n",
    "\n",
    "#     X_tilde_T[i]=np.concatenate((epsilon_1_T,I_t),axis=0)\n",
    "\n",
    "\n",
    "# X_tilde=X_tilde_T.transpose(0,2,1)\n",
    "\n",
    "\n",
    "\n",
    "# X_train_tilde=torch.tensor(X_tilde[:int(N/2)])\n",
    "# X_test_tilde=torch.tensor(X_tilde[int(N/2):int(0.75*N)])\n",
    "# X_val_tilde=torch.tensor(X_tilde[int(0.75*N):])\n",
    "\n",
    "# X_train_tilde.to(device)\n",
    "# X_test_tilde.to(device)\n",
    "# X_val_tilde.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20, 30])\n",
      "torch.Size([0, 20, 30])\n"
     ]
    }
   ],
   "source": [
    "S=10\n",
    "T=20\n",
    "\n",
    "#number of random sequence\n",
    "\n",
    "\n",
    "I_S=np.identity(S)\n",
    "I_t=np.identity(T)\n",
    "\n",
    "#(iter,S+T,T)\n",
    "X_tilde_T=np.empty((N,S+T,T))\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    epsilon_1_T=np.zeros((S,T))\n",
    "\n",
    "    #print(T)\n",
    "\n",
    "    for k in range(T):\n",
    "        epsilon_1_T[:,int(k)]=I_S[:,int(Status[i,int(k)])]\n",
    "\n",
    "    #print(Status[0])\n",
    "\n",
    "    X_tilde_T[i]=np.concatenate((epsilon_1_T,I_t),axis=0)\n",
    "\n",
    "\n",
    "X_tilde=X_tilde_T.transpose(0,2,1)\n",
    "\n",
    "\n",
    "\n",
    "X_train_tilde=torch.tensor(X_tilde[:1000])\n",
    "X_test_tilde=torch.tensor(X_tilde[1000:2000])\n",
    "X_val_tilde=torch.tensor(X_tilde[2000:3000])\n",
    "\n",
    "X_train_tilde.to(device)\n",
    "X_test_tilde.to(device)\n",
    "X_val_tilde.to(device)\n",
    "\n",
    "print(X_train_tilde.shape)\n",
    "print(X_test_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_547783/3710488808.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset=TensorDataset(torch.tensor(X_train_tilde).float(),torch.tensor(Target_train).float())\n"
     ]
    }
   ],
   "source": [
    "batch_size=1000\n",
    "\n",
    "train_dataset=TensorDataset(torch.tensor(X_train_tilde).float(),torch.tensor(Target_train).float())\n",
    "train_data=DataLoader(train_dataset,batch_size=batch_size)\n",
    "\n",
    "# test_dataset=TensorDataset(torch.tensor(X_test_tilde).float(),torch.tensor(Target_test).float())\n",
    "# test_data=DataLoader(test_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, a_label = next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20, 30])\n",
      "torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "# 打印出一个批次的数据大小\n",
    "print(a.shape)\n",
    "print(a_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f70bc3bcd90>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S=10\n",
    "T=20\n",
    "\n",
    "import torch\n",
    "\n",
    "# Example of defining a matrix\n",
    "total_matrix_A_1 = torch.randn(int(S+T), int(S+T), requires_grad=True)\n",
    "total_matrix_A_2 = torch.randn(int(2*S+2*T), int(2*S+2*T), requires_grad=True)\n",
    "W_tilde = torch.randn(int(S), int(4*S+4*T), requires_grad=True)\n",
    "\n",
    "\n",
    "# Define a mask where True means trainable\n",
    "mask_A_1 = torch.tensor([[True if (i in range(S,int(S+T))  and  j in range(int(S),int(S+T))) else False for j in range(int(S+T))] for i in range(int(S+T))])\n",
    "mask_A_2 = torch.tensor([[True if (i in range(0,S)  and  j in range(int(S+T),int(2*S+T))) else False for j in range(int(2*S+2*T))] for i in range(int(2*S+2*T))])\n",
    "mask_W = torch.tensor([[True if (j in range(int(2*S+2*T),int(3*S+2*T))) else False for j in range(int(4*S+4*T))]])\n",
    "mask_W_T = torch.tensor([[True if (j in range(int(2*S+2*T),int(3*S+2*T))) else False for j in range(int(4*S+4*T))]]).T\n",
    "\n",
    "# Hook to apply the mask to the gradients\n",
    "def apply_mask_1(grad):\n",
    "    return grad * mask_A_1.type_as(grad)\n",
    "\n",
    "def apply_mask_2(grad):\n",
    "    return grad * mask_A_2.type_as(grad)\n",
    "\n",
    "def apply_mask_W(grad):\n",
    "    return grad * mask_W.type_as(grad)\n",
    "\n",
    "def apply_mask_W_T(grad):\n",
    "    return grad * mask_W_T.type_as(grad)\n",
    "\n",
    "# Attach the hook\n",
    "total_matrix_A_1.register_hook(apply_mask_1)\n",
    "\n",
    "total_matrix_A_2.register_hook(apply_mask_2)\n",
    "\n",
    "W_tilde.register_hook(apply_mask_W)\n",
    "\n",
    "# output_3 = W_tilde.sum()\n",
    "# output_3.backward()\n",
    "# print(W_tilde.grad[:,int(2*S+2*T-1):int(3*S+2*T+1)])\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage in a dummy forward pass\n",
    "# output_2 = total_matrix_A_2.sum()\n",
    "# output_2.backward()\n",
    "\n",
    "\n",
    "\n",
    "# # Set print options\n",
    "# torch.set_printoptions(threshold=10_000)\n",
    "# # Check gradients\n",
    "# print(total_matrix_A_2.grad)\n",
    "\n",
    "# output_1 = total_matrix_A_1.sum()\n",
    "# output_1.backward()\n",
    "# print(total_matrix_A_1.grad[10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a mask where True means trainable\n",
    "# grad_mask_A_1 = torch.tensor([[True if (i in range(S,int(S+T))  and  j in range(int(S),int(S+T))) else False for j in range(int(S+T))] for i in range(int(S+T))])\n",
    "# grad_mask_A_2 = torch.tensor([[True if (i in range(0,S)  and  j in range(int(S+T),int(2*S+T))) else False for j in range(int(2*S+2*T))] for i in range(int(2*S+2*T))])\n",
    "\n",
    "# # Hook to apply the mask to the gradients\n",
    "# def apply_mask_1(grad):\n",
    "#     return grad * grad_mask_A_1.type_as(grad)\n",
    "\n",
    "# def apply_mask_2(grad):\n",
    "#     return grad * grad_mask_A_2.type_as(grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\tilde A^{(1)}&=\\begin{bmatrix}\n",
    "            0_{S\\times S} & 0_{S\\times T}\\\\\n",
    "            0_{T\\times S} & A^{(1)}_1\n",
    "        \\end{bmatrix}\\\\\n",
    "        % \\tilde A_2^{(1)}&=\\begin{bmatrix}\n",
    "        %     0_{S\\times S} & 0_{S\\times T}\\\\\n",
    "        %     0_{T\\times S} & A^{(1)}_2\n",
    "        % \\end{bmatrix}\\\\\n",
    "        \\tilde A^{(2)}&=\\begin{bmatrix}\n",
    "            0_{S\\times S} & 0_{S\\times T} & A^{(2)} & 0_{S\\times T}\\\\\n",
    "            0_{T\\times S} & 0_{T\\times T} & 0_{T\\times S} & 0_{T\\times T}\\\\\n",
    "            0_{S\\times S} & 0_{S\\times T} & 0_{S\\times S} & 0_{S\\times T}\\\\\n",
    "            0_{T\\times S} & 0_{T\\times T} & 0_{T\\times S} & 0_{T\\times T}\\\\\n",
    "        \\end{bmatrix}\\\\\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "attn(h;A)=\\mathcal{S}(MASK(hAh^T))h\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\underset{T\\times (S+T)}{h^{0}}=\\tilde{X}=\\begin{bmatrix}\n",
    "    e_{s_1}& e_{s_2}&\\cdots&e_{s_T}\\\\\n",
    "    e_1&e_2&\\cdots&e_T\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\underset{T\\times (1+m_\\ell)(S+T)}{h^{(\\ell)}}=\\bigl[h^{(\\ell-1)},attn(h^{(\\ell-1)};\\widetilde{A}^{(\\ell)}_1),\\cdots,attn(h^{(\\ell-1)};\\widetilde{A}^{(\\ell)}_1)\\bigr]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widetilde{TF}_{\\tilde\\theta}(S_{1:T})=h^{(L)}\\widetilde {W}_O^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "dim(\\widetilde {W}_O)=S\\times (1+m_2)(S+T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_matrix_A_1 = torch.zeros(int(S+T), int(S+T), requires_grad=True)\n",
    "# total_matrix_A_1.register_hook(apply_mask_1)\n",
    "\n",
    "# total_matrix_A_1=total_matrix_A_1.float()\n",
    "\n",
    "# total_matrix_A_2 = torch.zeros(int(2*S+2*T), int(2*S+2*T), requires_grad=True)\n",
    "# total_matrix_A_2.register_hook(apply_mask_2)\n",
    "\n",
    "# total_matrix_A_2=total_matrix_A_2.float()\n",
    "\n",
    "# W_tilde = torch.zeros(int(S), int(4*S+4*T), requires_grad=True)\n",
    "# W_tilde.register_hook(apply_mask_W)\n",
    "\n",
    "# W_tilde=W_tilde.float()\n",
    "# W_tilde_T=W_tilde.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_layer_1=torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "class Attention_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_1, self).__init__()\n",
    "\n",
    "        #Random initialize the matrix A_1\n",
    "        # self.total_matrix_A_1 = torch.zeros(int(S+T), int(S+T), requires_grad=True)\n",
    "        # self.total_matrix_A_1 = nn.Linear(int(S+T),int(S+T),bias=False)\n",
    "        # self.total_matrix_A_1 = nn.Parameter(torch.zeros(int(S+T), int(S+T)))\n",
    "\n",
    "        self.trainable_part = nn.Parameter(torch.zeros(T, T))\n",
    "        self.non_trainable_part_1 = torch.zeros(S, int(S+T), requires_grad=False).cuda()\n",
    "        self.non_trainable_part_2 = torch.zeros(T, S, requires_grad=False).cuda()\n",
    "#         self.total_matrix_A_1=nn.Parameter(torch.zeros(T+S,T+S))\n",
    "\n",
    "    def forward(self, h_0,attn_mask):\n",
    "        # A_1=self.total_matrix_A_1.register_hook(apply_mask_1)\n",
    "        # self.total_matrix_A_1.register_hook(apply_mask_1)\n",
    "\n",
    "        total_matrix_A_1=torch.cat((self.non_trainable_part_2,self.trainable_part),dim=1)\n",
    "        total_matrix_A_1=torch.cat((self.non_trainable_part_1,total_matrix_A_1),dim=0)\n",
    "        \n",
    "#         total_matrix_A_1=self.total_matrix_A_1\n",
    "        \n",
    "        h_0=h_0.float()\n",
    "\n",
    "        total_matrix_A_1=total_matrix_A_1.unsqueeze(0)\n",
    "        total_matrix_A_1=total_matrix_A_1.repeat(h_0.shape[0],1,1)\n",
    "\n",
    "        # print(total_matrix_A_1.shape, h_0.shape)\n",
    "\n",
    "        scores = torch.bmm(h_0, total_matrix_A_1) \n",
    "\n",
    "        scores = torch.bmm(scores,h_0.transpose(1,2))\n",
    "        # print(scores.shape)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(0)\n",
    "        attn_mask = attn_mask.repeat(scores.shape[0],1,1)\n",
    "\n",
    "        \n",
    "\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        #row-wise softmax\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.bmm(attn, h_0)\n",
    "        # print('layer 1 output shape: ',context.shape)\n",
    "\n",
    "        return context #, attn\n",
    "\n",
    "\n",
    "\n",
    "class Attention_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention_2, self).__init__()\n",
    "\n",
    "        #Random initialize the matrix A_2\n",
    "        # self.total_matrix_A_2 = torch.zeros(int(2*S+2*T), int(2*S+2*T), requires_grad=True)\n",
    "        # self.total_matrix_A_2 = nn.Linear(int(2*S+2*T),int(2*S+2*T),bias=False)\n",
    "        # self.total_matrix_A_2 = nn.Parameter(torch.zeros(int(2*S+2*T), int(2*S+2*T)))\n",
    "#         self.trainable_part = nn.Parameter(torch.zeros(S, S))\n",
    "        self.trainable_part = 1e3*torch.eye(S).cuda()\n",
    "        self.non_trainable_part_1 = torch.zeros(S, int(S+T), requires_grad=False).cuda()\n",
    "        self.non_trainable_part_2 = torch.zeros(S, T, requires_grad=False).cuda()\n",
    "        self.non_trainable_part_3 = torch.zeros(2*T+S, 2*S+2*T, requires_grad=False).cuda()\n",
    "\n",
    "    def forward(self, h_1):\n",
    "        # self.total_matrix_A_2.register_hook(apply_mask_2)\n",
    "        total_matrix_A_2=torch.cat((self.non_trainable_part_1,self.trainable_part,self.non_trainable_part_2),dim=1)\n",
    "        total_matrix_A_2=torch.cat((total_matrix_A_2,self.non_trainable_part_3),dim=0)\n",
    "        \n",
    "        h_1=h_1.float()\n",
    "        \n",
    "        total_matrix_A_2=total_matrix_A_2.unsqueeze(0)\n",
    "        total_matrix_A_2=total_matrix_A_2.repeat(h_1.shape[0],1,1)\n",
    "        #print(\"total_matrix_A_2_shape: \",total_matrix_A_2.shape)\n",
    "\n",
    "        scores = torch.bmm(h_1, total_matrix_A_2) \n",
    "        scores = torch.bmm(scores,h_1.transpose(1,2))\n",
    "        #row-wise softmax\n",
    "        \n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.bmm(attn, h_1)\n",
    "        #print('layer 2 output shape: ',context.shape)\n",
    "        \n",
    "        return context #, attn\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.attention_1 = Attention_1()\n",
    "        self.attention_2 = Attention_2()\n",
    "        # self.linear = nn.Linear(2*S+2*T, 1)\n",
    "        # self.W_tilde_T = nn.Parameter(torch.zeros(int(4*S+4*T), int(S)).float())\n",
    "        self.trainable_part = nn.Parameter(torch.zeros(S, S))\n",
    "#         self.trainable_part = torch.eye(S).cuda()\n",
    "        self.non_trainable_part_1 = torch.zeros((2*S+2*T), S, requires_grad=False).cuda()\n",
    "        self.non_trainable_part_2 = torch.zeros((S+2*T), S, requires_grad=False).cuda()\n",
    "\n",
    "    def forward(self, X):\n",
    "        W_tilde_T=torch.cat((self.non_trainable_part_1,self.trainable_part,self.non_trainable_part_2),dim=0).to(device)\n",
    "        \n",
    "        \n",
    "        attn_mask = mask_layer_1\n",
    "        context_1 = self.attention_1(X, attn_mask)\n",
    "        # print('h_0 shape: ',X.shape)\n",
    "        \n",
    "        h_1=torch.cat((X,context_1),dim=-1).float()\n",
    "        # print(\"h_1 shape: \",h_1.shape) \n",
    "        # print(h_1.shape)\n",
    "        \n",
    "        context_2 = self.attention_2(h_1)\n",
    "        h_2=torch.cat((h_1,context_2),dim=-1).float()\n",
    "        # output = self.linear(h_1)\n",
    "        \n",
    "        output=torch.matmul(h_2[:,-1,:],W_tilde_T)\n",
    "        #print('output shape: ',output.shape)\n",
    "        \n",
    "        return output#, W_tilde_T#,attn_1,attn_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_layer_1=torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "# class ScaledDotProductAttention(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ScaledDotProductAttention, self).__init__()\n",
    "#         #Random initialize the matrix A_1\n",
    "#         self.total_matrix_A_1 = torch.randn(int(S+T), int(S+T), requires_grad=True)\n",
    "\n",
    "\n",
    "#     def forward(self, h_0, attn_mask):\n",
    "#         A_1=self.total_matrix_A_1.register_hook(apply_mask_1)\n",
    "        \n",
    "#         scores = torch.matmul(h_0, A_1) \n",
    "#         scores = torch.matmul(scores,h_0.transpose)\n",
    "#         scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "#         attn = nn.Softmax(dim=-1)(scores)\n",
    "#         context = torch.matmul(attn, h_0)\n",
    "#         return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Transformer()\n",
    "net.to(device)\n",
    "# for param in net.parameters():\n",
    "#     print(param)\n",
    "#     param.requires_grad = False\n",
    "#     print(param.requires_grad)\n",
    "\n",
    "# net.attention_1.total_matrix_A_1[S:,S:].requires_grad = True\n",
    "# net.attention_2.total_matrix_A_2[0:S,int(S+T):int(2*S+T)].requires_grad = True\n",
    "# net.W_tilde_T[int(2*S+2*T):int(3*S+2*T),:].requires_grad = True\n",
    "\n",
    "# print(\"Requires grad for A_1 section:\", net.attention_1.total_matrix_A_1[S:, S:].requires_grad)\n",
    "# print(\"Requires grad for A_2 section:\", net.attention_2.total_matrix_A_2[0:S, int(S+T):int(2*S+T)].requires_grad)\n",
    "# print(\"Requires grad for W_tilde_T section:\", net.W_tilde_T[int(2*S+2*T):int(3*S+2*T), :].requires_grad)\n",
    "\n",
    "\n",
    "# print(\"Total matrix A_1 shape:\", net.attention_1.total_matrix_A_1.shape)\n",
    "# print(\"Total matrix A_2 shape:\", net.attention_2.total_matrix_A_2.shape)\n",
    "# print(\"W_tilde_T shape:\", net.W_tilde_T.shape)\n",
    "\n",
    "\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "# trainable_params = [p for p in net.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.3)\n",
    "\n",
    "# # Assuming these are the tensors you want to optimize\n",
    "# total_matrix_A_1 = torch.nn.Parameter(total_matrix_A_1)\n",
    "# total_matrix_A_1.register_hook(apply_mask_1)\n",
    "# total_matrix_A_2 = torch.nn.Parameter(total_matrix_A_2)\n",
    "# total_matrix_A_2.register_hook(apply_mask_2)\n",
    "# W_tilde_T = torch.nn.Parameter(W_tilde_T)\n",
    "# W_tilde_T.register_hook(apply_mask_W_T)\n",
    "\n",
    "# # Creating a list of parameters\n",
    "# parameters = [total_matrix_A_1, total_matrix_A_2, W_tilde_T]\n",
    "\n",
    "# Initializing the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([20, 20])\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(parameters[0].detach().numpy(), cmap='hot')  # 'hot' is a popular colormap for heatmaps\n",
    "# plt.colorbar()  # Show color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,Train Loss: 0.5529415607452393,Time: 0.039455413818359375\n",
      "Epoch: 500,Train Loss: 0.5527684688568115,Time: 8.063622951507568\n",
      "Epoch: 1000,Train Loss: 0.5526133179664612,Time: 16.087366104125977\n",
      "Epoch: 1500,Train Loss: 0.5524736642837524,Time: 23.973777294158936\n",
      "Epoch: 2000,Train Loss: 0.5523471832275391,Time: 32.11936140060425\n",
      "Epoch: 2500,Train Loss: 0.5522324442863464,Time: 40.119574308395386\n",
      "Epoch: 3000,Train Loss: 0.5521277189254761,Time: 48.17608833312988\n",
      "Epoch: 3500,Train Loss: 0.5520318746566772,Time: 56.1608612537384\n",
      "Epoch: 4000,Train Loss: 0.551943838596344,Time: 64.1645143032074\n",
      "Epoch: 4500,Train Loss: 0.5518628358840942,Time: 72.48238778114319\n",
      "Epoch: 5000,Train Loss: 0.5517879724502563,Time: 81.08679175376892\n",
      "Epoch: 5500,Train Loss: 0.551718533039093,Time: 89.55833959579468\n",
      "Epoch: 6000,Train Loss: 0.5516541600227356,Time: 98.1687240600586\n",
      "Epoch: 6500,Train Loss: 0.5515942573547363,Time: 106.46795392036438\n",
      "Epoch: 7000,Train Loss: 0.551538348197937,Time: 114.8722493648529\n",
      "Epoch: 7500,Train Loss: 0.5514861345291138,Time: 123.09537267684937\n",
      "Epoch: 8000,Train Loss: 0.5514371395111084,Time: 131.11722612380981\n",
      "Epoch: 8500,Train Loss: 0.5513912439346313,Time: 139.09951210021973\n",
      "Epoch: 9000,Train Loss: 0.5513480305671692,Time: 147.82950615882874\n",
      "Epoch: 9500,Train Loss: 0.5513073801994324,Time: 156.02930283546448\n",
      "Epoch: 10000,Train Loss: 0.5512690544128418,Time: 164.1951198577881\n",
      "Epoch: 10500,Train Loss: 0.5512328147888184,Time: 172.45018529891968\n",
      "Epoch: 11000,Train Loss: 0.5511986613273621,Time: 181.06362533569336\n",
      "Epoch: 11500,Train Loss: 0.5511661767959595,Time: 189.54614806175232\n",
      "Epoch: 12000,Train Loss: 0.5511354207992554,Time: 197.8290240764618\n",
      "Epoch: 12500,Train Loss: 0.5511061549186707,Time: 206.1688220500946\n",
      "Epoch: 13000,Train Loss: 0.5510783195495605,Time: 214.581463098526\n",
      "Epoch: 13500,Train Loss: 0.551051914691925,Time: 223.03961634635925\n",
      "Epoch: 14000,Train Loss: 0.5510267019271851,Time: 231.2127525806427\n",
      "Epoch: 14500,Train Loss: 0.551002562046051,Time: 239.39700603485107\n",
      "Epoch: 15000,Train Loss: 0.5509796738624573,Time: 247.95251488685608\n",
      "Epoch: 15500,Train Loss: 0.5509577393531799,Time: 256.0730724334717\n",
      "Epoch: 16000,Train Loss: 0.550936758518219,Time: 264.0654807090759\n",
      "Epoch: 16500,Train Loss: 0.5509166717529297,Time: 271.94676208496094\n",
      "Epoch: 17000,Train Loss: 0.5508973598480225,Time: 280.33961296081543\n",
      "Epoch: 17500,Train Loss: 0.5508788824081421,Time: 288.7482407093048\n",
      "Epoch: 18000,Train Loss: 0.5508611798286438,Time: 297.0845546722412\n",
      "Epoch: 18500,Train Loss: 0.5508440136909485,Time: 304.96932578086853\n",
      "Epoch: 19000,Train Loss: 0.55082768201828,Time: 313.14287638664246\n",
      "Epoch: 19500,Train Loss: 0.5508118867874146,Time: 321.1066610813141\n",
      "Epoch: 20000,Train Loss: 0.5507968068122864,Time: 329.39962339401245\n",
      "Epoch: 20500,Train Loss: 0.5507821440696716,Time: 337.7680368423462\n",
      "Epoch: 21000,Train Loss: 0.5507680177688599,Time: 346.2571482658386\n",
      "Epoch: 21500,Train Loss: 0.5507543087005615,Time: 354.62596893310547\n",
      "Epoch: 22000,Train Loss: 0.5507412552833557,Time: 362.87364196777344\n",
      "Epoch: 22500,Train Loss: 0.5507286190986633,Time: 371.0191979408264\n",
      "Epoch: 23000,Train Loss: 0.5507163405418396,Time: 379.06176710128784\n",
      "Epoch: 23500,Train Loss: 0.5507044792175293,Time: 387.05661392211914\n",
      "Epoch: 24000,Train Loss: 0.5506930947303772,Time: 395.38264083862305\n",
      "Epoch: 24500,Train Loss: 0.5506819486618042,Time: 403.3866205215454\n",
      "Epoch: 25000,Train Loss: 0.5506711602210999,Time: 411.6803824901581\n",
      "Epoch: 25500,Train Loss: 0.5506607890129089,Time: 419.59570956230164\n",
      "Epoch: 26000,Train Loss: 0.5506507754325867,Time: 428.19570755958557\n",
      "Epoch: 26500,Train Loss: 0.5506409406661987,Time: 436.59400725364685\n",
      "Epoch: 27000,Train Loss: 0.550631582736969,Time: 444.6593201160431\n",
      "Epoch: 27500,Train Loss: 0.5506223440170288,Time: 452.9933114051819\n",
      "Epoch: 28000,Train Loss: 0.5506134033203125,Time: 461.08988428115845\n",
      "Epoch: 28500,Train Loss: 0.5506048202514648,Time: 469.6871407032013\n",
      "Epoch: 29000,Train Loss: 0.5505963563919067,Time: 478.49717593193054\n",
      "Epoch: 29500,Train Loss: 0.5505881905555725,Time: 487.2384383678436\n",
      "Epoch: 30000,Train Loss: 0.5505802035331726,Time: 495.77428555488586\n",
      "Epoch: 30500,Train Loss: 0.5505724549293518,Time: 504.07171058654785\n",
      "Epoch: 31000,Train Loss: 0.5505650043487549,Time: 512.2355673313141\n",
      "Epoch: 31500,Train Loss: 0.5505576729774475,Time: 520.7166450023651\n",
      "Epoch: 32000,Train Loss: 0.5505505204200745,Time: 528.8558940887451\n",
      "Epoch: 32500,Train Loss: 0.5505436062812805,Time: 536.9357924461365\n",
      "Epoch: 33000,Train Loss: 0.5505368709564209,Time: 544.8371150493622\n",
      "Epoch: 33500,Train Loss: 0.5505302548408508,Time: 552.9901170730591\n",
      "Epoch: 34000,Train Loss: 0.5505238771438599,Time: 561.152619600296\n",
      "Epoch: 34500,Train Loss: 0.5505176186561584,Time: 569.964685678482\n",
      "Epoch: 35000,Train Loss: 0.5505115985870361,Time: 578.1512384414673\n",
      "Epoch: 35500,Train Loss: 0.5505056381225586,Time: 586.405731678009\n",
      "Epoch: 36000,Train Loss: 0.5504997968673706,Time: 594.3125064373016\n",
      "Epoch: 36500,Train Loss: 0.5504941344261169,Time: 602.4456894397736\n",
      "Epoch: 37000,Train Loss: 0.5504885315895081,Time: 610.3974175453186\n",
      "Epoch: 37500,Train Loss: 0.5504831671714783,Time: 618.6201574802399\n",
      "Epoch: 38000,Train Loss: 0.550477921962738,Time: 626.5414273738861\n",
      "Epoch: 38500,Train Loss: 0.5504727959632874,Time: 634.823084115982\n",
      "Epoch: 39000,Train Loss: 0.5504677295684814,Time: 642.9520120620728\n",
      "Epoch: 39500,Train Loss: 0.5504628419876099,Time: 651.3423113822937\n",
      "Epoch: 40000,Train Loss: 0.5504580140113831,Time: 659.6017379760742\n",
      "Epoch: 40500,Train Loss: 0.5504534244537354,Time: 668.0018556118011\n",
      "Epoch: 41000,Train Loss: 0.5504487752914429,Time: 675.9820010662079\n",
      "Epoch: 41500,Train Loss: 0.5504443049430847,Time: 684.3682961463928\n",
      "Epoch: 42000,Train Loss: 0.5504398345947266,Time: 692.642466545105\n",
      "Epoch: 42500,Train Loss: 0.5504356026649475,Time: 701.204772233963\n",
      "Epoch: 43000,Train Loss: 0.5504314303398132,Time: 709.1476163864136\n",
      "Epoch: 43500,Train Loss: 0.550427258014679,Time: 717.3422319889069\n",
      "Epoch: 44000,Train Loss: 0.5504232048988342,Time: 725.7210826873779\n",
      "Epoch: 44500,Train Loss: 0.5504193305969238,Time: 734.229660987854\n",
      "Epoch: 45000,Train Loss: 0.5504154562950134,Time: 742.1404428482056\n",
      "Epoch: 45500,Train Loss: 0.5504116415977478,Time: 750.2839598655701\n",
      "Epoch: 46000,Train Loss: 0.5504079461097717,Time: 758.638594865799\n",
      "Epoch: 46500,Train Loss: 0.5504043102264404,Time: 766.8915960788727\n",
      "Epoch: 47000,Train Loss: 0.5504007935523987,Time: 774.8566598892212\n",
      "Epoch: 47500,Train Loss: 0.5503972172737122,Time: 782.9545459747314\n",
      "Epoch: 48000,Train Loss: 0.55039381980896,Time: 790.976833820343\n",
      "Epoch: 48500,Train Loss: 0.5503904819488525,Time: 799.0609991550446\n",
      "Epoch: 49000,Train Loss: 0.5503870844841003,Time: 807.09916639328\n",
      "Epoch: 49500,Train Loss: 0.5503839254379272,Time: 815.5252051353455\n",
      "Epoch: 50000,Train Loss: 0.5503807663917542,Time: 823.8619668483734\n",
      "Epoch: 50500,Train Loss: 0.5503777265548706,Time: 832.3236742019653\n",
      "Epoch: 51000,Train Loss: 0.5503746271133423,Time: 840.2291407585144\n",
      "Epoch: 51500,Train Loss: 0.5503715872764587,Time: 848.3639075756073\n",
      "Epoch: 52000,Train Loss: 0.5503686666488647,Time: 856.3392164707184\n",
      "Epoch: 52500,Train Loss: 0.5503658056259155,Time: 864.7331948280334\n",
      "Epoch: 53000,Train Loss: 0.5503629446029663,Time: 873.1483142375946\n",
      "Epoch: 53500,Train Loss: 0.5503601431846619,Time: 881.8197476863861\n",
      "Epoch: 54000,Train Loss: 0.550357460975647,Time: 889.8700468540192\n",
      "Epoch: 54500,Train Loss: 0.5503547787666321,Time: 898.1168835163116\n",
      "Epoch: 55000,Train Loss: 0.5503520965576172,Time: 906.1294016838074\n",
      "Epoch: 55500,Train Loss: 0.5503495931625366,Time: 914.2302858829498\n",
      "Epoch: 56000,Train Loss: 0.5503469705581665,Time: 922.3860161304474\n",
      "Epoch: 56500,Train Loss: 0.5503445267677307,Time: 930.4417922496796\n",
      "Epoch: 57000,Train Loss: 0.5503420829772949,Time: 938.5782556533813\n",
      "Epoch: 57500,Train Loss: 0.5503396391868591,Time: 946.6381695270538\n",
      "Epoch: 58000,Train Loss: 0.5503371953964233,Time: 954.5033278465271\n",
      "Epoch: 58500,Train Loss: 0.5503348708152771,Time: 963.2798290252686\n",
      "Epoch: 59000,Train Loss: 0.5503326654434204,Time: 971.9501583576202\n",
      "Epoch: 59500,Train Loss: 0.5503303408622742,Time: 980.7114124298096\n",
      "Epoch: 60000,Train Loss: 0.5503280758857727,Time: 988.7197542190552\n",
      "Epoch: 60500,Train Loss: 0.5503259301185608,Time: 997.1465313434601\n",
      "Epoch: 61000,Train Loss: 0.5503237843513489,Time: 1005.1114060878754\n",
      "Epoch: 61500,Train Loss: 0.5503216981887817,Time: 1013.2792341709137\n",
      "Epoch: 62000,Train Loss: 0.5503195524215698,Time: 1021.5097525119781\n",
      "Epoch: 62500,Train Loss: 0.5503174662590027,Time: 1029.7641017436981\n",
      "Epoch: 63000,Train Loss: 0.5503154993057251,Time: 1037.753555059433\n",
      "Epoch: 63500,Train Loss: 0.5503135323524475,Time: 1046.0281121730804\n",
      "Epoch: 64000,Train Loss: 0.5503115653991699,Time: 1053.8206157684326\n",
      "Epoch: 64500,Train Loss: 0.5503096580505371,Time: 1061.8629729747772\n",
      "Epoch: 65000,Train Loss: 0.5503077507019043,Time: 1069.8774001598358\n",
      "Epoch: 65500,Train Loss: 0.5503059029579163,Time: 1077.9633095264435\n",
      "Epoch: 66000,Train Loss: 0.5503039956092834,Time: 1086.0625169277191\n",
      "Epoch: 66500,Train Loss: 0.5503021478652954,Time: 1094.7807440757751\n",
      "Epoch: 67000,Train Loss: 0.5503004193305969,Time: 1102.9079694747925\n",
      "Epoch: 67500,Train Loss: 0.5502986311912537,Time: 1110.9065253734589\n",
      "Epoch: 68000,Train Loss: 0.5502969622612,Time: 1118.7801604270935\n",
      "Epoch: 68500,Train Loss: 0.5502952337265015,Time: 1126.876984834671\n",
      "Epoch: 69000,Train Loss: 0.5502935647964478,Time: 1134.879906654358\n",
      "Epoch: 69500,Train Loss: 0.550291895866394,Time: 1142.9206676483154\n",
      "Epoch: 70000,Train Loss: 0.5502901673316956,Time: 1150.8158645629883\n",
      "Epoch: 70500,Train Loss: 0.550288736820221,Time: 1158.7924399375916\n",
      "Epoch: 71000,Train Loss: 0.5502870082855225,Time: 1166.7912135124207\n",
      "Epoch: 71500,Train Loss: 0.5502853989601135,Time: 1174.9087374210358\n",
      "Epoch: 72000,Train Loss: 0.5502838492393494,Time: 1182.8360278606415\n",
      "Epoch: 72500,Train Loss: 0.55028235912323,Time: 1191.018893480301\n",
      "Epoch: 73000,Train Loss: 0.5502809286117554,Time: 1198.9495778083801\n",
      "Epoch: 73500,Train Loss: 0.5502793788909912,Time: 1207.3070347309113\n",
      "Epoch: 74000,Train Loss: 0.5502779483795166,Time: 1215.7349672317505\n",
      "Epoch: 74500,Train Loss: 0.550276517868042,Time: 1224.1577966213226\n",
      "Epoch: 75000,Train Loss: 0.5502750277519226,Time: 1232.464117050171\n",
      "Epoch: 75500,Train Loss: 0.5502736568450928,Time: 1240.7021224498749\n",
      "Epoch: 76000,Train Loss: 0.5502722263336182,Time: 1248.5661697387695\n",
      "Epoch: 76500,Train Loss: 0.5502708554267883,Time: 1256.7800936698914\n",
      "Epoch: 77000,Train Loss: 0.5502694845199585,Time: 1264.7421634197235\n",
      "Epoch: 77500,Train Loss: 0.5502682328224182,Time: 1272.9059603214264\n",
      "Epoch: 78000,Train Loss: 0.5502668619155884,Time: 1280.9257137775421\n",
      "Epoch: 78500,Train Loss: 0.5502656102180481,Time: 1289.1172688007355\n",
      "Epoch: 79000,Train Loss: 0.550264298915863,Time: 1297.2662932872772\n",
      "Epoch: 79500,Train Loss: 0.5502630472183228,Time: 1305.4252107143402\n",
      "Epoch: 80000,Train Loss: 0.5502617955207825,Time: 1313.2844038009644\n",
      "Epoch: 80500,Train Loss: 0.550260603427887,Time: 1321.7105815410614\n",
      "Epoch: 81000,Train Loss: 0.5502593517303467,Time: 1329.7624487876892\n",
      "Epoch: 81500,Train Loss: 0.5502581000328064,Time: 1337.9730088710785\n",
      "Epoch: 82000,Train Loss: 0.5502569079399109,Time: 1346.1338202953339\n",
      "Epoch: 82500,Train Loss: 0.5502557754516602,Time: 1354.4445552825928\n",
      "Epoch: 83000,Train Loss: 0.5502545237541199,Time: 1362.7998881340027\n",
      "Epoch: 83500,Train Loss: 0.5502534508705139,Time: 1370.9861829280853\n",
      "Epoch: 84000,Train Loss: 0.5502523183822632,Time: 1379.1902933120728\n",
      "Epoch: 84500,Train Loss: 0.5502512454986572,Time: 1387.4280178546906\n",
      "Epoch: 85000,Train Loss: 0.5502501130104065,Time: 1395.4761264324188\n",
      "Epoch: 85500,Train Loss: 0.5502490401268005,Time: 1403.8171994686127\n",
      "Epoch: 86000,Train Loss: 0.5502479672431946,Time: 1411.8472027778625\n",
      "Epoch: 86500,Train Loss: 0.5502468347549438,Time: 1419.9232079982758\n",
      "Epoch: 87000,Train Loss: 0.5502458214759827,Time: 1427.939959526062\n",
      "Epoch: 87500,Train Loss: 0.5502447485923767,Time: 1436.8228209018707\n",
      "Epoch: 88000,Train Loss: 0.5502436757087708,Time: 1444.8791630268097\n",
      "Epoch: 88500,Train Loss: 0.5502427220344543,Time: 1453.33327460289\n",
      "Epoch: 89000,Train Loss: 0.5502417087554932,Time: 1461.292049407959\n",
      "Epoch: 89500,Train Loss: 0.550240695476532,Time: 1469.6937720775604\n",
      "Epoch: 90000,Train Loss: 0.550239622592926,Time: 1477.9297814369202\n",
      "Epoch: 90500,Train Loss: 0.5502387881278992,Time: 1486.3965587615967\n",
      "Epoch: 91000,Train Loss: 0.5502378344535828,Time: 1494.475472688675\n",
      "Epoch: 91500,Train Loss: 0.5502368211746216,Time: 1502.6151592731476\n",
      "Epoch: 92000,Train Loss: 0.5502358675003052,Time: 1510.5314528942108\n",
      "Epoch: 92500,Train Loss: 0.5502349734306335,Time: 1518.955646276474\n",
      "Epoch: 93000,Train Loss: 0.5502340197563171,Time: 1527.252676486969\n",
      "Epoch: 93500,Train Loss: 0.5502331256866455,Time: 1535.5866906642914\n",
      "Epoch: 94000,Train Loss: 0.5502322316169739,Time: 1543.520319223404\n",
      "Epoch: 94500,Train Loss: 0.5502313375473022,Time: 1552.2331511974335\n",
      "Epoch: 95000,Train Loss: 0.5502304434776306,Time: 1560.4619801044464\n",
      "Epoch: 95500,Train Loss: 0.5502296090126038,Time: 1568.6604669094086\n",
      "Epoch: 96000,Train Loss: 0.5502287149429321,Time: 1576.5985372066498\n",
      "Epoch: 96500,Train Loss: 0.5502278804779053,Time: 1584.8057177066803\n",
      "Epoch: 97000,Train Loss: 0.5502271056175232,Time: 1592.6631050109863\n",
      "Epoch: 97500,Train Loss: 0.5502261519432068,Time: 1601.0933780670166\n",
      "Epoch: 98000,Train Loss: 0.5502253770828247,Time: 1609.276591539383\n",
      "Epoch: 98500,Train Loss: 0.5502245426177979,Time: 1617.2316422462463\n",
      "Epoch: 99000,Train Loss: 0.550223708152771,Time: 1625.4011561870575\n",
      "Epoch: 99500,Train Loss: 0.5502229332923889,Time: 1633.6444158554077\n",
      "Epoch: 100000,Train Loss: 0.5502220988273621,Time: 1641.954585313797\n",
      "Epoch: 100500,Train Loss: 0.55022132396698,Time: 1649.9508926868439\n",
      "Epoch: 101000,Train Loss: 0.5502206087112427,Time: 1658.0290744304657\n",
      "Epoch: 101500,Train Loss: 0.5502198934555054,Time: 1665.8958141803741\n",
      "Epoch: 102000,Train Loss: 0.5502190589904785,Time: 1673.9385781288147\n",
      "Epoch: 102500,Train Loss: 0.5502182841300964,Time: 1682.0500762462616\n",
      "Epoch: 103000,Train Loss: 0.5502175688743591,Time: 1690.5432834625244\n",
      "Epoch: 103500,Train Loss: 0.550216794013977,Time: 1698.5938584804535\n",
      "Epoch: 104000,Train Loss: 0.550216019153595,Time: 1706.8577144145966\n",
      "Epoch: 104500,Train Loss: 0.5502153635025024,Time: 1715.197165966034\n",
      "Epoch: 105000,Train Loss: 0.5502146482467651,Time: 1723.7917366027832\n",
      "Epoch: 105500,Train Loss: 0.5502138733863831,Time: 1731.8444879055023\n",
      "Epoch: 106000,Train Loss: 0.5502131581306458,Time: 1740.3105874061584\n",
      "Epoch: 106500,Train Loss: 0.550212562084198,Time: 1748.2151737213135\n",
      "Epoch: 107000,Train Loss: 0.5502118468284607,Time: 1756.4054429531097\n",
      "Epoch: 107500,Train Loss: 0.5502111315727234,Time: 1764.3354930877686\n",
      "Epoch: 108000,Train Loss: 0.5502104163169861,Time: 1772.3032550811768\n",
      "Epoch: 108500,Train Loss: 0.5502097606658936,Time: 1780.4805393218994\n",
      "Epoch: 109000,Train Loss: 0.5502090454101562,Time: 1788.4981672763824\n",
      "Epoch: 109500,Train Loss: 0.5502084493637085,Time: 1796.3918273448944\n",
      "Epoch: 110000,Train Loss: 0.550207793712616,Time: 1804.5873067378998\n",
      "Epoch: 110500,Train Loss: 0.5502071976661682,Time: 1812.816469669342\n",
      "Epoch: 111000,Train Loss: 0.5502064824104309,Time: 1820.9264361858368\n",
      "Epoch: 111500,Train Loss: 0.5502058267593384,Time: 1828.8814060688019\n",
      "Epoch: 112000,Train Loss: 0.5502052307128906,Time: 1837.447634935379\n",
      "Epoch: 112500,Train Loss: 0.5502046346664429,Time: 1845.349348783493\n",
      "Epoch: 113000,Train Loss: 0.5502039790153503,Time: 1853.9527096748352\n",
      "Epoch: 113500,Train Loss: 0.5502033233642578,Time: 1862.2015326023102\n",
      "Epoch: 114000,Train Loss: 0.5502027273178101,Time: 1870.5550060272217\n",
      "Epoch: 114500,Train Loss: 0.5502021312713623,Time: 1878.883395433426\n",
      "Epoch: 115000,Train Loss: 0.5502014756202698,Time: 1887.0732336044312\n",
      "Epoch: 115500,Train Loss: 0.5502009391784668,Time: 1895.3476090431213\n",
      "Epoch: 116000,Train Loss: 0.550200343132019,Time: 1903.4050047397614\n",
      "Epoch: 116500,Train Loss: 0.5501997470855713,Time: 1911.2441890239716\n",
      "Epoch: 117000,Train Loss: 0.5501992702484131,Time: 1919.394897699356\n",
      "Epoch: 117500,Train Loss: 0.5501985549926758,Time: 1927.3476116657257\n",
      "Epoch: 118000,Train Loss: 0.5501980185508728,Time: 1935.461002111435\n",
      "Epoch: 118500,Train Loss: 0.5501974821090698,Time: 1943.6020896434784\n",
      "Epoch: 119000,Train Loss: 0.5501969456672668,Time: 1951.7785170078278\n",
      "Epoch: 119500,Train Loss: 0.5501963496208191,Time: 1959.906142950058\n",
      "Epoch: 120000,Train Loss: 0.5501957535743713,Time: 1968.0250067710876\n",
      "Epoch: 120500,Train Loss: 0.5501952767372131,Time: 1976.1598737239838\n",
      "Epoch: 121000,Train Loss: 0.5501947402954102,Time: 1984.4158244132996\n",
      "Epoch: 121500,Train Loss: 0.550194263458252,Time: 1992.4840149879456\n",
      "Epoch: 122000,Train Loss: 0.5501936674118042,Time: 2000.9766035079956\n",
      "Epoch: 122500,Train Loss: 0.5501931309700012,Time: 2008.950440645218\n",
      "Epoch: 123000,Train Loss: 0.550192654132843,Time: 2016.9860033988953\n",
      "Epoch: 123500,Train Loss: 0.5501921772956848,Time: 2024.829169511795\n",
      "Epoch: 124000,Train Loss: 0.5501917004585266,Time: 2032.7951636314392\n",
      "Epoch: 124500,Train Loss: 0.5501911044120789,Time: 2040.8201096057892\n",
      "Epoch: 125000,Train Loss: 0.5501905679702759,Time: 2048.9716923236847\n",
      "Epoch: 125500,Train Loss: 0.5501901507377625,Time: 2056.8151330947876\n",
      "Epoch: 126000,Train Loss: 0.5501896142959595,Time: 2064.9304790496826\n",
      "Epoch: 126500,Train Loss: 0.5501891374588013,Time: 2073.117056131363\n",
      "Epoch: 127000,Train Loss: 0.5501886010169983,Time: 2081.685056447983\n",
      "Epoch: 127500,Train Loss: 0.5501881241798401,Time: 2089.889096021652\n",
      "Epoch: 128000,Train Loss: 0.5501876473426819,Time: 2098.1834831237793\n",
      "Epoch: 128500,Train Loss: 0.5501871705055237,Time: 2106.1389071941376\n",
      "Epoch: 129000,Train Loss: 0.5501867532730103,Time: 2114.289299726486\n",
      "Epoch: 129500,Train Loss: 0.5501862168312073,Time: 2122.3268961906433\n",
      "Epoch: 130000,Train Loss: 0.5501857995986938,Time: 2130.4820659160614\n",
      "Epoch: 130500,Train Loss: 0.5501853227615356,Time: 2138.659613132477\n",
      "Epoch: 131000,Train Loss: 0.5501848459243774,Time: 2146.903026819229\n",
      "Epoch: 131500,Train Loss: 0.5501844882965088,Time: 2154.7482466697693\n",
      "Epoch: 132000,Train Loss: 0.5501840114593506,Time: 2163.288463830948\n",
      "Epoch: 132500,Train Loss: 0.5501835346221924,Time: 2171.187737226486\n",
      "Epoch: 133000,Train Loss: 0.550183117389679,Time: 2179.4104709625244\n",
      "Epoch: 133500,Train Loss: 0.5501826405525208,Time: 2187.5389161109924\n",
      "Epoch: 134000,Train Loss: 0.5501821637153625,Time: 2196.0780127048492\n",
      "Epoch: 134500,Train Loss: 0.5501818060874939,Time: 2204.0968544483185\n",
      "Epoch: 135000,Train Loss: 0.5501813292503357,Time: 2212.732628107071\n",
      "Epoch: 135500,Train Loss: 0.5501809120178223,Time: 2220.862055540085\n",
      "Epoch: 136000,Train Loss: 0.5501804947853088,Time: 2229.063726902008\n",
      "Epoch: 136500,Train Loss: 0.5501800775527954,Time: 2237.117744207382\n",
      "Epoch: 137000,Train Loss: 0.5501796007156372,Time: 2245.2600045204163\n",
      "Epoch: 137500,Train Loss: 0.5501792430877686,Time: 2253.176439523697\n",
      "Epoch: 138000,Train Loss: 0.5501787662506104,Time: 2261.2275989055634\n",
      "Epoch: 138500,Train Loss: 0.5501783490180969,Time: 2269.1520330905914\n",
      "Epoch: 139000,Train Loss: 0.5501779317855835,Time: 2277.576733350754\n",
      "Epoch: 139500,Train Loss: 0.5501775145530701,Time: 2285.7205226421356\n",
      "Epoch: 140000,Train Loss: 0.5501771569252014,Time: 2293.8809337615967\n",
      "Epoch: 140500,Train Loss: 0.550176739692688,Time: 2301.903358221054\n",
      "Epoch: 141000,Train Loss: 0.5501763224601746,Time: 2310.2651941776276\n",
      "Epoch: 141500,Train Loss: 0.5501759052276611,Time: 2318.4227735996246\n",
      "Epoch: 142000,Train Loss: 0.5501756072044373,Time: 2326.5128490924835\n",
      "Epoch: 142500,Train Loss: 0.5501751899719238,Time: 2334.5531191825867\n",
      "Epoch: 143000,Train Loss: 0.5501748323440552,Time: 2343.051504611969\n",
      "Epoch: 143500,Train Loss: 0.5501744747161865,Time: 2351.240873813629\n",
      "Epoch: 144000,Train Loss: 0.5501741170883179,Time: 2359.574229478836\n",
      "Epoch: 144500,Train Loss: 0.5501737594604492,Time: 2367.6980199813843\n",
      "Epoch: 145000,Train Loss: 0.550173282623291,Time: 2375.7350680828094\n",
      "Epoch: 145500,Train Loss: 0.5501729249954224,Time: 2383.5810072422028\n",
      "Epoch: 146000,Train Loss: 0.5501725673675537,Time: 2391.68799161911\n",
      "Epoch: 146500,Train Loss: 0.5501722097396851,Time: 2399.6147696971893\n",
      "Epoch: 147000,Train Loss: 0.5501719117164612,Time: 2407.727169752121\n",
      "Epoch: 147500,Train Loss: 0.5501714944839478,Time: 2415.92640376091\n",
      "Epoch: 148000,Train Loss: 0.5501711964607239,Time: 2424.4400470256805\n",
      "Epoch: 148500,Train Loss: 0.5501707792282104,Time: 2433.1006047725677\n",
      "Epoch: 149000,Train Loss: 0.5501704216003418,Time: 2441.2138538360596\n",
      "Epoch: 149500,Train Loss: 0.5501700639724731,Time: 2449.2913467884064\n",
      "Epoch: 150000,Train Loss: 0.5501697063446045,Time: 2457.3356549739838\n",
      "Epoch: 150500,Train Loss: 0.5501694679260254,Time: 2465.2437856197357\n",
      "Epoch: 151000,Train Loss: 0.550169050693512,Time: 2473.741434574127\n",
      "Epoch: 151500,Train Loss: 0.5501687526702881,Time: 2481.6262934207916\n",
      "Epoch: 152000,Train Loss: 0.5501683950424194,Time: 2489.872245788574\n",
      "Epoch: 152500,Train Loss: 0.5501680374145508,Time: 2497.7807998657227\n",
      "Epoch: 153000,Train Loss: 0.5501677393913269,Time: 2505.887808561325\n",
      "Epoch: 153500,Train Loss: 0.5501673817634583,Time: 2514.1365485191345\n",
      "Epoch: 154000,Train Loss: 0.5501670241355896,Time: 2522.2803630828857\n",
      "Epoch: 154500,Train Loss: 0.5501667857170105,Time: 2530.214322566986\n",
      "Epoch: 155000,Train Loss: 0.5501664280891418,Time: 2538.588698863983\n",
      "Epoch: 155500,Train Loss: 0.5501660704612732,Time: 2546.789078235626\n",
      "Epoch: 156000,Train Loss: 0.5501657128334045,Time: 2554.9799921512604\n",
      "Epoch: 156500,Train Loss: 0.5501654148101807,Time: 2562.8498232364655\n",
      "Epoch: 157000,Train Loss: 0.550165057182312,Time: 2571.2482941150665\n",
      "Epoch: 157500,Train Loss: 0.5501647591590881,Time: 2579.0516741275787\n",
      "Epoch: 158000,Train Loss: 0.5501644015312195,Time: 2587.106745481491\n",
      "Epoch: 158500,Train Loss: 0.5501642227172852,Time: 2595.0206685066223\n",
      "Epoch: 159000,Train Loss: 0.5501638054847717,Time: 2603.0705966949463\n",
      "Epoch: 159500,Train Loss: 0.5501635670661926,Time: 2610.9904108047485\n",
      "Epoch: 160000,Train Loss: 0.550163209438324,Time: 2619.08433675766\n",
      "Epoch: 160500,Train Loss: 0.5501628518104553,Time: 2627.4562084674835\n",
      "Epoch: 161000,Train Loss: 0.5501626133918762,Time: 2635.785400390625\n",
      "Epoch: 161500,Train Loss: 0.5501622557640076,Time: 2643.6729254722595\n",
      "Epoch: 162000,Train Loss: 0.5501620173454285,Time: 2651.876798391342\n",
      "Epoch: 162500,Train Loss: 0.5501617193222046,Time: 2659.8966414928436\n",
      "Epoch: 163000,Train Loss: 0.5501614212989807,Time: 2668.0605630874634\n",
      "Epoch: 163500,Train Loss: 0.5501611232757568,Time: 2675.9548256397247\n",
      "Epoch: 164000,Train Loss: 0.550160825252533,Time: 2684.1193222999573\n",
      "Epoch: 164500,Train Loss: 0.5501605272293091,Time: 2692.3649451732635\n",
      "Epoch: 165000,Train Loss: 0.5501602292060852,Time: 2701.020121574402\n",
      "Epoch: 165500,Train Loss: 0.5501599311828613,Time: 2708.8671202659607\n",
      "Epoch: 166000,Train Loss: 0.5501596927642822,Time: 2717.7309980392456\n",
      "Epoch: 166500,Train Loss: 0.5501593947410583,Time: 2726.3668303489685\n",
      "Epoch: 167000,Train Loss: 0.5501590967178345,Time: 2734.8830082416534\n",
      "Epoch: 167500,Train Loss: 0.5501589179039001,Time: 2743.263642311096\n",
      "Epoch: 168000,Train Loss: 0.5501586198806763,Time: 2751.737076282501\n",
      "Epoch: 168500,Train Loss: 0.5501582622528076,Time: 2760.064291238785\n",
      "Epoch: 169000,Train Loss: 0.5501580238342285,Time: 2768.305983066559\n",
      "Epoch: 169500,Train Loss: 0.5501577258110046,Time: 2776.172783136368\n",
      "Epoch: 170000,Train Loss: 0.5501574873924255,Time: 2784.136426925659\n",
      "Epoch: 170500,Train Loss: 0.5501572489738464,Time: 2792.4734723567963\n",
      "Epoch: 171000,Train Loss: 0.5501570105552673,Time: 2800.492924928665\n",
      "Epoch: 171500,Train Loss: 0.5501566529273987,Time: 2808.725686311722\n",
      "Epoch: 172000,Train Loss: 0.5501564145088196,Time: 2816.9623165130615\n",
      "Epoch: 172500,Train Loss: 0.5501561760902405,Time: 2825.0449888706207\n",
      "Epoch: 173000,Train Loss: 0.5501559376716614,Time: 2833.200898170471\n",
      "Epoch: 173500,Train Loss: 0.5501556396484375,Time: 2841.42285823822\n",
      "Epoch: 174000,Train Loss: 0.5501553416252136,Time: 2849.4488332271576\n",
      "Epoch: 174500,Train Loss: 0.5501550436019897,Time: 2857.590125799179\n",
      "Epoch: 175000,Train Loss: 0.5501548647880554,Time: 2865.7609543800354\n",
      "Epoch: 175500,Train Loss: 0.5501546263694763,Time: 2874.1716842651367\n",
      "Epoch: 176000,Train Loss: 0.550154447555542,Time: 2882.1801896095276\n",
      "Epoch: 176500,Train Loss: 0.5501541495323181,Time: 2890.539400577545\n",
      "Epoch: 177000,Train Loss: 0.550153911113739,Time: 2898.8046934604645\n",
      "Epoch: 177500,Train Loss: 0.5501536726951599,Time: 2907.0323016643524\n",
      "Epoch: 178000,Train Loss: 0.5501534938812256,Time: 2914.895183801651\n",
      "Epoch: 178500,Train Loss: 0.5501531958580017,Time: 2923.383953332901\n",
      "Epoch: 179000,Train Loss: 0.5501528978347778,Time: 2931.351973772049\n",
      "Epoch: 179500,Train Loss: 0.5501526594161987,Time: 2939.433753013611\n",
      "Epoch: 180000,Train Loss: 0.5501524806022644,Time: 2947.6131477355957\n",
      "Epoch: 180500,Train Loss: 0.5501522421836853,Time: 2955.83558511734\n",
      "Epoch: 181000,Train Loss: 0.5501519441604614,Time: 2963.80038523674\n",
      "Epoch: 181500,Train Loss: 0.5501517653465271,Time: 2971.8791494369507\n",
      "Epoch: 182000,Train Loss: 0.550151526927948,Time: 2979.873642683029\n",
      "Epoch: 182500,Train Loss: 0.5501512885093689,Time: 2987.980746269226\n",
      "Epoch: 183000,Train Loss: 0.5501510500907898,Time: 2996.137008190155\n",
      "Epoch: 183500,Train Loss: 0.5501508712768555,Time: 3004.38693523407\n",
      "Epoch: 184000,Train Loss: 0.5501505136489868,Time: 3012.37256193161\n",
      "Epoch: 184500,Train Loss: 0.5501503348350525,Time: 3020.5324025154114\n",
      "Epoch: 185000,Train Loss: 0.5501500368118286,Time: 3028.5342128276825\n",
      "Epoch: 185500,Train Loss: 0.5501499176025391,Time: 3036.730600118637\n",
      "Epoch: 186000,Train Loss: 0.55014967918396,Time: 3044.7136838436127\n",
      "Epoch: 186500,Train Loss: 0.5501494407653809,Time: 3053.02978014946\n",
      "Epoch: 187000,Train Loss: 0.5501492619514465,Time: 3061.35622882843\n",
      "Epoch: 187500,Train Loss: 0.5501489639282227,Time: 3069.7479927539825\n",
      "Epoch: 188000,Train Loss: 0.5501487851142883,Time: 3077.7625348567963\n",
      "Epoch: 188500,Train Loss: 0.550148606300354,Time: 3085.9238612651825\n",
      "Epoch: 189000,Train Loss: 0.5501483678817749,Time: 3094.197464942932\n",
      "Epoch: 189500,Train Loss: 0.550148069858551,Time: 3102.4626796245575\n",
      "Epoch: 190000,Train Loss: 0.5501479506492615,Time: 3110.8464930057526\n",
      "Epoch: 190500,Train Loss: 0.5501477122306824,Time: 3119.3076252937317\n",
      "Epoch: 191000,Train Loss: 0.5501474738121033,Time: 3127.2920920848846\n",
      "Epoch: 191500,Train Loss: 0.550147294998169,Time: 3135.355458498001\n",
      "Epoch: 192000,Train Loss: 0.5501470565795898,Time: 3143.3834400177\n",
      "Epoch: 192500,Train Loss: 0.5501468777656555,Time: 3151.516098022461\n",
      "Epoch: 193000,Train Loss: 0.5501466393470764,Time: 3159.9065992832184\n",
      "Epoch: 193500,Train Loss: 0.5501464009284973,Time: 3168.1412930488586\n",
      "Epoch: 194000,Train Loss: 0.550146222114563,Time: 3176.208555459976\n",
      "Epoch: 194500,Train Loss: 0.5501460433006287,Time: 3184.2637503147125\n",
      "Epoch: 195000,Train Loss: 0.5501458644866943,Time: 3192.4204320907593\n",
      "Epoch: 195500,Train Loss: 0.5501455664634705,Time: 3200.838358640671\n",
      "Epoch: 196000,Train Loss: 0.5501453876495361,Time: 3208.793422460556\n",
      "Epoch: 196500,Train Loss: 0.550145149230957,Time: 3217.052891969681\n",
      "Epoch: 197000,Train Loss: 0.5501450300216675,Time: 3225.2521777153015\n",
      "Epoch: 197500,Train Loss: 0.5501447916030884,Time: 3233.9094276428223\n",
      "Epoch: 198000,Train Loss: 0.5501445531845093,Time: 3241.90855550766\n",
      "Epoch: 198500,Train Loss: 0.550144374370575,Time: 3250.1354417800903\n",
      "Epoch: 199000,Train Loss: 0.5501441955566406,Time: 3258.0478751659393\n",
      "Epoch: 199500,Train Loss: 0.5501440167427063,Time: 3266.3933091163635\n",
      "Epoch: 200000,Train Loss: 0.550143837928772,Time: 3274.3082423210144\n",
      "Epoch: 200500,Train Loss: 0.5501435995101929,Time: 3282.4578816890717\n",
      "Epoch: 201000,Train Loss: 0.5501433610916138,Time: 3290.6194293498993\n",
      "Epoch: 201500,Train Loss: 0.5501432418823242,Time: 3299.2681980133057\n",
      "Epoch: 202000,Train Loss: 0.5501430034637451,Time: 3307.1864473819733\n",
      "Epoch: 202500,Train Loss: 0.5501428246498108,Time: 3315.3535509109497\n",
      "Epoch: 203000,Train Loss: 0.5501425862312317,Time: 3323.473001718521\n",
      "Epoch: 203500,Train Loss: 0.5501424670219421,Time: 3331.632745742798\n",
      "Epoch: 204000,Train Loss: 0.550142228603363,Time: 3339.5858538150787\n",
      "Epoch: 204500,Train Loss: 0.5501421093940735,Time: 3347.8488302230835\n",
      "Epoch: 205000,Train Loss: 0.5501418709754944,Time: 3355.9513227939606\n",
      "Epoch: 205500,Train Loss: 0.5501416325569153,Time: 3364.288733482361\n",
      "Epoch: 206000,Train Loss: 0.5501415133476257,Time: 3372.242884159088\n",
      "Epoch: 206500,Train Loss: 0.5501413345336914,Time: 3380.4587228298187\n",
      "Epoch: 207000,Train Loss: 0.5501410961151123,Time: 3388.716997861862\n",
      "Epoch: 207500,Train Loss: 0.5501409769058228,Time: 3396.9129860401154\n",
      "Epoch: 208000,Train Loss: 0.5501407980918884,Time: 3404.8156831264496\n",
      "Epoch: 208500,Train Loss: 0.5501406788825989,Time: 3413.024747610092\n",
      "Epoch: 209000,Train Loss: 0.5501404404640198,Time: 3420.8691895008087\n",
      "Epoch: 209500,Train Loss: 0.5501402616500854,Time: 3429.059187889099\n",
      "Epoch: 210000,Train Loss: 0.5501400828361511,Time: 3437.111639738083\n",
      "Epoch: 210500,Train Loss: 0.5501399040222168,Time: 3445.124302625656\n",
      "Epoch: 211000,Train Loss: 0.5501397848129272,Time: 3452.9836463928223\n",
      "Epoch: 211500,Train Loss: 0.5501395463943481,Time: 3460.9797558784485\n",
      "Epoch: 212000,Train Loss: 0.5501394271850586,Time: 3469.175563097\n",
      "Epoch: 212500,Train Loss: 0.5501391887664795,Time: 3477.582743883133\n",
      "Epoch: 213000,Train Loss: 0.5501390695571899,Time: 3485.763091802597\n",
      "Epoch: 213500,Train Loss: 0.5501388311386108,Time: 3493.840772151947\n",
      "Epoch: 214000,Train Loss: 0.5501387119293213,Time: 3501.9842154979706\n",
      "Epoch: 214500,Train Loss: 0.5501385927200317,Time: 3510.194893836975\n",
      "Epoch: 215000,Train Loss: 0.5501383543014526,Time: 3518.3443455696106\n",
      "Epoch: 215500,Train Loss: 0.5501382350921631,Time: 3526.4870793819427\n",
      "Epoch: 216000,Train Loss: 0.5501381158828735,Time: 3534.5886056423187\n",
      "Epoch: 216500,Train Loss: 0.5501378178596497,Time: 3542.7538211345673\n",
      "Epoch: 217000,Train Loss: 0.5501376986503601,Time: 3551.066742181778\n",
      "Epoch: 217500,Train Loss: 0.5501375794410706,Time: 3559.4068093299866\n",
      "Epoch: 218000,Train Loss: 0.5501373410224915,Time: 3567.356900215149\n",
      "Epoch: 218500,Train Loss: 0.5501372218132019,Time: 3575.4385647773743\n",
      "Epoch: 219000,Train Loss: 0.5501370429992676,Time: 3583.9057564735413\n",
      "Epoch: 219500,Train Loss: 0.550136923789978,Time: 3592.382763147354\n",
      "Epoch: 220000,Train Loss: 0.5501367449760437,Time: 3600.216644525528\n",
      "Epoch: 220500,Train Loss: 0.5501366257667542,Time: 3608.3214643001556\n",
      "Epoch: 221000,Train Loss: 0.5501365065574646,Time: 3616.4356656074524\n",
      "Epoch: 221500,Train Loss: 0.5501363277435303,Time: 3624.751336812973\n",
      "Epoch: 222000,Train Loss: 0.550136148929596,Time: 3633.2132551670074\n",
      "Epoch: 222500,Train Loss: 0.5501360297203064,Time: 3641.5008635520935\n",
      "Epoch: 223000,Train Loss: 0.5501358509063721,Time: 3649.805775165558\n",
      "Epoch: 223500,Train Loss: 0.5501357316970825,Time: 3657.9459567070007\n",
      "Epoch: 224000,Train Loss: 0.5501355528831482,Time: 3665.954927444458\n",
      "Epoch: 224500,Train Loss: 0.5501353740692139,Time: 3674.622750043869\n",
      "Epoch: 225000,Train Loss: 0.5501351952552795,Time: 3682.6506690979004\n",
      "Epoch: 225500,Train Loss: 0.55013507604599,Time: 3690.7082164287567\n",
      "Epoch: 226000,Train Loss: 0.5501348972320557,Time: 3698.711570739746\n",
      "Epoch: 226500,Train Loss: 0.5501347780227661,Time: 3706.8915617465973\n",
      "Epoch: 227000,Train Loss: 0.5501346588134766,Time: 3714.773890018463\n",
      "Epoch: 227500,Train Loss: 0.5501344203948975,Time: 3722.731636285782\n",
      "Epoch: 228000,Train Loss: 0.5501343607902527,Time: 3730.8111522197723\n",
      "Epoch: 228500,Train Loss: 0.5501341819763184,Time: 3738.8866860866547\n",
      "Epoch: 229000,Train Loss: 0.5501340627670288,Time: 3746.8755028247833\n",
      "Epoch: 229500,Train Loss: 0.5501338839530945,Time: 3755.201896429062\n",
      "Epoch: 230000,Train Loss: 0.5501337647438049,Time: 3763.258675813675\n",
      "Epoch: 230500,Train Loss: 0.5501336455345154,Time: 3771.3090496063232\n",
      "Epoch: 231000,Train Loss: 0.550133466720581,Time: 3779.5036058425903\n",
      "Epoch: 231500,Train Loss: 0.5501333475112915,Time: 3787.609117746353\n",
      "Epoch: 232000,Train Loss: 0.5501331686973572,Time: 3795.7029886245728\n",
      "Epoch: 232500,Train Loss: 0.5501330494880676,Time: 3803.9113261699677\n",
      "Epoch: 233000,Train Loss: 0.5501328110694885,Time: 3811.87433552742\n",
      "Epoch: 233500,Train Loss: 0.550132691860199,Time: 3820.139108657837\n",
      "Epoch: 234000,Train Loss: 0.5501325726509094,Time: 3828.25310254097\n",
      "Epoch: 234500,Train Loss: 0.5501324534416199,Time: 3836.370966911316\n",
      "Epoch: 235000,Train Loss: 0.5501322746276855,Time: 3844.557514667511\n",
      "Epoch: 235500,Train Loss: 0.550132155418396,Time: 3853.0693917274475\n",
      "Epoch: 236000,Train Loss: 0.5501319766044617,Time: 3861.127520799637\n",
      "Epoch: 236500,Train Loss: 0.5501318573951721,Time: 3869.514488697052\n",
      "Epoch: 237000,Train Loss: 0.5501317381858826,Time: 3877.454475402832\n",
      "Epoch: 237500,Train Loss: 0.550131618976593,Time: 3885.6097598075867\n",
      "Epoch: 238000,Train Loss: 0.5501314997673035,Time: 3893.555730819702\n",
      "Epoch: 238500,Train Loss: 0.5501313209533691,Time: 3902.067312479019\n",
      "Epoch: 239000,Train Loss: 0.5501312613487244,Time: 3910.0981760025024\n",
      "Epoch: 239500,Train Loss: 0.5501311421394348,Time: 3918.5879826545715\n",
      "Epoch: 240000,Train Loss: 0.5501309633255005,Time: 3926.671125650406\n",
      "Epoch: 240500,Train Loss: 0.5501308441162109,Time: 3935.1811645030975\n",
      "Epoch: 241000,Train Loss: 0.5501307249069214,Time: 3943.1980044841766\n",
      "Epoch: 241500,Train Loss: 0.5501305460929871,Time: 3951.1678977012634\n",
      "Epoch: 242000,Train Loss: 0.5501304268836975,Time: 3959.4676785469055\n",
      "Epoch: 242500,Train Loss: 0.5501302480697632,Time: 3967.4983043670654\n",
      "Epoch: 243000,Train Loss: 0.5501301288604736,Time: 3975.6465513706207\n",
      "Epoch: 243500,Train Loss: 0.5501300096511841,Time: 3983.773524045944\n",
      "Epoch: 244000,Train Loss: 0.5501298904418945,Time: 3991.8798563480377\n",
      "Epoch: 244500,Train Loss: 0.550129771232605,Time: 3999.8208298683167\n",
      "Epoch: 245000,Train Loss: 0.5501296520233154,Time: 4007.9987530708313\n",
      "Epoch: 245500,Train Loss: 0.5501295328140259,Time: 4016.016020298004\n",
      "Epoch: 246000,Train Loss: 0.5501294136047363,Time: 4024.0762345790863\n",
      "Epoch: 246500,Train Loss: 0.5501292943954468,Time: 4032.1010150909424\n",
      "Epoch: 247000,Train Loss: 0.5501291155815125,Time: 4040.3154792785645\n",
      "Epoch: 247500,Train Loss: 0.5501290559768677,Time: 4048.3284029960632\n",
      "Epoch: 248000,Train Loss: 0.5501289367675781,Time: 4056.541336774826\n",
      "Epoch: 248500,Train Loss: 0.5501287579536438,Time: 4065.0462551116943\n",
      "Epoch: 249000,Train Loss: 0.5501286387443542,Time: 4073.623373270035\n",
      "Epoch: 249500,Train Loss: 0.5501285791397095,Time: 4081.7646386623383\n",
      "Epoch: 250000,Train Loss: 0.5501284599304199,Time: 4089.778562784195\n",
      "Epoch: 250500,Train Loss: 0.5501282215118408,Time: 4097.773691654205\n",
      "Epoch: 251000,Train Loss: 0.550128161907196,Time: 4105.991645812988\n",
      "Epoch: 251500,Train Loss: 0.5501281023025513,Time: 4114.03683423996\n",
      "Epoch: 252000,Train Loss: 0.5501279234886169,Time: 4122.148725509644\n",
      "Epoch: 252500,Train Loss: 0.5501277446746826,Time: 4130.61977148056\n",
      "Epoch: 253000,Train Loss: 0.5501276254653931,Time: 4138.850759983063\n",
      "Epoch: 253500,Train Loss: 0.5501275658607483,Time: 4146.73525094986\n",
      "Epoch: 254000,Train Loss: 0.5501274466514587,Time: 4154.813384056091\n",
      "Epoch: 254500,Train Loss: 0.5501272678375244,Time: 4162.857954263687\n",
      "Epoch: 255000,Train Loss: 0.5501271486282349,Time: 4171.175755977631\n",
      "Epoch: 255500,Train Loss: 0.5501270294189453,Time: 4179.195110321045\n",
      "Epoch: 256000,Train Loss: 0.5501269102096558,Time: 4187.347593545914\n",
      "Epoch: 256500,Train Loss: 0.5501267910003662,Time: 4195.32848572731\n",
      "Epoch: 257000,Train Loss: 0.5501267313957214,Time: 4203.365151166916\n",
      "Epoch: 257500,Train Loss: 0.5501266121864319,Time: 4211.51443362236\n",
      "Epoch: 258000,Train Loss: 0.5501264929771423,Time: 4219.627511501312\n",
      "Epoch: 258500,Train Loss: 0.550126314163208,Time: 4227.5745503902435\n",
      "Epoch: 259000,Train Loss: 0.5501262545585632,Time: 4236.083250761032\n",
      "Epoch: 259500,Train Loss: 0.5501261353492737,Time: 4244.030098199844\n",
      "Epoch: 260000,Train Loss: 0.5501260161399841,Time: 4252.598121166229\n",
      "Epoch: 260500,Train Loss: 0.5501258969306946,Time: 4260.61515712738\n",
      "Epoch: 261000,Train Loss: 0.550125777721405,Time: 4268.686388969421\n",
      "Epoch: 261500,Train Loss: 0.5501256585121155,Time: 4276.938658237457\n",
      "Epoch: 262000,Train Loss: 0.5501255393028259,Time: 4285.152992010117\n",
      "Epoch: 262500,Train Loss: 0.5501254200935364,Time: 4293.146422863007\n",
      "Epoch: 263000,Train Loss: 0.5501253604888916,Time: 4301.513378858566\n",
      "Epoch: 263500,Train Loss: 0.5501251220703125,Time: 4309.583780527115\n",
      "Epoch: 264000,Train Loss: 0.5501250624656677,Time: 4317.899992465973\n",
      "Epoch: 264500,Train Loss: 0.5501249432563782,Time: 4325.936217546463\n",
      "Epoch: 265000,Train Loss: 0.5501248836517334,Time: 4334.129670143127\n",
      "Epoch: 265500,Train Loss: 0.5501247644424438,Time: 4342.101929664612\n",
      "Epoch: 266000,Train Loss: 0.5501246452331543,Time: 4350.552136421204\n",
      "Epoch: 266500,Train Loss: 0.55012446641922,Time: 4358.841616868973\n",
      "Epoch: 267000,Train Loss: 0.5501243472099304,Time: 4366.97873544693\n",
      "Epoch: 267500,Train Loss: 0.5501242876052856,Time: 4375.452403306961\n",
      "Epoch: 268000,Train Loss: 0.5501241683959961,Time: 4383.78307890892\n",
      "Epoch: 268500,Train Loss: 0.5501241087913513,Time: 4391.69248509407\n",
      "Epoch: 269000,Train Loss: 0.5501239895820618,Time: 4399.815535783768\n",
      "Epoch: 269500,Train Loss: 0.550123929977417,Time: 4407.777584314346\n",
      "Epoch: 270000,Train Loss: 0.5501238107681274,Time: 4415.875828027725\n",
      "Epoch: 270500,Train Loss: 0.5501236915588379,Time: 4423.830062866211\n",
      "Epoch: 271000,Train Loss: 0.5501235723495483,Time: 4431.778121948242\n",
      "Epoch: 271500,Train Loss: 0.5501234531402588,Time: 4439.877161741257\n",
      "Epoch: 272000,Train Loss: 0.5501233339309692,Time: 4448.055336713791\n",
      "Epoch: 272500,Train Loss: 0.5501233339309692,Time: 4455.965877056122\n",
      "Epoch: 273000,Train Loss: 0.5501231551170349,Time: 4464.351504325867\n",
      "Epoch: 273500,Train Loss: 0.5501230359077454,Time: 4472.311330795288\n",
      "Epoch: 274000,Train Loss: 0.5501229763031006,Time: 4480.463221073151\n",
      "Epoch: 274500,Train Loss: 0.550122857093811,Time: 4488.253108739853\n",
      "Epoch: 275000,Train Loss: 0.5501226782798767,Time: 4496.550225496292\n",
      "Epoch: 275500,Train Loss: 0.5501226186752319,Time: 4504.522181034088\n",
      "Epoch: 276000,Train Loss: 0.5501225590705872,Time: 4512.590525865555\n",
      "Epoch: 276500,Train Loss: 0.5501224398612976,Time: 4520.704118013382\n",
      "Epoch: 277000,Train Loss: 0.5501223206520081,Time: 4528.937296390533\n",
      "Epoch: 277500,Train Loss: 0.5501222610473633,Time: 4536.953746557236\n",
      "Epoch: 278000,Train Loss: 0.5501221418380737,Time: 4545.063622236252\n",
      "Epoch: 278500,Train Loss: 0.550122082233429,Time: 4553.058036565781\n",
      "Epoch: 279000,Train Loss: 0.5501219630241394,Time: 4561.309237003326\n",
      "Epoch: 279500,Train Loss: 0.5501218438148499,Time: 4569.446913719177\n",
      "Epoch: 280000,Train Loss: 0.5501217842102051,Time: 4577.596081972122\n",
      "Epoch: 280500,Train Loss: 0.5501216650009155,Time: 4585.598912239075\n",
      "Epoch: 281000,Train Loss: 0.5501216053962708,Time: 4593.925722122192\n",
      "Epoch: 281500,Train Loss: 0.5501214861869812,Time: 4601.85951590538\n",
      "Epoch: 282000,Train Loss: 0.5501213669776917,Time: 4609.8722603321075\n",
      "Epoch: 282500,Train Loss: 0.5501212477684021,Time: 4617.76550579071\n",
      "Epoch: 283000,Train Loss: 0.5501211285591125,Time: 4625.793339967728\n",
      "Epoch: 283500,Train Loss: 0.5501211285591125,Time: 4633.880153656006\n",
      "Epoch: 284000,Train Loss: 0.550121009349823,Time: 4642.33792424202\n",
      "Epoch: 284500,Train Loss: 0.5501208901405334,Time: 4650.151193618774\n",
      "Epoch: 285000,Train Loss: 0.5501207709312439,Time: 4658.152245998383\n",
      "Epoch: 285500,Train Loss: 0.5501207709312439,Time: 4666.192686080933\n",
      "Epoch: 286000,Train Loss: 0.5501206517219543,Time: 4674.627558231354\n",
      "Epoch: 286500,Train Loss: 0.5501205325126648,Time: 4682.6489152908325\n",
      "Epoch: 287000,Train Loss: 0.5501204133033752,Time: 4691.134630918503\n",
      "Epoch: 287500,Train Loss: 0.5501204133033752,Time: 4699.447718143463\n",
      "Epoch: 288000,Train Loss: 0.5501202940940857,Time: 4707.5191304683685\n",
      "Epoch: 288500,Train Loss: 0.5501201152801514,Time: 4715.376714706421\n",
      "Epoch: 289000,Train Loss: 0.5501200556755066,Time: 4723.360665798187\n",
      "Epoch: 289500,Train Loss: 0.5501199960708618,Time: 4731.259456396103\n",
      "Epoch: 290000,Train Loss: 0.5501198768615723,Time: 4739.31595158577\n",
      "Epoch: 290500,Train Loss: 0.5501197576522827,Time: 4747.385569572449\n",
      "Epoch: 291000,Train Loss: 0.5501197576522827,Time: 4755.45908498764\n",
      "Epoch: 291500,Train Loss: 0.5501196384429932,Time: 4763.324566364288\n",
      "Epoch: 292000,Train Loss: 0.5501195192337036,Time: 4771.286609649658\n",
      "Epoch: 292500,Train Loss: 0.5501195192337036,Time: 4779.856072902679\n",
      "Epoch: 293000,Train Loss: 0.5501194000244141,Time: 4787.92714881897\n",
      "Epoch: 293500,Train Loss: 0.5501192808151245,Time: 4795.924528360367\n",
      "Epoch: 294000,Train Loss: 0.550119161605835,Time: 4804.165054321289\n",
      "Epoch: 294500,Train Loss: 0.550119161605835,Time: 4812.19556760788\n",
      "Epoch: 295000,Train Loss: 0.5501190423965454,Time: 4820.225212097168\n",
      "Epoch: 295500,Train Loss: 0.5501189231872559,Time: 4828.496898889542\n",
      "Epoch: 296000,Train Loss: 0.5501188635826111,Time: 4836.897427082062\n",
      "Epoch: 296500,Train Loss: 0.5501187443733215,Time: 4844.822499036789\n",
      "Epoch: 297000,Train Loss: 0.5501186847686768,Time: 4852.937557697296\n",
      "Epoch: 297500,Train Loss: 0.5501186847686768,Time: 4861.094863653183\n",
      "Epoch: 298000,Train Loss: 0.5501185655593872,Time: 4869.357067108154\n",
      "Epoch: 298500,Train Loss: 0.5501184463500977,Time: 4877.376431465149\n",
      "Epoch: 299000,Train Loss: 0.5501183271408081,Time: 4885.512756347656\n",
      "Epoch: 299500,Train Loss: 0.5501183271408081,Time: 4893.515337228775\n",
      "Epoch: 300000,Train Loss: 0.5501182079315186,Time: 4901.677001237869\n",
      "Epoch: 300500,Train Loss: 0.550118088722229,Time: 4909.717853546143\n",
      "Epoch: 301000,Train Loss: 0.5501180291175842,Time: 4917.7223472595215\n",
      "Epoch: 301500,Train Loss: 0.5501179695129395,Time: 4925.575941562653\n",
      "Epoch: 302000,Train Loss: 0.5501178503036499,Time: 4933.706051826477\n",
      "Epoch: 302500,Train Loss: 0.5501178503036499,Time: 4941.872642040253\n",
      "Epoch: 303000,Train Loss: 0.5501176118850708,Time: 4950.217524051666\n",
      "Epoch: 303500,Train Loss: 0.550117552280426,Time: 4958.315265655518\n",
      "Epoch: 304000,Train Loss: 0.5501174926757812,Time: 4966.388677358627\n",
      "Epoch: 304500,Train Loss: 0.5501174330711365,Time: 4974.421361684799\n",
      "Epoch: 305000,Train Loss: 0.5501173138618469,Time: 4982.663893699646\n",
      "Epoch: 305500,Train Loss: 0.5501172542572021,Time: 4990.510702848434\n",
      "Epoch: 306000,Train Loss: 0.5501171946525574,Time: 4998.851356744766\n",
      "Epoch: 306500,Train Loss: 0.5501170754432678,Time: 5006.994385957718\n",
      "Epoch: 307000,Train Loss: 0.550117015838623,Time: 5015.166578292847\n",
      "Epoch: 307500,Train Loss: 0.5501169562339783,Time: 5023.176454782486\n",
      "Epoch: 308000,Train Loss: 0.5501168966293335,Time: 5031.37925696373\n",
      "Epoch: 308500,Train Loss: 0.550116777420044,Time: 5039.70302939415\n",
      "Epoch: 309000,Train Loss: 0.5501167178153992,Time: 5047.938632011414\n",
      "Epoch: 309500,Train Loss: 0.5501165986061096,Time: 5055.87361574173\n",
      "Epoch: 310000,Train Loss: 0.5501165390014648,Time: 5063.965754747391\n",
      "Epoch: 310500,Train Loss: 0.5501164793968201,Time: 5072.34557390213\n",
      "Epoch: 311000,Train Loss: 0.5501163601875305,Time: 5080.653862953186\n",
      "Epoch: 311500,Train Loss: 0.5501163601875305,Time: 5088.736322402954\n",
      "Epoch: 312000,Train Loss: 0.550116240978241,Time: 5097.657767772675\n",
      "Epoch: 312500,Train Loss: 0.5501161217689514,Time: 5105.8499257564545\n",
      "Epoch: 313000,Train Loss: 0.5501160621643066,Time: 5113.898747444153\n",
      "Epoch: 313500,Train Loss: 0.5501160025596619,Time: 5122.00607419014\n",
      "Epoch: 314000,Train Loss: 0.5501158833503723,Time: 5130.0839240550995\n",
      "Epoch: 314500,Train Loss: 0.5501158833503723,Time: 5138.3254935741425\n",
      "Epoch: 315000,Train Loss: 0.5501157641410828,Time: 5146.3243107795715\n",
      "Epoch: 315500,Train Loss: 0.550115704536438,Time: 5154.724752664566\n",
      "Epoch: 316000,Train Loss: 0.5501155853271484,Time: 5162.69323015213\n",
      "Epoch: 316500,Train Loss: 0.5501155257225037,Time: 5170.689204454422\n",
      "Epoch: 317000,Train Loss: 0.5501155257225037,Time: 5178.5752539634705\n",
      "Epoch: 317500,Train Loss: 0.5501154065132141,Time: 5186.797000646591\n",
      "Epoch: 318000,Train Loss: 0.5501153469085693,Time: 5194.877903461456\n",
      "Epoch: 318500,Train Loss: 0.5501152873039246,Time: 5202.88084936142\n",
      "Epoch: 319000,Train Loss: 0.550115168094635,Time: 5210.692257165909\n",
      "Epoch: 319500,Train Loss: 0.5501150488853455,Time: 5219.080434799194\n",
      "Epoch: 320000,Train Loss: 0.5501149892807007,Time: 5227.036100625992\n",
      "Epoch: 320500,Train Loss: 0.5501149892807007,Time: 5235.361644268036\n",
      "Epoch: 321000,Train Loss: 0.5501148700714111,Time: 5243.526448965073\n",
      "Epoch: 321500,Train Loss: 0.5501148104667664,Time: 5251.60636973381\n",
      "Epoch: 322000,Train Loss: 0.5501146912574768,Time: 5259.446990966797\n",
      "Epoch: 322500,Train Loss: 0.5501146912574768,Time: 5267.580594778061\n",
      "Epoch: 323000,Train Loss: 0.5501145720481873,Time: 5275.533330202103\n",
      "Epoch: 323500,Train Loss: 0.5501145124435425,Time: 5283.750372886658\n",
      "Epoch: 324000,Train Loss: 0.5501144528388977,Time: 5291.8661460876465\n",
      "Epoch: 324500,Train Loss: 0.5501143336296082,Time: 5299.846444368362\n",
      "Epoch: 325000,Train Loss: 0.5501142740249634,Time: 5307.857310295105\n",
      "Epoch: 325500,Train Loss: 0.5501141548156738,Time: 5315.860048294067\n",
      "Epoch: 326000,Train Loss: 0.5501141548156738,Time: 5323.853096008301\n",
      "Epoch: 326500,Train Loss: 0.5501140356063843,Time: 5332.3230946063995\n",
      "Epoch: 327000,Train Loss: 0.5501140356063843,Time: 5340.308316230774\n",
      "Epoch: 327500,Train Loss: 0.5501139163970947,Time: 5348.458142280579\n",
      "Epoch: 328000,Train Loss: 0.5501139163970947,Time: 5356.494512319565\n",
      "Epoch: 328500,Train Loss: 0.5501137971878052,Time: 5364.629752397537\n",
      "Epoch: 329000,Train Loss: 0.5501137375831604,Time: 5372.4914565086365\n",
      "Epoch: 329500,Train Loss: 0.5501136779785156,Time: 5380.717156887054\n",
      "Epoch: 330000,Train Loss: 0.5501136183738708,Time: 5388.6403884887695\n",
      "Epoch: 330500,Train Loss: 0.5501134991645813,Time: 5396.723183631897\n",
      "Epoch: 331000,Train Loss: 0.5501134991645813,Time: 5404.523992538452\n",
      "Epoch: 331500,Train Loss: 0.5501133799552917,Time: 5412.746914148331\n",
      "Epoch: 332000,Train Loss: 0.550113320350647,Time: 5421.172439813614\n",
      "Epoch: 332500,Train Loss: 0.5501132607460022,Time: 5429.484691858292\n",
      "Epoch: 333000,Train Loss: 0.5501132011413574,Time: 5437.338488817215\n",
      "Epoch: 333500,Train Loss: 0.5501130819320679,Time: 5445.406126737595\n",
      "Epoch: 334000,Train Loss: 0.5501130819320679,Time: 5453.956027269363\n",
      "Epoch: 334500,Train Loss: 0.5501129627227783,Time: 5462.2221326828\n",
      "Epoch: 335000,Train Loss: 0.5501129031181335,Time: 5470.398587465286\n",
      "Epoch: 335500,Train Loss: 0.5501128435134888,Time: 5478.652873516083\n",
      "Epoch: 336000,Train Loss: 0.550112783908844,Time: 5486.61403465271\n",
      "Epoch: 336500,Train Loss: 0.5501127243041992,Time: 5494.594311952591\n",
      "Epoch: 337000,Train Loss: 0.5501125454902649,Time: 5502.444924592972\n",
      "Epoch: 337500,Train Loss: 0.5501125454902649,Time: 5510.577144861221\n",
      "Epoch: 338000,Train Loss: 0.5501124858856201,Time: 5518.506196260452\n",
      "Epoch: 338500,Train Loss: 0.5501124262809753,Time: 5526.698933362961\n",
      "Epoch: 339000,Train Loss: 0.5501123666763306,Time: 5534.600889444351\n",
      "Epoch: 339500,Train Loss: 0.550112247467041,Time: 5542.763800621033\n",
      "Epoch: 340000,Train Loss: 0.5501121878623962,Time: 5550.76075553894\n",
      "Epoch: 340500,Train Loss: 0.5501121282577515,Time: 5558.884665727615\n",
      "Epoch: 341000,Train Loss: 0.5501120686531067,Time: 5567.106323957443\n",
      "Epoch: 341500,Train Loss: 0.5501120090484619,Time: 5575.367936849594\n",
      "Epoch: 342000,Train Loss: 0.5501119494438171,Time: 5583.503941059113\n",
      "Epoch: 342500,Train Loss: 0.5501118302345276,Time: 5591.477316856384\n",
      "Epoch: 343000,Train Loss: 0.5501118302345276,Time: 5599.75625038147\n",
      "Epoch: 343500,Train Loss: 0.550111711025238,Time: 5607.95548582077\n",
      "Epoch: 344000,Train Loss: 0.550111711025238,Time: 5616.286699295044\n",
      "Epoch: 344500,Train Loss: 0.5501115918159485,Time: 5624.52939248085\n",
      "Epoch: 345000,Train Loss: 0.5501115918159485,Time: 5632.569214582443\n",
      "Epoch: 345500,Train Loss: 0.5501114726066589,Time: 5640.747339963913\n",
      "Epoch: 346000,Train Loss: 0.5501114130020142,Time: 5648.6251702308655\n",
      "Epoch: 346500,Train Loss: 0.5501113533973694,Time: 5657.136150836945\n",
      "Epoch: 347000,Train Loss: 0.5501112937927246,Time: 5665.209229230881\n",
      "Epoch: 347500,Train Loss: 0.5501112341880798,Time: 5673.511824607849\n",
      "Epoch: 348000,Train Loss: 0.5501111745834351,Time: 5681.485889196396\n",
      "Epoch: 348500,Train Loss: 0.5501110553741455,Time: 5689.742581605911\n",
      "Epoch: 349000,Train Loss: 0.5501110553741455,Time: 5697.715790271759\n",
      "Epoch: 349500,Train Loss: 0.550110936164856,Time: 5705.868015527725\n",
      "Epoch: 350000,Train Loss: 0.550110936164856,Time: 5713.878581762314\n",
      "Epoch: 350500,Train Loss: 0.5501108765602112,Time: 5722.148583889008\n",
      "Epoch: 351000,Train Loss: 0.5501108169555664,Time: 5730.178077459335\n",
      "Epoch: 351500,Train Loss: 0.5501106977462769,Time: 5738.3298552036285\n",
      "Epoch: 352000,Train Loss: 0.5501106381416321,Time: 5746.863938808441\n",
      "Epoch: 352500,Train Loss: 0.5501106381416321,Time: 5755.3396208286285\n",
      "Epoch: 353000,Train Loss: 0.5501105189323425,Time: 5763.3486495018005\n",
      "Epoch: 353500,Train Loss: 0.5501105189323425,Time: 5771.655340671539\n",
      "Epoch: 354000,Train Loss: 0.550110399723053,Time: 5779.939776182175\n",
      "Epoch: 354500,Train Loss: 0.550110399723053,Time: 5788.330070734024\n",
      "Epoch: 355000,Train Loss: 0.5501102805137634,Time: 5796.272733926773\n",
      "Epoch: 355500,Train Loss: 0.5501102209091187,Time: 5804.7805342674255\n",
      "Epoch: 356000,Train Loss: 0.5501101613044739,Time: 5812.764148712158\n",
      "Epoch: 356500,Train Loss: 0.5501101613044739,Time: 5820.850345611572\n",
      "Epoch: 357000,Train Loss: 0.5501099824905396,Time: 5828.9138007164\n",
      "Epoch: 357500,Train Loss: 0.5501099228858948,Time: 5836.996190547943\n",
      "Epoch: 358000,Train Loss: 0.55010986328125,Time: 5845.055380105972\n",
      "Epoch: 358500,Train Loss: 0.55010986328125,Time: 5853.373531341553\n",
      "Epoch: 359000,Train Loss: 0.5501097440719604,Time: 5861.517307758331\n",
      "Epoch: 359500,Train Loss: 0.5501097440719604,Time: 5869.584095478058\n",
      "Epoch: 360000,Train Loss: 0.5501096248626709,Time: 5877.695706129074\n",
      "Epoch: 360500,Train Loss: 0.5501096248626709,Time: 5885.9595646858215\n",
      "Epoch: 361000,Train Loss: 0.5501095056533813,Time: 5893.9224145412445\n",
      "Epoch: 361500,Train Loss: 0.5501095056533813,Time: 5902.039381027222\n",
      "Epoch: 362000,Train Loss: 0.5501094460487366,Time: 5910.114525556564\n",
      "Epoch: 362500,Train Loss: 0.5501093864440918,Time: 5918.539655208588\n",
      "Epoch: 363000,Train Loss: 0.550109326839447,Time: 5926.463994503021\n",
      "Epoch: 363500,Train Loss: 0.5501092672348022,Time: 5934.516128063202\n",
      "Epoch: 364000,Train Loss: 0.5501092076301575,Time: 5942.490679502487\n",
      "Epoch: 364500,Train Loss: 0.5501091480255127,Time: 5950.567911863327\n",
      "Epoch: 365000,Train Loss: 0.5501090884208679,Time: 5958.546833515167\n",
      "Epoch: 365500,Train Loss: 0.5501090884208679,Time: 5966.558319807053\n",
      "Epoch: 366000,Train Loss: 0.5501089692115784,Time: 5974.447932958603\n",
      "Epoch: 366500,Train Loss: 0.5501089096069336,Time: 5982.783353090286\n",
      "Epoch: 367000,Train Loss: 0.5501089096069336,Time: 5990.8775107860565\n",
      "Epoch: 367500,Train Loss: 0.5501088500022888,Time: 5999.3789348602295\n",
      "Epoch: 368000,Train Loss: 0.550108790397644,Time: 6007.188824176788\n",
      "Epoch: 368500,Train Loss: 0.5501086711883545,Time: 6015.419214725494\n",
      "Epoch: 369000,Train Loss: 0.5501086711883545,Time: 6023.39258646965\n",
      "Epoch: 369500,Train Loss: 0.5501085519790649,Time: 6031.720009565353\n",
      "Epoch: 370000,Train Loss: 0.5501085519790649,Time: 6039.948387384415\n",
      "Epoch: 370500,Train Loss: 0.5501084327697754,Time: 6048.391005039215\n",
      "Epoch: 371000,Train Loss: 0.5501084327697754,Time: 6056.411790847778\n",
      "Epoch: 371500,Train Loss: 0.5501083731651306,Time: 6064.598710775375\n",
      "Epoch: 372000,Train Loss: 0.5501083135604858,Time: 6072.598480463028\n",
      "Epoch: 372500,Train Loss: 0.5501082539558411,Time: 6080.888712406158\n",
      "Epoch: 373000,Train Loss: 0.5501081943511963,Time: 6088.939212560654\n",
      "Epoch: 373500,Train Loss: 0.5501081347465515,Time: 6097.028272867203\n",
      "Epoch: 374000,Train Loss: 0.5501080751419067,Time: 6105.174759864807\n",
      "Epoch: 374500,Train Loss: 0.5501080751419067,Time: 6113.203444480896\n",
      "Epoch: 375000,Train Loss: 0.550108015537262,Time: 6121.25323843956\n",
      "Epoch: 375500,Train Loss: 0.5501079559326172,Time: 6129.912487745285\n",
      "Epoch: 376000,Train Loss: 0.5501078963279724,Time: 6137.803628921509\n",
      "Epoch: 376500,Train Loss: 0.5501078367233276,Time: 6146.460599184036\n",
      "Epoch: 377000,Train Loss: 0.5501077175140381,Time: 6154.482098579407\n",
      "Epoch: 377500,Train Loss: 0.5501077175140381,Time: 6162.575781822205\n",
      "Epoch: 378000,Train Loss: 0.5501075983047485,Time: 6170.613359212875\n",
      "Epoch: 378500,Train Loss: 0.5501075983047485,Time: 6178.979492425919\n",
      "Epoch: 379000,Train Loss: 0.5501075387001038,Time: 6186.929825305939\n",
      "Epoch: 379500,Train Loss: 0.5501074194908142,Time: 6195.618040800095\n",
      "Epoch: 380000,Train Loss: 0.5501074194908142,Time: 6203.820058345795\n",
      "Epoch: 380500,Train Loss: 0.5501073002815247,Time: 6212.155456542969\n",
      "Epoch: 381000,Train Loss: 0.5501073002815247,Time: 6219.971675395966\n",
      "Epoch: 381500,Train Loss: 0.5501072406768799,Time: 6228.108272314072\n",
      "Epoch: 382000,Train Loss: 0.5501071810722351,Time: 6236.283421993256\n",
      "Epoch: 382500,Train Loss: 0.5501071214675903,Time: 6244.477321386337\n",
      "Epoch: 383000,Train Loss: 0.5501070618629456,Time: 6252.434020280838\n",
      "Epoch: 383500,Train Loss: 0.5501070618629456,Time: 6260.678119659424\n",
      "Epoch: 384000,Train Loss: 0.5501070022583008,Time: 6268.521257162094\n",
      "Epoch: 384500,Train Loss: 0.550106942653656,Time: 6276.520323753357\n",
      "Epoch: 385000,Train Loss: 0.5501068830490112,Time: 6284.85914683342\n",
      "Epoch: 385500,Train Loss: 0.5501068234443665,Time: 6293.030215501785\n",
      "Epoch: 386000,Train Loss: 0.5501067638397217,Time: 6301.346609354019\n",
      "Epoch: 386500,Train Loss: 0.5501067042350769,Time: 6309.833575487137\n",
      "Epoch: 387000,Train Loss: 0.5501067042350769,Time: 6318.161112546921\n",
      "Epoch: 387500,Train Loss: 0.5501065850257874,Time: 6326.5355858802795\n",
      "Epoch: 388000,Train Loss: 0.5501065850257874,Time: 6335.217396020889\n",
      "Epoch: 388500,Train Loss: 0.5501064658164978,Time: 6343.236606836319\n",
      "Epoch: 389000,Train Loss: 0.5501064658164978,Time: 6351.291680574417\n",
      "Epoch: 389500,Train Loss: 0.5501064658164978,Time: 6359.224795818329\n",
      "Epoch: 390000,Train Loss: 0.5501063466072083,Time: 6367.736051082611\n",
      "Epoch: 390500,Train Loss: 0.5501063466072083,Time: 6375.758674383163\n",
      "Epoch: 391000,Train Loss: 0.5501062870025635,Time: 6383.903388977051\n",
      "Epoch: 391500,Train Loss: 0.5501062273979187,Time: 6391.753441333771\n",
      "Epoch: 392000,Train Loss: 0.5501062273979187,Time: 6399.895794391632\n",
      "Epoch: 392500,Train Loss: 0.5501061677932739,Time: 6408.05726313591\n",
      "Epoch: 393000,Train Loss: 0.5501061081886292,Time: 6416.471541643143\n",
      "Epoch: 393500,Train Loss: 0.5501061081886292,Time: 6424.31633067131\n",
      "Epoch: 394000,Train Loss: 0.5501059889793396,Time: 6432.837473869324\n",
      "Epoch: 394500,Train Loss: 0.5501059889793396,Time: 6441.09908747673\n",
      "Epoch: 395000,Train Loss: 0.5501059293746948,Time: 6449.249363183975\n",
      "Epoch: 395500,Train Loss: 0.55010586977005,Time: 6457.464200973511\n",
      "Epoch: 396000,Train Loss: 0.55010586977005,Time: 6465.677453994751\n",
      "Epoch: 396500,Train Loss: 0.5501057505607605,Time: 6473.825630664825\n",
      "Epoch: 397000,Train Loss: 0.5501057505607605,Time: 6482.5864334106445\n",
      "Epoch: 397500,Train Loss: 0.5501056909561157,Time: 6491.180572748184\n",
      "Epoch: 398000,Train Loss: 0.550105631351471,Time: 6499.65610909462\n",
      "Epoch: 398500,Train Loss: 0.550105631351471,Time: 6507.915738582611\n",
      "Epoch: 399000,Train Loss: 0.5501055717468262,Time: 6516.51499414444\n",
      "Epoch: 399500,Train Loss: 0.5501055121421814,Time: 6524.824835062027\n",
      "Epoch: 400000,Train Loss: 0.5501054525375366,Time: 6533.184642791748\n",
      "Epoch: 400500,Train Loss: 0.5501053929328918,Time: 6541.274767637253\n",
      "Epoch: 401000,Train Loss: 0.5501053929328918,Time: 6549.715670585632\n",
      "Epoch: 401500,Train Loss: 0.5501053333282471,Time: 6557.973608016968\n",
      "Epoch: 402000,Train Loss: 0.5501052737236023,Time: 6566.2401378154755\n",
      "Epoch: 402500,Train Loss: 0.5501052737236023,Time: 6574.363272428513\n",
      "Epoch: 403000,Train Loss: 0.5501051545143127,Time: 6582.549565792084\n",
      "Epoch: 403500,Train Loss: 0.5501051545143127,Time: 6590.718456268311\n",
      "Epoch: 404000,Train Loss: 0.5501051545143127,Time: 6599.15189576149\n",
      "Epoch: 404500,Train Loss: 0.5501050353050232,Time: 6607.202733755112\n",
      "Epoch: 405000,Train Loss: 0.5501050353050232,Time: 6615.230580806732\n",
      "Epoch: 405500,Train Loss: 0.5501048564910889,Time: 6623.26865029335\n",
      "Epoch: 406000,Train Loss: 0.5501048564910889,Time: 6631.642797708511\n",
      "Epoch: 406500,Train Loss: 0.5501047968864441,Time: 6639.618473291397\n",
      "Epoch: 407000,Train Loss: 0.5501047968864441,Time: 6647.743123292923\n",
      "Epoch: 407500,Train Loss: 0.5501047372817993,Time: 6655.831965923309\n",
      "Epoch: 408000,Train Loss: 0.5501046776771545,Time: 6663.941126823425\n",
      "Epoch: 408500,Train Loss: 0.5501046180725098,Time: 6671.863575935364\n",
      "Epoch: 409000,Train Loss: 0.5501046180725098,Time: 6679.960882425308\n",
      "Epoch: 409500,Train Loss: 0.5501046180725098,Time: 6687.837790489197\n",
      "Epoch: 410000,Train Loss: 0.5501044988632202,Time: 6695.9575715065\n",
      "Epoch: 410500,Train Loss: 0.5501044988632202,Time: 6704.053471326828\n",
      "Epoch: 411000,Train Loss: 0.5501044988632202,Time: 6712.22819519043\n",
      "Epoch: 411500,Train Loss: 0.5501043796539307,Time: 6720.518300294876\n",
      "Epoch: 412000,Train Loss: 0.5501043796539307,Time: 6729.090346574783\n",
      "Epoch: 412500,Train Loss: 0.5501042604446411,Time: 6736.9193279743195\n",
      "Epoch: 413000,Train Loss: 0.5501042604446411,Time: 6745.105867385864\n",
      "Epoch: 413500,Train Loss: 0.5501042604446411,Time: 6753.056522846222\n",
      "Epoch: 414000,Train Loss: 0.5501042008399963,Time: 6761.484412431717\n",
      "Epoch: 414500,Train Loss: 0.5501041412353516,Time: 6769.421621322632\n",
      "Epoch: 415000,Train Loss: 0.5501040816307068,Time: 6777.695139408112\n",
      "Epoch: 415500,Train Loss: 0.550104022026062,Time: 6786.184094905853\n",
      "Epoch: 416000,Train Loss: 0.550104022026062,Time: 6794.297003984451\n",
      "Epoch: 416500,Train Loss: 0.5501039624214172,Time: 6802.41991686821\n",
      "Epoch: 417000,Train Loss: 0.5501039028167725,Time: 6810.548603773117\n",
      "Epoch: 417500,Train Loss: 0.5501039028167725,Time: 6818.413928747177\n",
      "Epoch: 418000,Train Loss: 0.5501039028167725,Time: 6826.434916973114\n",
      "Epoch: 418500,Train Loss: 0.5501037836074829,Time: 6834.481123209\n",
      "Epoch: 419000,Train Loss: 0.5501037836074829,Time: 6842.553099393845\n",
      "Epoch: 419500,Train Loss: 0.5501036643981934,Time: 6850.390164136887\n",
      "Epoch: 420000,Train Loss: 0.5501036643981934,Time: 6858.549416542053\n",
      "Epoch: 420500,Train Loss: 0.5501036643981934,Time: 6866.555532217026\n",
      "Epoch: 421000,Train Loss: 0.5501036047935486,Time: 6874.741683721542\n",
      "Epoch: 421500,Train Loss: 0.5501035451889038,Time: 6882.649945259094\n",
      "Epoch: 422000,Train Loss: 0.550103485584259,Time: 6890.814141988754\n",
      "Epoch: 422500,Train Loss: 0.550103485584259,Time: 6898.873601198196\n",
      "Epoch: 423000,Train Loss: 0.5501034259796143,Time: 6907.068466186523\n",
      "Epoch: 423500,Train Loss: 0.5501033663749695,Time: 6915.257002830505\n",
      "Epoch: 424000,Train Loss: 0.5501033663749695,Time: 6923.425246953964\n",
      "Epoch: 424500,Train Loss: 0.5501033067703247,Time: 6931.614223718643\n",
      "Epoch: 425000,Train Loss: 0.5501033067703247,Time: 6939.798562049866\n",
      "Epoch: 425500,Train Loss: 0.5501031875610352,Time: 6947.867788314819\n",
      "Epoch: 426000,Train Loss: 0.5501031875610352,Time: 6956.012941837311\n",
      "Epoch: 426500,Train Loss: 0.5501031875610352,Time: 6963.98685836792\n",
      "Epoch: 427000,Train Loss: 0.5501030683517456,Time: 6972.051499605179\n",
      "Epoch: 427500,Train Loss: 0.5501030683517456,Time: 6980.006750822067\n",
      "Epoch: 428000,Train Loss: 0.5501030683517456,Time: 6988.281634569168\n",
      "Epoch: 428500,Train Loss: 0.5501030087471008,Time: 6996.567223787308\n",
      "Epoch: 429000,Train Loss: 0.550102949142456,Time: 7004.872400999069\n",
      "Epoch: 429500,Train Loss: 0.5501028895378113,Time: 7012.760064601898\n",
      "Epoch: 430000,Train Loss: 0.5501028895378113,Time: 7020.741775035858\n",
      "Epoch: 430500,Train Loss: 0.5501028299331665,Time: 7028.60281252861\n",
      "Epoch: 431000,Train Loss: 0.5501028299331665,Time: 7036.645160913467\n",
      "Epoch: 431500,Train Loss: 0.5501028299331665,Time: 7044.614789962769\n",
      "Epoch: 432000,Train Loss: 0.550102710723877,Time: 7052.713988780975\n",
      "Epoch: 432500,Train Loss: 0.550102710723877,Time: 7060.7169597148895\n",
      "Epoch: 433000,Train Loss: 0.550102710723877,Time: 7069.067463874817\n",
      "Epoch: 433500,Train Loss: 0.5501025915145874,Time: 7077.300936698914\n",
      "Epoch: 434000,Train Loss: 0.5501025915145874,Time: 7085.597483873367\n",
      "Epoch: 434500,Train Loss: 0.5501024723052979,Time: 7093.5167598724365\n",
      "Epoch: 435000,Train Loss: 0.5501024723052979,Time: 7101.523188591003\n",
      "Epoch: 435500,Train Loss: 0.5501024723052979,Time: 7109.375846862793\n",
      "Epoch: 436000,Train Loss: 0.5501023530960083,Time: 7117.496215105057\n",
      "Epoch: 436500,Train Loss: 0.5501023530960083,Time: 7125.395001411438\n",
      "Epoch: 437000,Train Loss: 0.5501022934913635,Time: 7134.091806173325\n",
      "Epoch: 437500,Train Loss: 0.5501022338867188,Time: 7142.027461051941\n",
      "Epoch: 438000,Train Loss: 0.5501022338867188,Time: 7150.124705553055\n",
      "Epoch: 438500,Train Loss: 0.550102174282074,Time: 7158.081507921219\n",
      "Epoch: 439000,Train Loss: 0.5501021146774292,Time: 7166.578065633774\n",
      "Epoch: 439500,Train Loss: 0.5501020550727844,Time: 7174.59049987793\n",
      "Epoch: 440000,Train Loss: 0.5501020550727844,Time: 7182.738741159439\n",
      "Epoch: 440500,Train Loss: 0.5501020550727844,Time: 7190.771462202072\n",
      "Epoch: 441000,Train Loss: 0.5501019954681396,Time: 7199.16465306282\n",
      "Epoch: 441500,Train Loss: 0.5501019358634949,Time: 7207.294162511826\n",
      "Epoch: 442000,Train Loss: 0.5501019358634949,Time: 7215.646905660629\n",
      "Epoch: 442500,Train Loss: 0.5501019358634949,Time: 7223.690241575241\n",
      "Epoch: 443000,Train Loss: 0.5501018762588501,Time: 7231.727341175079\n",
      "Epoch: 443500,Train Loss: 0.5501017570495605,Time: 7239.5513780117035\n",
      "Epoch: 444000,Train Loss: 0.5501017570495605,Time: 7247.667306184769\n",
      "Epoch: 444500,Train Loss: 0.5501016974449158,Time: 7255.693984985352\n",
      "Epoch: 445000,Train Loss: 0.5501016974449158,Time: 7263.873206615448\n",
      "Epoch: 445500,Train Loss: 0.5501016974449158,Time: 7271.819803714752\n",
      "Epoch: 446000,Train Loss: 0.550101637840271,Time: 7280.09491276741\n",
      "Epoch: 446500,Train Loss: 0.5501015782356262,Time: 7288.140771627426\n",
      "Epoch: 447000,Train Loss: 0.5501015782356262,Time: 7296.267016172409\n",
      "Epoch: 447500,Train Loss: 0.5501014590263367,Time: 7304.44783782959\n",
      "Epoch: 448000,Train Loss: 0.5501015186309814,Time: 7313.03634929657\n",
      "Epoch: 448500,Train Loss: 0.5501014590263367,Time: 7321.234731912613\n",
      "Epoch: 449000,Train Loss: 0.5501013994216919,Time: 7329.227606296539\n",
      "Epoch: 449500,Train Loss: 0.5501013398170471,Time: 7337.090737104416\n",
      "Epoch: 450000,Train Loss: 0.5501013398170471,Time: 7345.387351989746\n",
      "Epoch: 450500,Train Loss: 0.5501013398170471,Time: 7353.605763196945\n",
      "Epoch: 451000,Train Loss: 0.5501012802124023,Time: 7362.111015796661\n",
      "Epoch: 451500,Train Loss: 0.5501012206077576,Time: 7369.988807916641\n",
      "Epoch: 452000,Train Loss: 0.5501012802124023,Time: 7378.27806520462\n",
      "Epoch: 452500,Train Loss: 0.5501011610031128,Time: 7386.432143211365\n",
      "Epoch: 453000,Train Loss: 0.550101101398468,Time: 7394.636852502823\n",
      "Epoch: 453500,Train Loss: 0.550101101398468,Time: 7402.626675367355\n",
      "Epoch: 454000,Train Loss: 0.5501010417938232,Time: 7410.751773357391\n",
      "Epoch: 454500,Train Loss: 0.5501010417938232,Time: 7418.723793745041\n",
      "Epoch: 455000,Train Loss: 0.5501009821891785,Time: 7426.747616291046\n",
      "Epoch: 455500,Train Loss: 0.5501009821891785,Time: 7434.771238327026\n",
      "Epoch: 456000,Train Loss: 0.5501008629798889,Time: 7442.811513900757\n",
      "Epoch: 456500,Train Loss: 0.5501008629798889,Time: 7451.012352466583\n",
      "Epoch: 457000,Train Loss: 0.5501008629798889,Time: 7459.058935880661\n",
      "Epoch: 457500,Train Loss: 0.5501008033752441,Time: 7467.325171947479\n",
      "Epoch: 458000,Train Loss: 0.5501007437705994,Time: 7475.322799444199\n",
      "Epoch: 458500,Train Loss: 0.5501007437705994,Time: 7483.579756975174\n",
      "Epoch: 459000,Train Loss: 0.5501006841659546,Time: 7491.790954589844\n",
      "Epoch: 459500,Train Loss: 0.5501006841659546,Time: 7500.054457426071\n",
      "Epoch: 460000,Train Loss: 0.5501006245613098,Time: 7508.401125669479\n",
      "Epoch: 460500,Train Loss: 0.550100564956665,Time: 7517.314458608627\n",
      "Epoch: 461000,Train Loss: 0.550100564956665,Time: 7525.988480091095\n",
      "Epoch: 461500,Train Loss: 0.550100564956665,Time: 7534.660407781601\n",
      "Epoch: 462000,Train Loss: 0.5501005053520203,Time: 7542.502646207809\n",
      "Epoch: 462500,Train Loss: 0.5501004457473755,Time: 7550.689733982086\n",
      "Epoch: 463000,Train Loss: 0.5501003861427307,Time: 7559.201309204102\n",
      "Epoch: 463500,Train Loss: 0.5501003861427307,Time: 7567.391817808151\n",
      "Epoch: 464000,Train Loss: 0.5501003265380859,Time: 7575.673047542572\n",
      "Epoch: 464500,Train Loss: 0.5501003265380859,Time: 7583.90142416954\n",
      "Epoch: 465000,Train Loss: 0.5501002669334412,Time: 7591.855786085129\n",
      "Epoch: 465500,Train Loss: 0.5501002669334412,Time: 7600.024955511093\n",
      "Epoch: 466000,Train Loss: 0.5501002669334412,Time: 7607.971675157547\n",
      "Epoch: 466500,Train Loss: 0.5501002073287964,Time: 7616.2111756801605\n",
      "Epoch: 467000,Train Loss: 0.5501001477241516,Time: 7624.474519729614\n",
      "Epoch: 467500,Train Loss: 0.5501001477241516,Time: 7632.753798961639\n",
      "Epoch: 468000,Train Loss: 0.5501000881195068,Time: 7640.90741276741\n",
      "Epoch: 468500,Train Loss: 0.5501000285148621,Time: 7649.573257446289\n",
      "Epoch: 469000,Train Loss: 0.5501000285148621,Time: 7657.511392354965\n",
      "Epoch: 469500,Train Loss: 0.5501000285148621,Time: 7665.6182816028595\n",
      "Epoch: 470000,Train Loss: 0.5501000285148621,Time: 7673.598200082779\n",
      "Epoch: 470500,Train Loss: 0.5500999093055725,Time: 7681.681693315506\n",
      "Epoch: 471000,Train Loss: 0.5500999093055725,Time: 7689.496693849564\n",
      "Epoch: 471500,Train Loss: 0.5500999093055725,Time: 7697.601674079895\n",
      "Epoch: 472000,Train Loss: 0.5500999093055725,Time: 7705.713404178619\n",
      "Epoch: 472500,Train Loss: 0.5500997304916382,Time: 7714.057561159134\n",
      "Epoch: 473000,Train Loss: 0.550099790096283,Time: 7722.195076704025\n",
      "Epoch: 473500,Train Loss: 0.5500997304916382,Time: 7730.560692310333\n",
      "Epoch: 474000,Train Loss: 0.5500996708869934,Time: 7739.17959856987\n",
      "Epoch: 474500,Train Loss: 0.5500996112823486,Time: 7747.570204257965\n",
      "Epoch: 475000,Train Loss: 0.5500996112823486,Time: 7755.784747123718\n",
      "Epoch: 475500,Train Loss: 0.5500996112823486,Time: 7764.236580133438\n",
      "Epoch: 476000,Train Loss: 0.5500994920730591,Time: 7772.844702005386\n",
      "Epoch: 476500,Train Loss: 0.5500994920730591,Time: 7781.109027862549\n",
      "Epoch: 477000,Train Loss: 0.5500994920730591,Time: 7789.439510345459\n",
      "Epoch: 477500,Train Loss: 0.5500994324684143,Time: 7798.0788996219635\n",
      "Epoch: 478000,Train Loss: 0.5500993728637695,Time: 7806.245569467545\n",
      "Epoch: 478500,Train Loss: 0.5500993728637695,Time: 7814.541093826294\n",
      "Epoch: 479000,Train Loss: 0.5500993728637695,Time: 7822.556066989899\n",
      "Epoch: 479500,Train Loss: 0.5500993728637695,Time: 7830.597603797913\n",
      "Epoch: 480000,Train Loss: 0.55009925365448,Time: 7838.573783159256\n",
      "Epoch: 480500,Train Loss: 0.55009925365448,Time: 7846.6913294792175\n",
      "Epoch: 481000,Train Loss: 0.55009925365448,Time: 7854.819602966309\n",
      "Epoch: 481500,Train Loss: 0.55009925365448,Time: 7862.815103054047\n",
      "Epoch: 482000,Train Loss: 0.5500991940498352,Time: 7870.763399600983\n",
      "Epoch: 482500,Train Loss: 0.5500991344451904,Time: 7878.835800647736\n",
      "Epoch: 483000,Train Loss: 0.5500991344451904,Time: 7887.141718387604\n",
      "Epoch: 483500,Train Loss: 0.5500990748405457,Time: 7895.350576400757\n",
      "Epoch: 484000,Train Loss: 0.5500990748405457,Time: 7903.374248981476\n",
      "Epoch: 484500,Train Loss: 0.5500990152359009,Time: 7911.851443529129\n",
      "Epoch: 485000,Train Loss: 0.5500990152359009,Time: 7919.905132293701\n",
      "Epoch: 485500,Train Loss: 0.5500990152359009,Time: 7927.999905824661\n",
      "Epoch: 486000,Train Loss: 0.5500989556312561,Time: 7935.992112636566\n",
      "Epoch: 486500,Train Loss: 0.5500988960266113,Time: 7944.18615937233\n",
      "Epoch: 487000,Train Loss: 0.5500988960266113,Time: 7952.212818145752\n",
      "Epoch: 487500,Train Loss: 0.5500988960266113,Time: 7960.643478631973\n",
      "Epoch: 488000,Train Loss: 0.5500988364219666,Time: 7968.746831417084\n",
      "Epoch: 488500,Train Loss: 0.5500987768173218,Time: 7976.681224107742\n",
      "Epoch: 489000,Train Loss: 0.5500987768173218,Time: 7984.652734994888\n",
      "Epoch: 489500,Train Loss: 0.550098717212677,Time: 7992.801234960556\n",
      "Epoch: 490000,Train Loss: 0.550098717212677,Time: 8000.721486568451\n",
      "Epoch: 490500,Train Loss: 0.550098717212677,Time: 8008.84037566185\n",
      "Epoch: 491000,Train Loss: 0.5500986576080322,Time: 8017.0667889118195\n",
      "Epoch: 491500,Train Loss: 0.5500986576080322,Time: 8025.519721031189\n",
      "Epoch: 492000,Train Loss: 0.5500985980033875,Time: 8033.5433094501495\n",
      "Epoch: 492500,Train Loss: 0.5500985980033875,Time: 8041.637754917145\n",
      "Epoch: 493000,Train Loss: 0.5500985383987427,Time: 8049.585860013962\n",
      "Epoch: 493500,Train Loss: 0.5500985383987427,Time: 8057.888187646866\n",
      "Epoch: 494000,Train Loss: 0.5500985383987427,Time: 8065.9391095638275\n",
      "Epoch: 494500,Train Loss: 0.5500984191894531,Time: 8073.988146781921\n",
      "Epoch: 495000,Train Loss: 0.5500984191894531,Time: 8081.989063024521\n",
      "Epoch: 495500,Train Loss: 0.5500983595848083,Time: 8090.5713403224945\n",
      "Epoch: 496000,Train Loss: 0.5500983595848083,Time: 8098.598435878754\n",
      "Epoch: 496500,Train Loss: 0.5500982999801636,Time: 8106.739627599716\n",
      "Epoch: 497000,Train Loss: 0.5500982999801636,Time: 8114.607294082642\n",
      "Epoch: 497500,Train Loss: 0.5500982999801636,Time: 8122.976756095886\n",
      "Epoch: 498000,Train Loss: 0.5500982403755188,Time: 8130.997215509415\n",
      "Epoch: 498500,Train Loss: 0.5500982403755188,Time: 8139.133871078491\n",
      "Epoch: 499000,Train Loss: 0.550098180770874,Time: 8147.00589466095\n",
      "Epoch: 499500,Train Loss: 0.550098180770874,Time: 8155.103845834732\n",
      "Epoch: 500000,Train Loss: 0.550098180770874,Time: 8163.3900508880615\n",
      "Epoch: 500500,Train Loss: 0.5500981211662292,Time: 8171.5176656246185\n",
      "Epoch: 501000,Train Loss: 0.5500980615615845,Time: 8179.493437767029\n",
      "Epoch: 501500,Train Loss: 0.5500980615615845,Time: 8187.844577550888\n",
      "Epoch: 502000,Train Loss: 0.5500980615615845,Time: 8195.786694765091\n",
      "Epoch: 502500,Train Loss: 0.5500980019569397,Time: 8203.908764123917\n",
      "Epoch: 503000,Train Loss: 0.5500980019569397,Time: 8211.945383787155\n",
      "Epoch: 503500,Train Loss: 0.5500980019569397,Time: 8219.931191921234\n",
      "Epoch: 504000,Train Loss: 0.5500978827476501,Time: 8227.855943202972\n",
      "Epoch: 504500,Train Loss: 0.5500978827476501,Time: 8235.887792825699\n",
      "Epoch: 505000,Train Loss: 0.5500978231430054,Time: 8243.858519792557\n",
      "Epoch: 505500,Train Loss: 0.5500978231430054,Time: 8251.982638597488\n",
      "Epoch: 506000,Train Loss: 0.5500978231430054,Time: 8260.016144752502\n",
      "Epoch: 506500,Train Loss: 0.5500977635383606,Time: 8268.529665231705\n",
      "Epoch: 507000,Train Loss: 0.5500978231430054,Time: 8276.447336912155\n",
      "Epoch: 507500,Train Loss: 0.5500977039337158,Time: 8284.7293446064\n",
      "Epoch: 508000,Train Loss: 0.5500977039337158,Time: 8292.973643302917\n",
      "Epoch: 508500,Train Loss: 0.5500977039337158,Time: 8301.179443120956\n",
      "Epoch: 509000,Train Loss: 0.550097644329071,Time: 8309.135136604309\n",
      "Epoch: 509500,Train Loss: 0.5500975847244263,Time: 8317.167913198471\n",
      "Epoch: 510000,Train Loss: 0.5500975847244263,Time: 8325.456899642944\n",
      "Epoch: 510500,Train Loss: 0.5500975847244263,Time: 8333.650148391724\n",
      "Epoch: 511000,Train Loss: 0.5500975251197815,Time: 8341.8109228611\n",
      "Epoch: 511500,Train Loss: 0.5500974655151367,Time: 8350.127128124237\n",
      "Epoch: 512000,Train Loss: 0.5500974655151367,Time: 8358.287434339523\n",
      "Epoch: 512500,Train Loss: 0.5500974655151367,Time: 8366.457747459412\n",
      "Epoch: 513000,Train Loss: 0.5500974655151367,Time: 8374.45770072937\n",
      "Epoch: 513500,Train Loss: 0.5500972867012024,Time: 8382.588376045227\n",
      "Epoch: 514000,Train Loss: 0.5500972867012024,Time: 8390.58918428421\n",
      "Epoch: 514500,Train Loss: 0.5500972867012024,Time: 8398.77037858963\n",
      "Epoch: 515000,Train Loss: 0.5500972867012024,Time: 8406.755386352539\n",
      "Epoch: 515500,Train Loss: 0.5500972270965576,Time: 8415.111544847488\n",
      "Epoch: 516000,Train Loss: 0.5500971674919128,Time: 8423.001596927643\n",
      "Epoch: 516500,Train Loss: 0.5500971674919128,Time: 8431.336617946625\n",
      "Epoch: 517000,Train Loss: 0.5500971674919128,Time: 8439.469557523727\n",
      "Epoch: 517500,Train Loss: 0.5500970482826233,Time: 8447.61574625969\n",
      "Epoch: 518000,Train Loss: 0.5500971078872681,Time: 8455.558390140533\n",
      "Epoch: 518500,Train Loss: 0.5500970482826233,Time: 8463.692896842957\n",
      "Epoch: 519000,Train Loss: 0.5500969886779785,Time: 8471.712946176529\n",
      "Epoch: 519500,Train Loss: 0.5500969886779785,Time: 8479.81762957573\n",
      "Epoch: 520000,Train Loss: 0.5500969290733337,Time: 8487.715617895126\n",
      "Epoch: 520500,Train Loss: 0.5500969290733337,Time: 8495.71318078041\n",
      "Epoch: 521000,Train Loss: 0.5500969290733337,Time: 8503.802932977676\n",
      "Epoch: 521500,Train Loss: 0.550096869468689,Time: 8512.126628398895\n",
      "Epoch: 522000,Train Loss: 0.5500968098640442,Time: 8519.951177835464\n",
      "Epoch: 522500,Train Loss: 0.5500968098640442,Time: 8528.008669376373\n",
      "Epoch: 523000,Train Loss: 0.5500968098640442,Time: 8535.9404900074\n",
      "Epoch: 523500,Train Loss: 0.5500968098640442,Time: 8544.228781223297\n",
      "Epoch: 524000,Train Loss: 0.5500967502593994,Time: 8552.292350292206\n",
      "Epoch: 524500,Train Loss: 0.5500966906547546,Time: 8560.398116827011\n",
      "Epoch: 525000,Train Loss: 0.5500966906547546,Time: 8568.624056816101\n",
      "Epoch: 525500,Train Loss: 0.5500966906547546,Time: 8576.80609035492\n",
      "Epoch: 526000,Train Loss: 0.5500966906547546,Time: 8584.58450460434\n",
      "Epoch: 526500,Train Loss: 0.5500966310501099,Time: 8592.875415563583\n",
      "Epoch: 527000,Train Loss: 0.5500965714454651,Time: 8601.252856016159\n",
      "Epoch: 527500,Train Loss: 0.5500965714454651,Time: 8609.551934480667\n",
      "Epoch: 528000,Train Loss: 0.5500965714454651,Time: 8618.125359296799\n",
      "Epoch: 528500,Train Loss: 0.5500965118408203,Time: 8626.540742635727\n",
      "Epoch: 529000,Train Loss: 0.5500965714454651,Time: 8634.973691701889\n",
      "Epoch: 529500,Train Loss: 0.5500964522361755,Time: 8642.939486980438\n",
      "Epoch: 530000,Train Loss: 0.5500964522361755,Time: 8651.278811454773\n",
      "Epoch: 530500,Train Loss: 0.5500963926315308,Time: 8659.508688926697\n",
      "Epoch: 531000,Train Loss: 0.5500963926315308,Time: 8668.103278398514\n",
      "Epoch: 531500,Train Loss: 0.5500963926315308,Time: 8676.12119102478\n",
      "Epoch: 532000,Train Loss: 0.550096333026886,Time: 8684.293442487717\n",
      "Epoch: 532500,Train Loss: 0.550096333026886,Time: 8692.346675395966\n",
      "Epoch: 533000,Train Loss: 0.5500962138175964,Time: 8700.421012163162\n",
      "Epoch: 533500,Train Loss: 0.5500962138175964,Time: 8708.374539136887\n",
      "Epoch: 534000,Train Loss: 0.5500962138175964,Time: 8716.660856485367\n",
      "Epoch: 534500,Train Loss: 0.5500962138175964,Time: 8725.083826065063\n",
      "Epoch: 535000,Train Loss: 0.5500962138175964,Time: 8733.412180185318\n",
      "Epoch: 535500,Train Loss: 0.5500961542129517,Time: 8741.737329483032\n",
      "Epoch: 536000,Train Loss: 0.5500960946083069,Time: 8749.962527036667\n",
      "Epoch: 536500,Train Loss: 0.5500960946083069,Time: 8757.87992811203\n",
      "Epoch: 537000,Train Loss: 0.5500960946083069,Time: 8766.01679778099\n",
      "Epoch: 537500,Train Loss: 0.5500959753990173,Time: 8774.040708780289\n",
      "Epoch: 538000,Train Loss: 0.5500959753990173,Time: 8782.540329933167\n",
      "Epoch: 538500,Train Loss: 0.5500959753990173,Time: 8790.513878107071\n",
      "Epoch: 539000,Train Loss: 0.5500959753990173,Time: 8798.768551826477\n",
      "Epoch: 539500,Train Loss: 0.5500959753990173,Time: 8806.780453443527\n",
      "Epoch: 540000,Train Loss: 0.5500958561897278,Time: 8814.919095993042\n",
      "Epoch: 540500,Train Loss: 0.5500958561897278,Time: 8822.874196529388\n",
      "Epoch: 541000,Train Loss: 0.5500958561897278,Time: 8831.116162061691\n",
      "Epoch: 541500,Train Loss: 0.5500958561897278,Time: 8839.045498609543\n",
      "Epoch: 542000,Train Loss: 0.550095796585083,Time: 8847.299275636673\n",
      "Epoch: 542500,Train Loss: 0.550095796585083,Time: 8855.236085414886\n",
      "Epoch: 543000,Train Loss: 0.5500957369804382,Time: 8863.316269397736\n",
      "Epoch: 543500,Train Loss: 0.5500957369804382,Time: 8871.469730377197\n",
      "Epoch: 544000,Train Loss: 0.5500956773757935,Time: 8879.585921525955\n",
      "Epoch: 544500,Train Loss: 0.5500956773757935,Time: 8887.740989923477\n",
      "Epoch: 545000,Train Loss: 0.5500956177711487,Time: 8895.858811378479\n",
      "Epoch: 545500,Train Loss: 0.5500956177711487,Time: 8903.806237459183\n",
      "Epoch: 546000,Train Loss: 0.5500956177711487,Time: 8911.874072551727\n",
      "Epoch: 546500,Train Loss: 0.5500955581665039,Time: 8919.77568244934\n",
      "Epoch: 547000,Train Loss: 0.5500955581665039,Time: 8927.912875652313\n",
      "Epoch: 547500,Train Loss: 0.5500955581665039,Time: 8935.848888635635\n",
      "Epoch: 548000,Train Loss: 0.5500954985618591,Time: 8944.207407474518\n",
      "Epoch: 548500,Train Loss: 0.5500954985618591,Time: 8952.641156196594\n",
      "Epoch: 549000,Train Loss: 0.5500954389572144,Time: 8960.704305648804\n",
      "Epoch: 549500,Train Loss: 0.5500954389572144,Time: 8968.905044794083\n",
      "Epoch: 550000,Train Loss: 0.5500954389572144,Time: 8977.09575009346\n",
      "Epoch: 550500,Train Loss: 0.5500953793525696,Time: 8985.134841442108\n",
      "Epoch: 551000,Train Loss: 0.5500953793525696,Time: 8993.550812005997\n",
      "Epoch: 551500,Train Loss: 0.5500953793525696,Time: 9001.697353363037\n",
      "Epoch: 552000,Train Loss: 0.5500953793525696,Time: 9010.417068481445\n",
      "Epoch: 552500,Train Loss: 0.5500953197479248,Time: 9019.034038305283\n",
      "Epoch: 553000,Train Loss: 0.55009526014328,Time: 9027.81042432785\n",
      "Epoch: 553500,Train Loss: 0.55009526014328,Time: 9035.782273054123\n",
      "Epoch: 554000,Train Loss: 0.55009526014328,Time: 9043.855432987213\n",
      "Epoch: 554500,Train Loss: 0.5500951409339905,Time: 9051.797035217285\n",
      "Epoch: 555000,Train Loss: 0.5500951409339905,Time: 9060.069725990295\n",
      "Epoch: 555500,Train Loss: 0.5500951409339905,Time: 9068.112318277359\n",
      "Epoch: 556000,Train Loss: 0.5500951409339905,Time: 9076.139307260513\n",
      "Epoch: 556500,Train Loss: 0.5500950813293457,Time: 9084.069531917572\n",
      "Epoch: 557000,Train Loss: 0.5500950217247009,Time: 9092.094084262848\n",
      "Epoch: 557500,Train Loss: 0.5500950217247009,Time: 9100.00341796875\n",
      "Epoch: 558000,Train Loss: 0.5500950217247009,Time: 9108.269556045532\n",
      "Epoch: 558500,Train Loss: 0.5500950217247009,Time: 9116.347534179688\n",
      "Epoch: 559000,Train Loss: 0.5500949621200562,Time: 9124.652091264725\n",
      "Epoch: 559500,Train Loss: 0.5500949025154114,Time: 9132.774011135101\n",
      "Epoch: 560000,Train Loss: 0.5500949025154114,Time: 9140.98444032669\n",
      "Epoch: 560500,Train Loss: 0.5500949025154114,Time: 9149.054655790329\n",
      "Epoch: 561000,Train Loss: 0.5500949025154114,Time: 9157.071682929993\n",
      "Epoch: 561500,Train Loss: 0.5500948429107666,Time: 9165.183488368988\n",
      "Epoch: 562000,Train Loss: 0.550094723701477,Time: 9173.31042432785\n",
      "Epoch: 562500,Train Loss: 0.550094723701477,Time: 9181.162632703781\n",
      "Epoch: 563000,Train Loss: 0.550094723701477,Time: 9189.673681259155\n",
      "Epoch: 563500,Train Loss: 0.5500946640968323,Time: 9197.517499446869\n",
      "Epoch: 564000,Train Loss: 0.5500946640968323,Time: 9205.809053659439\n",
      "Epoch: 564500,Train Loss: 0.5500946044921875,Time: 9213.840880393982\n",
      "Epoch: 565000,Train Loss: 0.5500946044921875,Time: 9221.950275659561\n",
      "Epoch: 565500,Train Loss: 0.5500946044921875,Time: 9229.866396665573\n",
      "Epoch: 566000,Train Loss: 0.5500946044921875,Time: 9237.953273534775\n",
      "Epoch: 566500,Train Loss: 0.5500946044921875,Time: 9245.831874132156\n",
      "Epoch: 567000,Train Loss: 0.5500946044921875,Time: 9253.982418298721\n",
      "Epoch: 567500,Train Loss: 0.550094485282898,Time: 9261.96069407463\n",
      "Epoch: 568000,Train Loss: 0.550094485282898,Time: 9270.055552959442\n",
      "Epoch: 568500,Train Loss: 0.550094485282898,Time: 9278.307280302048\n",
      "Epoch: 569000,Train Loss: 0.550094485282898,Time: 9286.543939828873\n",
      "Epoch: 569500,Train Loss: 0.5500944256782532,Time: 9294.688440084457\n",
      "Epoch: 570000,Train Loss: 0.5500943660736084,Time: 9303.206908226013\n",
      "Epoch: 570500,Train Loss: 0.5500943660736084,Time: 9311.417056560516\n",
      "Epoch: 571000,Train Loss: 0.5500943660736084,Time: 9319.596642494202\n",
      "Epoch: 571500,Train Loss: 0.5500943660736084,Time: 9327.758028030396\n",
      "Epoch: 572000,Train Loss: 0.5500943660736084,Time: 9336.146656751633\n",
      "Epoch: 572500,Train Loss: 0.5500942468643188,Time: 9344.172342777252\n",
      "Epoch: 573000,Train Loss: 0.5500942468643188,Time: 9352.363455533981\n",
      "Epoch: 573500,Train Loss: 0.5500942468643188,Time: 9360.387055635452\n",
      "Epoch: 574000,Train Loss: 0.5500942468643188,Time: 9368.421610593796\n",
      "Epoch: 574500,Train Loss: 0.5500941872596741,Time: 9376.619954824448\n",
      "Epoch: 575000,Train Loss: 0.5500941872596741,Time: 9385.078713417053\n",
      "Epoch: 575500,Train Loss: 0.5500941872596741,Time: 9393.130481004715\n",
      "Epoch: 576000,Train Loss: 0.5500941872596741,Time: 9401.396624326706\n",
      "Epoch: 576500,Train Loss: 0.5500941276550293,Time: 9409.302522182465\n",
      "Epoch: 577000,Train Loss: 0.5500940680503845,Time: 9417.487351894379\n",
      "Epoch: 577500,Train Loss: 0.5500940680503845,Time: 9425.615131616592\n",
      "Epoch: 578000,Train Loss: 0.5500940680503845,Time: 9433.73515200615\n",
      "Epoch: 578500,Train Loss: 0.5500940084457397,Time: 9441.725820541382\n",
      "Epoch: 579000,Train Loss: 0.5500940084457397,Time: 9449.844406366348\n",
      "Epoch: 579500,Train Loss: 0.5500940084457397,Time: 9457.79120516777\n",
      "Epoch: 580000,Train Loss: 0.5500940084457397,Time: 9466.214236497879\n",
      "Epoch: 580500,Train Loss: 0.550093948841095,Time: 9474.520840644836\n",
      "Epoch: 581000,Train Loss: 0.5500938892364502,Time: 9482.965816259384\n",
      "Epoch: 581500,Train Loss: 0.5500938892364502,Time: 9491.005589723587\n",
      "Epoch: 582000,Train Loss: 0.5500938892364502,Time: 9499.205129861832\n",
      "Epoch: 582500,Train Loss: 0.5500938892364502,Time: 9507.437319517136\n",
      "Epoch: 583000,Train Loss: 0.5500938296318054,Time: 9515.551990747452\n",
      "Epoch: 583500,Train Loss: 0.5500937700271606,Time: 9523.58406662941\n",
      "Epoch: 584000,Train Loss: 0.5500937700271606,Time: 9531.712111234665\n",
      "Epoch: 584500,Train Loss: 0.5500937700271606,Time: 9539.6343023777\n",
      "Epoch: 585000,Train Loss: 0.5500937700271606,Time: 9547.76013469696\n",
      "Epoch: 585500,Train Loss: 0.5500937700271606,Time: 9555.662674188614\n",
      "Epoch: 586000,Train Loss: 0.5500937700271606,Time: 9564.017672538757\n",
      "Epoch: 586500,Train Loss: 0.5500936508178711,Time: 9571.967047214508\n",
      "Epoch: 587000,Train Loss: 0.5500936508178711,Time: 9579.956269264221\n",
      "Epoch: 587500,Train Loss: 0.5500936508178711,Time: 9587.864834547043\n",
      "Epoch: 588000,Train Loss: 0.5500936508178711,Time: 9596.289240837097\n",
      "Epoch: 588500,Train Loss: 0.5500935316085815,Time: 9604.363752126694\n",
      "Epoch: 589000,Train Loss: 0.5500935316085815,Time: 9612.725282907486\n",
      "Epoch: 589500,Train Loss: 0.5500935316085815,Time: 9620.696741580963\n",
      "Epoch: 590000,Train Loss: 0.5500935912132263,Time: 9628.9728910923\n",
      "Epoch: 590500,Train Loss: 0.5500935316085815,Time: 9636.917871236801\n",
      "Epoch: 591000,Train Loss: 0.5500934720039368,Time: 9645.0497777462\n",
      "Epoch: 591500,Train Loss: 0.5500934720039368,Time: 9652.905566215515\n",
      "Epoch: 592000,Train Loss: 0.5500934720039368,Time: 9661.091521263123\n",
      "Epoch: 592500,Train Loss: 0.550093412399292,Time: 9669.197890281677\n",
      "Epoch: 593000,Train Loss: 0.550093412399292,Time: 9677.538405179977\n",
      "Epoch: 593500,Train Loss: 0.550093412399292,Time: 9685.651447296143\n",
      "Epoch: 594000,Train Loss: 0.5500933527946472,Time: 9693.822308778763\n",
      "Epoch: 594500,Train Loss: 0.5500933527946472,Time: 9701.897592782974\n",
      "Epoch: 595000,Train Loss: 0.5500933527946472,Time: 9710.123048067093\n",
      "Epoch: 595500,Train Loss: 0.5500932931900024,Time: 9718.091943740845\n",
      "Epoch: 596000,Train Loss: 0.5500932931900024,Time: 9726.818242549896\n",
      "Epoch: 596500,Train Loss: 0.5500932931900024,Time: 9734.812871217728\n",
      "Epoch: 597000,Train Loss: 0.5500932931900024,Time: 9742.972184181213\n",
      "Epoch: 597500,Train Loss: 0.5500932931900024,Time: 9750.919896125793\n",
      "Epoch: 598000,Train Loss: 0.5500931739807129,Time: 9759.25057387352\n",
      "Epoch: 598500,Train Loss: 0.5500931739807129,Time: 9767.360264062881\n",
      "Epoch: 599000,Train Loss: 0.5500931739807129,Time: 9775.654549837112\n",
      "Epoch: 599500,Train Loss: 0.5500931739807129,Time: 9783.85669374466\n",
      "Epoch: 600000,Train Loss: 0.5500931739807129,Time: 9791.88140296936\n",
      "Epoch: 600500,Train Loss: 0.5500930547714233,Time: 9800.384118556976\n",
      "Epoch: 601000,Train Loss: 0.5500930547714233,Time: 9808.36649107933\n",
      "Epoch: 601500,Train Loss: 0.5500930547714233,Time: 9816.836436271667\n",
      "Epoch: 602000,Train Loss: 0.5500930547714233,Time: 9824.680658102036\n",
      "Epoch: 602500,Train Loss: 0.5500930547714233,Time: 9832.86853146553\n",
      "Epoch: 603000,Train Loss: 0.5500930547714233,Time: 9840.973332166672\n",
      "Epoch: 603500,Train Loss: 0.5500930547714233,Time: 9849.243071556091\n",
      "Epoch: 604000,Train Loss: 0.5500930547714233,Time: 9857.195588350296\n",
      "Epoch: 604500,Train Loss: 0.5500929355621338,Time: 9865.272034406662\n",
      "Epoch: 605000,Train Loss: 0.5500929355621338,Time: 9873.227097988129\n",
      "Epoch: 605500,Train Loss: 0.5500929355621338,Time: 9881.803117752075\n",
      "Epoch: 606000,Train Loss: 0.5500929355621338,Time: 9889.902918100357\n",
      "Epoch: 606500,Train Loss: 0.550092875957489,Time: 9898.374040603638\n",
      "Epoch: 607000,Train Loss: 0.5500928163528442,Time: 9906.634430408478\n",
      "Epoch: 607500,Train Loss: 0.5500928163528442,Time: 9914.826181650162\n",
      "Epoch: 608000,Train Loss: 0.5500928163528442,Time: 9922.841151475906\n",
      "Epoch: 608500,Train Loss: 0.5500928163528442,Time: 9931.062137126923\n",
      "Epoch: 609000,Train Loss: 0.5500927567481995,Time: 9939.186845064163\n",
      "Epoch: 609500,Train Loss: 0.5500927567481995,Time: 9947.296038627625\n",
      "Epoch: 610000,Train Loss: 0.5500926971435547,Time: 9955.331532001495\n",
      "Epoch: 610500,Train Loss: 0.5500927567481995,Time: 9963.428147554398\n",
      "Epoch: 611000,Train Loss: 0.5500926971435547,Time: 9971.422583341599\n",
      "Epoch: 611500,Train Loss: 0.5500926971435547,Time: 9979.91893196106\n",
      "Epoch: 612000,Train Loss: 0.5500926375389099,Time: 9987.837050676346\n",
      "Epoch: 612500,Train Loss: 0.5500926375389099,Time: 9995.954029798508\n",
      "Epoch: 613000,Train Loss: 0.5500926375389099,Time: 10004.014209270477\n",
      "Epoch: 613500,Train Loss: 0.5500925779342651,Time: 10012.249755859375\n",
      "Epoch: 614000,Train Loss: 0.5500925779342651,Time: 10020.24995303154\n",
      "Epoch: 614500,Train Loss: 0.5500925779342651,Time: 10028.45194029808\n",
      "Epoch: 615000,Train Loss: 0.5500925183296204,Time: 10036.734673023224\n",
      "Epoch: 615500,Train Loss: 0.5500925183296204,Time: 10045.093382358551\n",
      "Epoch: 616000,Train Loss: 0.5500925183296204,Time: 10053.16656589508\n",
      "Epoch: 616500,Train Loss: 0.5500925183296204,Time: 10061.28263425827\n",
      "Epoch: 617000,Train Loss: 0.5500924587249756,Time: 10069.190824508667\n",
      "Epoch: 617500,Train Loss: 0.5500924587249756,Time: 10077.347030878067\n",
      "Epoch: 618000,Train Loss: 0.5500924587249756,Time: 10085.20525598526\n",
      "Epoch: 618500,Train Loss: 0.5500924587249756,Time: 10093.658987045288\n",
      "Epoch: 619000,Train Loss: 0.550092339515686,Time: 10101.727612257004\n",
      "Epoch: 619500,Train Loss: 0.550092339515686,Time: 10109.876259088516\n",
      "Epoch: 620000,Train Loss: 0.550092339515686,Time: 10117.761791944504\n",
      "Epoch: 620500,Train Loss: 0.550092339515686,Time: 10125.719193220139\n",
      "Epoch: 621000,Train Loss: 0.550092339515686,Time: 10133.702095031738\n",
      "Epoch: 621500,Train Loss: 0.550092339515686,Time: 10141.829615831375\n",
      "Epoch: 622000,Train Loss: 0.550092339515686,Time: 10149.766038417816\n",
      "Epoch: 622500,Train Loss: 0.5500921607017517,Time: 10157.93883061409\n",
      "Epoch: 623000,Train Loss: 0.5500921607017517,Time: 10165.92042684555\n",
      "Epoch: 623500,Train Loss: 0.5500921607017517,Time: 10174.799857378006\n",
      "Epoch: 624000,Train Loss: 0.5500921607017517,Time: 10182.92687869072\n",
      "Epoch: 624500,Train Loss: 0.5500921607017517,Time: 10190.951644659042\n",
      "Epoch: 625000,Train Loss: 0.5500921010971069,Time: 10198.892004966736\n",
      "Epoch: 625500,Train Loss: 0.5500921010971069,Time: 10207.09735751152\n",
      "Epoch: 626000,Train Loss: 0.5500921010971069,Time: 10215.193558454514\n",
      "Epoch: 626500,Train Loss: 0.5500920414924622,Time: 10223.706148386002\n",
      "Epoch: 627000,Train Loss: 0.5500920414924622,Time: 10231.71130990982\n",
      "Epoch: 627500,Train Loss: 0.5500919818878174,Time: 10240.06824374199\n",
      "Epoch: 628000,Train Loss: 0.5500919818878174,Time: 10248.370852947235\n",
      "Epoch: 628500,Train Loss: 0.5500919818878174,Time: 10256.551417350769\n",
      "Epoch: 629000,Train Loss: 0.5500919818878174,Time: 10264.629166603088\n",
      "Epoch: 629500,Train Loss: 0.5500919222831726,Time: 10272.905360221863\n",
      "Epoch: 630000,Train Loss: 0.5500919222831726,Time: 10281.056715488434\n",
      "Epoch: 630500,Train Loss: 0.5500919222831726,Time: 10289.627575874329\n",
      "Epoch: 631000,Train Loss: 0.5500918626785278,Time: 10297.92613863945\n",
      "Epoch: 631500,Train Loss: 0.5500918626785278,Time: 10306.09155511856\n",
      "Epoch: 632000,Train Loss: 0.5500918626785278,Time: 10314.703706026077\n",
      "Epoch: 632500,Train Loss: 0.5500918030738831,Time: 10323.172991752625\n",
      "Epoch: 633000,Train Loss: 0.5500918030738831,Time: 10331.351280212402\n",
      "Epoch: 633500,Train Loss: 0.5500918030738831,Time: 10339.527205467224\n",
      "Epoch: 634000,Train Loss: 0.5500918030738831,Time: 10347.697070598602\n",
      "Epoch: 634500,Train Loss: 0.5500917434692383,Time: 10355.728719472885\n",
      "Epoch: 635000,Train Loss: 0.5500918030738831,Time: 10363.654987096786\n",
      "Epoch: 635500,Train Loss: 0.5500917434692383,Time: 10372.072284698486\n",
      "Epoch: 636000,Train Loss: 0.5500916838645935,Time: 10380.007771015167\n",
      "Epoch: 636500,Train Loss: 0.5500916838645935,Time: 10388.137925863266\n",
      "Epoch: 637000,Train Loss: 0.5500916838645935,Time: 10396.021394491196\n",
      "Epoch: 637500,Train Loss: 0.5500916838645935,Time: 10404.444326162338\n",
      "Epoch: 638000,Train Loss: 0.5500916242599487,Time: 10412.625466823578\n",
      "Epoch: 638500,Train Loss: 0.5500916838645935,Time: 10420.67953300476\n",
      "Epoch: 639000,Train Loss: 0.5500916242599487,Time: 10429.10670208931\n",
      "Epoch: 639500,Train Loss: 0.550091564655304,Time: 10437.163828849792\n",
      "Epoch: 640000,Train Loss: 0.550091564655304,Time: 10445.478929042816\n",
      "Epoch: 640500,Train Loss: 0.550091564655304,Time: 10453.898104906082\n",
      "Epoch: 641000,Train Loss: 0.550091564655304,Time: 10461.870423316956\n",
      "Epoch: 641500,Train Loss: 0.550091564655304,Time: 10470.110924720764\n",
      "Epoch: 642000,Train Loss: 0.5500915050506592,Time: 10478.149068117142\n",
      "Epoch: 642500,Train Loss: 0.5500915050506592,Time: 10486.30758357048\n",
      "Epoch: 643000,Train Loss: 0.5500914454460144,Time: 10494.2822868824\n",
      "Epoch: 643500,Train Loss: 0.5500914454460144,Time: 10502.337115049362\n",
      "Epoch: 644000,Train Loss: 0.5500914454460144,Time: 10510.405816555023\n",
      "Epoch: 644500,Train Loss: 0.5500913858413696,Time: 10518.442576408386\n",
      "Epoch: 645000,Train Loss: 0.5500913858413696,Time: 10526.4704246521\n",
      "Epoch: 645500,Train Loss: 0.5500913858413696,Time: 10534.638306379318\n",
      "Epoch: 646000,Train Loss: 0.5500913262367249,Time: 10542.5155107975\n",
      "Epoch: 646500,Train Loss: 0.5500913262367249,Time: 10550.569576263428\n",
      "Epoch: 647000,Train Loss: 0.5500913262367249,Time: 10558.681171417236\n",
      "Epoch: 647500,Train Loss: 0.5500913262367249,Time: 10566.84705400467\n",
      "Epoch: 648000,Train Loss: 0.5500913262367249,Time: 10574.741655111313\n",
      "Epoch: 648500,Train Loss: 0.5500913262367249,Time: 10582.938149929047\n",
      "Epoch: 649000,Train Loss: 0.5500912070274353,Time: 10590.863642454147\n",
      "Epoch: 649500,Train Loss: 0.5500912070274353,Time: 10598.997631549835\n",
      "Epoch: 650000,Train Loss: 0.5500912070274353,Time: 10607.049344778061\n",
      "Epoch: 650500,Train Loss: 0.5500912070274353,Time: 10615.211962938309\n",
      "Epoch: 651000,Train Loss: 0.5500912070274353,Time: 10623.255056858063\n",
      "Epoch: 651500,Train Loss: 0.5500912070274353,Time: 10631.283323526382\n",
      "Epoch: 652000,Train Loss: 0.5500911474227905,Time: 10639.368845701218\n",
      "Epoch: 652500,Train Loss: 0.5500912070274353,Time: 10647.804124355316\n",
      "Epoch: 653000,Train Loss: 0.5500911474227905,Time: 10655.945277929306\n",
      "Epoch: 653500,Train Loss: 0.5500910878181458,Time: 10664.135979652405\n",
      "Epoch: 654000,Train Loss: 0.5500910878181458,Time: 10672.016075372696\n",
      "Epoch: 654500,Train Loss: 0.5500910878181458,Time: 10680.24496459961\n",
      "Epoch: 655000,Train Loss: 0.5500910878181458,Time: 10688.329055547714\n",
      "Epoch: 655500,Train Loss: 0.550091028213501,Time: 10696.514202356339\n",
      "Epoch: 656000,Train Loss: 0.550091028213501,Time: 10704.736129283905\n",
      "Epoch: 656500,Train Loss: 0.550091028213501,Time: 10712.869215011597\n",
      "Epoch: 657000,Train Loss: 0.550091028213501,Time: 10720.787338733673\n",
      "Epoch: 657500,Train Loss: 0.5500909686088562,Time: 10729.172617435455\n",
      "Epoch: 658000,Train Loss: 0.5500909686088562,Time: 10737.059635162354\n",
      "Epoch: 658500,Train Loss: 0.5500909686088562,Time: 10745.473973035812\n",
      "Epoch: 659000,Train Loss: 0.5500909686088562,Time: 10753.463401556015\n",
      "Epoch: 659500,Train Loss: 0.5500909090042114,Time: 10761.603357315063\n",
      "Epoch: 660000,Train Loss: 0.5500908493995667,Time: 10769.670876026154\n",
      "Epoch: 660500,Train Loss: 0.5500908493995667,Time: 10778.045217037201\n",
      "Epoch: 661000,Train Loss: 0.5500908493995667,Time: 10786.03353023529\n",
      "Epoch: 661500,Train Loss: 0.5500908493995667,Time: 10794.345970630646\n",
      "Epoch: 662000,Train Loss: 0.5500908493995667,Time: 10802.441906452179\n",
      "Epoch: 662500,Train Loss: 0.5500908493995667,Time: 10810.80726981163\n",
      "Epoch: 663000,Train Loss: 0.5500907897949219,Time: 10819.095361232758\n",
      "Epoch: 663500,Train Loss: 0.5500907301902771,Time: 10827.272437095642\n",
      "Epoch: 664000,Train Loss: 0.5500907301902771,Time: 10835.496922492981\n",
      "Epoch: 664500,Train Loss: 0.5500907301902771,Time: 10843.583160877228\n",
      "Epoch: 665000,Train Loss: 0.5500907301902771,Time: 10851.5216319561\n",
      "Epoch: 665500,Train Loss: 0.5500907301902771,Time: 10859.741268873215\n",
      "Epoch: 666000,Train Loss: 0.5500907301902771,Time: 10867.46886229515\n",
      "Epoch: 666500,Train Loss: 0.5500907301902771,Time: 10875.582205057144\n",
      "Epoch: 667000,Train Loss: 0.5500906109809875,Time: 10883.698369026184\n",
      "Epoch: 667500,Train Loss: 0.5500906705856323,Time: 10891.795205116272\n",
      "Epoch: 668000,Train Loss: 0.5500906109809875,Time: 10900.075756072998\n",
      "Epoch: 668500,Train Loss: 0.5500906109809875,Time: 10908.275247573853\n",
      "Epoch: 669000,Train Loss: 0.5500906109809875,Time: 10916.16670536995\n",
      "Epoch: 669500,Train Loss: 0.5500906109809875,Time: 10924.681080818176\n",
      "Epoch: 670000,Train Loss: 0.5500906109809875,Time: 10932.797444820404\n",
      "Epoch: 670500,Train Loss: 0.5500906109809875,Time: 10940.741058588028\n",
      "Epoch: 671000,Train Loss: 0.5500906109809875,Time: 10948.94289803505\n",
      "Epoch: 671500,Train Loss: 0.550090491771698,Time: 10957.654346942902\n",
      "Epoch: 672000,Train Loss: 0.550090491771698,Time: 10965.950879573822\n",
      "Epoch: 672500,Train Loss: 0.550090491771698,Time: 10973.897644758224\n",
      "Epoch: 673000,Train Loss: 0.550090491771698,Time: 10982.394242286682\n",
      "Epoch: 673500,Train Loss: 0.550090491771698,Time: 10990.731368541718\n",
      "Epoch: 674000,Train Loss: 0.5500904321670532,Time: 10999.05989933014\n",
      "Epoch: 674500,Train Loss: 0.5500904321670532,Time: 11007.085027456284\n",
      "Epoch: 675000,Train Loss: 0.5500904321670532,Time: 11015.39823460579\n",
      "Epoch: 675500,Train Loss: 0.5500904321670532,Time: 11023.214898109436\n",
      "Epoch: 676000,Train Loss: 0.5500903725624084,Time: 11031.24648308754\n",
      "Epoch: 676500,Train Loss: 0.5500903129577637,Time: 11039.300967216492\n",
      "Epoch: 677000,Train Loss: 0.5500903725624084,Time: 11047.480009794235\n",
      "Epoch: 677500,Train Loss: 0.5500903725624084,Time: 11055.461474180222\n",
      "Epoch: 678000,Train Loss: 0.5500903129577637,Time: 11063.788717746735\n",
      "Epoch: 678500,Train Loss: 0.5500903129577637,Time: 11072.408465862274\n",
      "Epoch: 679000,Train Loss: 0.5500902533531189,Time: 11080.971534729004\n",
      "Epoch: 679500,Train Loss: 0.5500902533531189,Time: 11088.93268585205\n",
      "Epoch: 680000,Train Loss: 0.5500902533531189,Time: 11097.26431107521\n",
      "Epoch: 680500,Train Loss: 0.5500902533531189,Time: 11105.290345907211\n",
      "Epoch: 681000,Train Loss: 0.5500902533531189,Time: 11113.405494213104\n",
      "Epoch: 681500,Train Loss: 0.5500902533531189,Time: 11121.821071624756\n",
      "Epoch: 682000,Train Loss: 0.5500902533531189,Time: 11130.04383444786\n",
      "Epoch: 682500,Train Loss: 0.5500902533531189,Time: 11137.913724660873\n",
      "Epoch: 683000,Train Loss: 0.5500901937484741,Time: 11146.227904081345\n",
      "Epoch: 683500,Train Loss: 0.5500901341438293,Time: 11153.878118753433\n",
      "Epoch: 684000,Train Loss: 0.5500901341438293,Time: 11161.957528829575\n",
      "Epoch: 684500,Train Loss: 0.5500901341438293,Time: 11170.120033502579\n",
      "Epoch: 685000,Train Loss: 0.5500901341438293,Time: 11178.12720131874\n",
      "Epoch: 685500,Train Loss: 0.5500901341438293,Time: 11186.197676897049\n",
      "Epoch: 686000,Train Loss: 0.5500901341438293,Time: 11194.290782928467\n",
      "Epoch: 686500,Train Loss: 0.5500900745391846,Time: 11202.367494344711\n",
      "Epoch: 687000,Train Loss: 0.5500900745391846,Time: 11210.432623386383\n",
      "Epoch: 687500,Train Loss: 0.5500900745391846,Time: 11218.537156581879\n",
      "Epoch: 688000,Train Loss: 0.5500900745391846,Time: 11226.660919904709\n",
      "Epoch: 688500,Train Loss: 0.5500900745391846,Time: 11234.61944437027\n",
      "Epoch: 689000,Train Loss: 0.5500900149345398,Time: 11242.812054872513\n",
      "Epoch: 689500,Train Loss: 0.5500900149345398,Time: 11250.691409111023\n",
      "Epoch: 690000,Train Loss: 0.5500900149345398,Time: 11258.77364206314\n",
      "Epoch: 690500,Train Loss: 0.550089955329895,Time: 11266.68250131607\n",
      "Epoch: 691000,Train Loss: 0.550089955329895,Time: 11274.814698457718\n",
      "Epoch: 691500,Train Loss: 0.550089955329895,Time: 11282.734818220139\n",
      "Epoch: 692000,Train Loss: 0.550089955329895,Time: 11290.886397361755\n",
      "Epoch: 692500,Train Loss: 0.5500898957252502,Time: 11298.901983976364\n",
      "Epoch: 693000,Train Loss: 0.5500898957252502,Time: 11307.123502254486\n",
      "Epoch: 693500,Train Loss: 0.5500898957252502,Time: 11315.23204755783\n",
      "Epoch: 694000,Train Loss: 0.5500898957252502,Time: 11323.80953502655\n",
      "Epoch: 694500,Train Loss: 0.5500898957252502,Time: 11331.891077756882\n",
      "Epoch: 695000,Train Loss: 0.5500898361206055,Time: 11340.021956443787\n",
      "Epoch: 695500,Train Loss: 0.5500898361206055,Time: 11347.951773643494\n",
      "Epoch: 696000,Train Loss: 0.5500897765159607,Time: 11356.02522444725\n",
      "Epoch: 696500,Train Loss: 0.5500897765159607,Time: 11364.276144504547\n",
      "Epoch: 697000,Train Loss: 0.5500897765159607,Time: 11372.754251480103\n",
      "Epoch: 697500,Train Loss: 0.5500897765159607,Time: 11380.765229225159\n",
      "Epoch: 698000,Train Loss: 0.5500897765159607,Time: 11389.061305999756\n",
      "Epoch: 698500,Train Loss: 0.5500897765159607,Time: 11396.971156597137\n",
      "Epoch: 699000,Train Loss: 0.5500896573066711,Time: 11405.302056074142\n",
      "Epoch: 699500,Train Loss: 0.5500897765159607,Time: 11413.41994214058\n",
      "Epoch: 700000,Train Loss: 0.5500896573066711,Time: 11421.609447479248\n",
      "Epoch: 700500,Train Loss: 0.5500895977020264,Time: 11429.48925280571\n",
      "Epoch: 701000,Train Loss: 0.5500895977020264,Time: 11437.851877689362\n",
      "Epoch: 701500,Train Loss: 0.5500895977020264,Time: 11446.01761174202\n",
      "Epoch: 702000,Train Loss: 0.5500895977020264,Time: 11454.15125989914\n",
      "Epoch: 702500,Train Loss: 0.5500895977020264,Time: 11462.052327156067\n",
      "Epoch: 703000,Train Loss: 0.5500895977020264,Time: 11470.743598461151\n",
      "Epoch: 703500,Train Loss: 0.5500895977020264,Time: 11479.085861682892\n",
      "Epoch: 704000,Train Loss: 0.5500895380973816,Time: 11487.365135192871\n",
      "Epoch: 704500,Train Loss: 0.5500895977020264,Time: 11495.288837432861\n",
      "Epoch: 705000,Train Loss: 0.5500895380973816,Time: 11503.596962213516\n",
      "Epoch: 705500,Train Loss: 0.5500894784927368,Time: 11511.659243822098\n",
      "Epoch: 706000,Train Loss: 0.5500894784927368,Time: 11520.46246600151\n",
      "Epoch: 706500,Train Loss: 0.5500894784927368,Time: 11528.604532003403\n",
      "Epoch: 707000,Train Loss: 0.5500894784927368,Time: 11536.948590993881\n",
      "Epoch: 707500,Train Loss: 0.550089418888092,Time: 11544.839991569519\n",
      "Epoch: 708000,Train Loss: 0.550089418888092,Time: 11553.02112865448\n",
      "Epoch: 708500,Train Loss: 0.550089418888092,Time: 11560.96624135971\n",
      "Epoch: 709000,Train Loss: 0.550089418888092,Time: 11569.142235279083\n",
      "Epoch: 709500,Train Loss: 0.550089418888092,Time: 11577.100456237793\n",
      "Epoch: 710000,Train Loss: 0.550089418888092,Time: 11585.40654039383\n",
      "Epoch: 710500,Train Loss: 0.5500893592834473,Time: 11593.687255859375\n",
      "Epoch: 711000,Train Loss: 0.5500892996788025,Time: 11602.14902639389\n",
      "Epoch: 711500,Train Loss: 0.5500892996788025,Time: 11610.078604221344\n",
      "Epoch: 712000,Train Loss: 0.5500892996788025,Time: 11618.022111654282\n",
      "Epoch: 712500,Train Loss: 0.5500892996788025,Time: 11625.956958293915\n",
      "Epoch: 713000,Train Loss: 0.5500892400741577,Time: 11634.219023704529\n",
      "Epoch: 713500,Train Loss: 0.5500892400741577,Time: 11642.257156610489\n",
      "Epoch: 714000,Train Loss: 0.5500892400741577,Time: 11650.474913835526\n",
      "Epoch: 714500,Train Loss: 0.5500892400741577,Time: 11658.406344413757\n",
      "Epoch: 715000,Train Loss: 0.5500892400741577,Time: 11666.744066476822\n",
      "Epoch: 715500,Train Loss: 0.5500892400741577,Time: 11674.985388994217\n",
      "Epoch: 716000,Train Loss: 0.5500892400741577,Time: 11683.252118825912\n",
      "Epoch: 716500,Train Loss: 0.5500892400741577,Time: 11691.298053264618\n",
      "Epoch: 717000,Train Loss: 0.5500892400741577,Time: 11699.578881978989\n",
      "Epoch: 717500,Train Loss: 0.5500891804695129,Time: 11707.534152269363\n",
      "Epoch: 718000,Train Loss: 0.5500891804695129,Time: 11715.748123168945\n",
      "Epoch: 718500,Train Loss: 0.5500891208648682,Time: 11723.964897155762\n",
      "Epoch: 719000,Train Loss: 0.5500891208648682,Time: 11732.225091934204\n",
      "Epoch: 719500,Train Loss: 0.5500891208648682,Time: 11740.243176221848\n",
      "Epoch: 720000,Train Loss: 0.5500891208648682,Time: 11748.347723960876\n",
      "Epoch: 720500,Train Loss: 0.5500891208648682,Time: 11756.47386598587\n",
      "Epoch: 721000,Train Loss: 0.5500891208648682,Time: 11764.840081214905\n",
      "Epoch: 721500,Train Loss: 0.5500891208648682,Time: 11772.838673114777\n",
      "Epoch: 722000,Train Loss: 0.5500891208648682,Time: 11781.007686853409\n",
      "Epoch: 722500,Train Loss: 0.5500890612602234,Time: 11789.347378015518\n",
      "Epoch: 723000,Train Loss: 0.5500890016555786,Time: 11797.470932245255\n",
      "Epoch: 723500,Train Loss: 0.5500890612602234,Time: 11805.530222892761\n",
      "Epoch: 724000,Train Loss: 0.5500890016555786,Time: 11814.222896575928\n",
      "Epoch: 724500,Train Loss: 0.5500890016555786,Time: 11823.072534561157\n",
      "Epoch: 725000,Train Loss: 0.5500890016555786,Time: 11831.387949228287\n",
      "Epoch: 725500,Train Loss: 0.5500890016555786,Time: 11839.460128307343\n",
      "Epoch: 726000,Train Loss: 0.5500890016555786,Time: 11847.804236650467\n",
      "Epoch: 726500,Train Loss: 0.5500890016555786,Time: 11855.733040809631\n",
      "Epoch: 727000,Train Loss: 0.5500889420509338,Time: 11864.147328138351\n",
      "Epoch: 727500,Train Loss: 0.5500888824462891,Time: 11872.337604522705\n",
      "Epoch: 728000,Train Loss: 0.5500889420509338,Time: 11880.725987911224\n",
      "Epoch: 728500,Train Loss: 0.5500889420509338,Time: 11889.040709733963\n",
      "Epoch: 729000,Train Loss: 0.5500888824462891,Time: 11897.633448123932\n",
      "Epoch: 729500,Train Loss: 0.5500888824462891,Time: 11905.909084796906\n",
      "Epoch: 730000,Train Loss: 0.5500888824462891,Time: 11914.028125286102\n",
      "Epoch: 730500,Train Loss: 0.5500888824462891,Time: 11921.856707334518\n",
      "Epoch: 731000,Train Loss: 0.5500888824462891,Time: 11930.147763490677\n",
      "Epoch: 731500,Train Loss: 0.5500888824462891,Time: 11937.957712650299\n",
      "Epoch: 732000,Train Loss: 0.5500888824462891,Time: 11946.239527225494\n",
      "Epoch: 732500,Train Loss: 0.5500888824462891,Time: 11954.426206588745\n",
      "Epoch: 733000,Train Loss: 0.5500887632369995,Time: 11962.58560013771\n",
      "Epoch: 733500,Train Loss: 0.5500887632369995,Time: 11970.884846448898\n",
      "Epoch: 734000,Train Loss: 0.5500887632369995,Time: 11979.191519498825\n",
      "Epoch: 734500,Train Loss: 0.5500887632369995,Time: 11987.163538455963\n",
      "Epoch: 735000,Train Loss: 0.5500887632369995,Time: 11995.212372541428\n",
      "Epoch: 735500,Train Loss: 0.5500887632369995,Time: 12003.19787812233\n",
      "Epoch: 736000,Train Loss: 0.5500887632369995,Time: 12011.560435533524\n",
      "Epoch: 736500,Train Loss: 0.5500887632369995,Time: 12019.46656870842\n",
      "Epoch: 737000,Train Loss: 0.5500887632369995,Time: 12027.53531718254\n",
      "Epoch: 737500,Train Loss: 0.5500887036323547,Time: 12035.476465940475\n",
      "Epoch: 738000,Train Loss: 0.55008864402771,Time: 12043.643141508102\n",
      "Epoch: 738500,Train Loss: 0.55008864402771,Time: 12051.614101171494\n",
      "Epoch: 739000,Train Loss: 0.55008864402771,Time: 12059.872866868973\n",
      "Epoch: 739500,Train Loss: 0.55008864402771,Time: 12067.728576898575\n",
      "Epoch: 740000,Train Loss: 0.55008864402771,Time: 12076.032861948013\n",
      "Epoch: 740500,Train Loss: 0.55008864402771,Time: 12084.139082193375\n",
      "Epoch: 741000,Train Loss: 0.55008864402771,Time: 12092.373527765274\n",
      "Epoch: 741500,Train Loss: 0.55008864402771,Time: 12100.213255405426\n",
      "Epoch: 742000,Train Loss: 0.5500885844230652,Time: 12108.377193927765\n",
      "Epoch: 742500,Train Loss: 0.5500885844230652,Time: 12116.712709665298\n",
      "Epoch: 743000,Train Loss: 0.5500885844230652,Time: 12124.747962236404\n",
      "Epoch: 743500,Train Loss: 0.5500885844230652,Time: 12132.88961982727\n",
      "Epoch: 744000,Train Loss: 0.5500885248184204,Time: 12140.84065914154\n",
      "Epoch: 744500,Train Loss: 0.5500885248184204,Time: 12149.262107610703\n",
      "Epoch: 745000,Train Loss: 0.5500885248184204,Time: 12157.234439611435\n",
      "Epoch: 745500,Train Loss: 0.5500885248184204,Time: 12165.527659893036\n",
      "Epoch: 746000,Train Loss: 0.5500885248184204,Time: 12173.335211515427\n",
      "Epoch: 746500,Train Loss: 0.5500884652137756,Time: 12181.344987869263\n",
      "Epoch: 747000,Train Loss: 0.5500884652137756,Time: 12189.321322917938\n",
      "Epoch: 747500,Train Loss: 0.5500884652137756,Time: 12197.576946258545\n",
      "Epoch: 748000,Train Loss: 0.5500884652137756,Time: 12205.440303087234\n",
      "Epoch: 748500,Train Loss: 0.5500884652137756,Time: 12213.665003061295\n",
      "Epoch: 749000,Train Loss: 0.5500884056091309,Time: 12221.911633968353\n",
      "Epoch: 749500,Train Loss: 0.5500884056091309,Time: 12230.53237938881\n",
      "Epoch: 750000,Train Loss: 0.5500884056091309,Time: 12238.810294628143\n",
      "Epoch: 750500,Train Loss: 0.5500884056091309,Time: 12247.078988313675\n",
      "Epoch: 751000,Train Loss: 0.5500884056091309,Time: 12255.484014987946\n",
      "Epoch: 751500,Train Loss: 0.5500883460044861,Time: 12263.708701133728\n",
      "Epoch: 752000,Train Loss: 0.5500883460044861,Time: 12271.870502471924\n",
      "Epoch: 752500,Train Loss: 0.5500883460044861,Time: 12280.040873289108\n",
      "Epoch: 753000,Train Loss: 0.5500883460044861,Time: 12288.011843681335\n",
      "Epoch: 753500,Train Loss: 0.5500883460044861,Time: 12296.405133962631\n",
      "Epoch: 754000,Train Loss: 0.5500882863998413,Time: 12304.261369228363\n",
      "Epoch: 754500,Train Loss: 0.5500882863998413,Time: 12312.397757530212\n",
      "Epoch: 755000,Train Loss: 0.5500882863998413,Time: 12320.765195846558\n",
      "Epoch: 755500,Train Loss: 0.5500882863998413,Time: 12328.883386135101\n",
      "Epoch: 756000,Train Loss: 0.5500882863998413,Time: 12336.943734884262\n",
      "Epoch: 756500,Train Loss: 0.5500882267951965,Time: 12345.544078588486\n",
      "Epoch: 757000,Train Loss: 0.5500882267951965,Time: 12353.589436292648\n",
      "Epoch: 757500,Train Loss: 0.5500882267951965,Time: 12361.735237121582\n",
      "Epoch: 758000,Train Loss: 0.5500881671905518,Time: 12369.77071762085\n",
      "Epoch: 758500,Train Loss: 0.5500882267951965,Time: 12378.058388710022\n",
      "Epoch: 759000,Train Loss: 0.5500881671905518,Time: 12386.117955684662\n",
      "Epoch: 759500,Train Loss: 0.5500881671905518,Time: 12394.303372859955\n",
      "Epoch: 760000,Train Loss: 0.5500881671905518,Time: 12402.346909046173\n",
      "Epoch: 760500,Train Loss: 0.5500881671905518,Time: 12410.85569190979\n",
      "Epoch: 761000,Train Loss: 0.5500881671905518,Time: 12418.716286182404\n",
      "Epoch: 761500,Train Loss: 0.5500881671905518,Time: 12426.882272005081\n",
      "Epoch: 762000,Train Loss: 0.550088107585907,Time: 12434.963327884674\n",
      "Epoch: 762500,Train Loss: 0.550088107585907,Time: 12443.387394666672\n",
      "Epoch: 763000,Train Loss: 0.550088107585907,Time: 12451.550735473633\n",
      "Epoch: 763500,Train Loss: 0.550088107585907,Time: 12459.854760885239\n",
      "Epoch: 764000,Train Loss: 0.5500880479812622,Time: 12467.851872682571\n",
      "Epoch: 764500,Train Loss: 0.5500880479812622,Time: 12476.645423173904\n",
      "Epoch: 765000,Train Loss: 0.5500880479812622,Time: 12485.171651601791\n",
      "Epoch: 765500,Train Loss: 0.5500880479812622,Time: 12493.20198225975\n",
      "Epoch: 766000,Train Loss: 0.5500880479812622,Time: 12501.156870603561\n",
      "Epoch: 766500,Train Loss: 0.5500880479812622,Time: 12509.48965215683\n",
      "Epoch: 767000,Train Loss: 0.5500880479812622,Time: 12517.926219701767\n",
      "Epoch: 767500,Train Loss: 0.5500879883766174,Time: 12526.302825450897\n",
      "Epoch: 768000,Train Loss: 0.5500880479812622,Time: 12534.354088306427\n",
      "Epoch: 768500,Train Loss: 0.5500880479812622,Time: 12542.383073329926\n",
      "Epoch: 769000,Train Loss: 0.5500879287719727,Time: 12550.288830041885\n",
      "Epoch: 769500,Train Loss: 0.5500879287719727,Time: 12559.081643104553\n",
      "Epoch: 770000,Train Loss: 0.5500879287719727,Time: 12567.06612277031\n",
      "Epoch: 770500,Train Loss: 0.5500879287719727,Time: 12575.072296619415\n",
      "Epoch: 771000,Train Loss: 0.5500879287719727,Time: 12583.045976638794\n",
      "Epoch: 771500,Train Loss: 0.5500879287719727,Time: 12591.291484355927\n",
      "Epoch: 772000,Train Loss: 0.5500879287719727,Time: 12599.203886270523\n",
      "Epoch: 772500,Train Loss: 0.5500878691673279,Time: 12607.267949104309\n",
      "Epoch: 773000,Train Loss: 0.5500878691673279,Time: 12615.238286733627\n",
      "Epoch: 773500,Train Loss: 0.5500878691673279,Time: 12623.498454332352\n",
      "Epoch: 774000,Train Loss: 0.5500878095626831,Time: 12631.41776919365\n",
      "Epoch: 774500,Train Loss: 0.5500878691673279,Time: 12639.6010556221\n",
      "Epoch: 775000,Train Loss: 0.5500878095626831,Time: 12647.503350019455\n",
      "Epoch: 775500,Train Loss: 0.5500878095626831,Time: 12655.601682901382\n",
      "Epoch: 776000,Train Loss: 0.5500878095626831,Time: 12663.456687927246\n",
      "Epoch: 776500,Train Loss: 0.5500878095626831,Time: 12671.517340421677\n",
      "Epoch: 777000,Train Loss: 0.5500878095626831,Time: 12679.561367034912\n",
      "Epoch: 777500,Train Loss: 0.5500878095626831,Time: 12687.56060385704\n",
      "Epoch: 778000,Train Loss: 0.5500878095626831,Time: 12695.514206171036\n",
      "Epoch: 778500,Train Loss: 0.5500878095626831,Time: 12703.601407766342\n",
      "Epoch: 779000,Train Loss: 0.5500878095626831,Time: 12711.582261323929\n",
      "Epoch: 779500,Train Loss: 0.5500878095626831,Time: 12719.623234510422\n",
      "Epoch: 780000,Train Loss: 0.5500877499580383,Time: 12727.6363260746\n",
      "Epoch: 780500,Train Loss: 0.5500876903533936,Time: 12735.597514867783\n",
      "Epoch: 781000,Train Loss: 0.5500876903533936,Time: 12743.55763721466\n",
      "Epoch: 781500,Train Loss: 0.5500876903533936,Time: 12751.61536026001\n",
      "Epoch: 782000,Train Loss: 0.5500876903533936,Time: 12759.578741550446\n",
      "Epoch: 782500,Train Loss: 0.5500876903533936,Time: 12767.595103025436\n",
      "Epoch: 783000,Train Loss: 0.5500876903533936,Time: 12776.091500759125\n",
      "Epoch: 783500,Train Loss: 0.5500876903533936,Time: 12784.382358074188\n",
      "Epoch: 784000,Train Loss: 0.5500876903533936,Time: 12792.292074918747\n",
      "Epoch: 784500,Train Loss: 0.5500876307487488,Time: 12800.48893237114\n",
      "Epoch: 785000,Train Loss: 0.5500876903533936,Time: 12808.609034061432\n",
      "Epoch: 785500,Train Loss: 0.550087571144104,Time: 12816.8231112957\n",
      "Epoch: 786000,Train Loss: 0.5500876307487488,Time: 12824.857934713364\n",
      "Epoch: 786500,Train Loss: 0.550087571144104,Time: 12833.152666568756\n",
      "Epoch: 787000,Train Loss: 0.550087571144104,Time: 12841.160436153412\n",
      "Epoch: 787500,Train Loss: 0.550087571144104,Time: 12849.547342538834\n",
      "Epoch: 788000,Train Loss: 0.550087571144104,Time: 12857.434788942337\n",
      "Epoch: 788500,Train Loss: 0.550087571144104,Time: 12865.757599830627\n",
      "Epoch: 789000,Train Loss: 0.550087571144104,Time: 12873.985602855682\n",
      "Epoch: 789500,Train Loss: 0.550087571144104,Time: 12882.24717092514\n",
      "Epoch: 790000,Train Loss: 0.550087571144104,Time: 12890.176886796951\n",
      "Epoch: 790500,Train Loss: 0.550087571144104,Time: 12898.483085155487\n",
      "Epoch: 791000,Train Loss: 0.550087571144104,Time: 12906.511103153229\n",
      "Epoch: 791500,Train Loss: 0.5500874519348145,Time: 12914.707029104233\n",
      "Epoch: 792000,Train Loss: 0.5500874519348145,Time: 12922.679659843445\n",
      "Epoch: 792500,Train Loss: 0.5500874519348145,Time: 12930.72207570076\n",
      "Epoch: 793000,Train Loss: 0.5500874519348145,Time: 12938.60808467865\n",
      "Epoch: 793500,Train Loss: 0.5500874519348145,Time: 12946.653859376907\n",
      "Epoch: 794000,Train Loss: 0.5500874519348145,Time: 12954.810162782669\n",
      "Epoch: 794500,Train Loss: 0.5500873923301697,Time: 12963.003054618835\n",
      "Epoch: 795000,Train Loss: 0.5500874519348145,Time: 12971.00619506836\n",
      "Epoch: 795500,Train Loss: 0.5500874519348145,Time: 12979.277747392654\n",
      "Epoch: 796000,Train Loss: 0.5500874519348145,Time: 12987.311832904816\n",
      "Epoch: 796500,Train Loss: 0.5500873923301697,Time: 12995.542749881744\n",
      "Epoch: 797000,Train Loss: 0.5500873327255249,Time: 13003.468921422958\n",
      "Epoch: 797500,Train Loss: 0.5500873327255249,Time: 13011.711738348007\n",
      "Epoch: 798000,Train Loss: 0.5500873327255249,Time: 13019.757888793945\n",
      "Epoch: 798500,Train Loss: 0.5500873327255249,Time: 13027.902590751648\n",
      "Epoch: 799000,Train Loss: 0.5500873327255249,Time: 13035.918180704117\n",
      "Epoch: 799500,Train Loss: 0.5500873327255249,Time: 13044.73493361473\n",
      "Epoch: 800000,Train Loss: 0.5500873327255249,Time: 13053.109688520432\n",
      "Epoch: 800500,Train Loss: 0.5500873327255249,Time: 13061.173223257065\n",
      "Epoch: 801000,Train Loss: 0.5500873327255249,Time: 13069.205624818802\n",
      "Epoch: 801500,Train Loss: 0.5500873327255249,Time: 13077.235268115997\n",
      "Epoch: 802000,Train Loss: 0.5500873327255249,Time: 13085.364449262619\n",
      "Epoch: 802500,Train Loss: 0.5500872135162354,Time: 13093.914006710052\n",
      "Epoch: 803000,Train Loss: 0.5500872135162354,Time: 13101.79610157013\n",
      "Epoch: 803500,Train Loss: 0.5500872135162354,Time: 13109.885722875595\n",
      "Epoch: 804000,Train Loss: 0.5500872135162354,Time: 13117.939944982529\n",
      "Epoch: 804500,Train Loss: 0.5500872135162354,Time: 13126.26630449295\n",
      "Epoch: 805000,Train Loss: 0.5500872135162354,Time: 13134.238496541977\n",
      "Epoch: 805500,Train Loss: 0.5500872135162354,Time: 13142.4736058712\n",
      "Epoch: 806000,Train Loss: 0.5500872135162354,Time: 13150.853280067444\n",
      "Epoch: 806500,Train Loss: 0.5500872135162354,Time: 13159.741785049438\n",
      "Epoch: 807000,Train Loss: 0.5500870943069458,Time: 13167.787343740463\n",
      "Epoch: 807500,Train Loss: 0.5500872135162354,Time: 13176.143068552017\n",
      "Epoch: 808000,Train Loss: 0.550087034702301,Time: 13184.180762052536\n",
      "Epoch: 808500,Train Loss: 0.550087034702301,Time: 13192.296386957169\n",
      "Epoch: 809000,Train Loss: 0.550087034702301,Time: 13200.511415243149\n",
      "Epoch: 809500,Train Loss: 0.550087034702301,Time: 13208.899840831757\n",
      "Epoch: 810000,Train Loss: 0.550087034702301,Time: 13216.815284729004\n",
      "Epoch: 810500,Train Loss: 0.550087034702301,Time: 13225.490387439728\n",
      "Epoch: 811000,Train Loss: 0.550087034702301,Time: 13233.618930101395\n",
      "Epoch: 811500,Train Loss: 0.550087034702301,Time: 13241.77980351448\n",
      "Epoch: 812000,Train Loss: 0.550087034702301,Time: 13249.809141635895\n",
      "Epoch: 812500,Train Loss: 0.550087034702301,Time: 13257.92988705635\n",
      "Epoch: 813000,Train Loss: 0.550087034702301,Time: 13266.056646823883\n",
      "Epoch: 813500,Train Loss: 0.550087034702301,Time: 13274.012732744217\n",
      "Epoch: 814000,Train Loss: 0.5500869154930115,Time: 13282.400128602982\n",
      "Epoch: 814500,Train Loss: 0.5500869750976562,Time: 13290.650028467178\n",
      "Epoch: 815000,Train Loss: 0.5500869154930115,Time: 13299.04658317566\n",
      "Epoch: 815500,Train Loss: 0.5500869154930115,Time: 13307.0134100914\n",
      "Epoch: 816000,Train Loss: 0.5500869154930115,Time: 13315.304912567139\n",
      "Epoch: 816500,Train Loss: 0.5500869154930115,Time: 13323.270445108414\n",
      "Epoch: 817000,Train Loss: 0.5500869154930115,Time: 13331.556024074554\n",
      "Epoch: 817500,Train Loss: 0.5500868558883667,Time: 13339.49500131607\n",
      "Epoch: 818000,Train Loss: 0.5500868558883667,Time: 13347.689757585526\n",
      "Epoch: 818500,Train Loss: 0.5500868558883667,Time: 13355.719175100327\n",
      "Epoch: 819000,Train Loss: 0.5500868558883667,Time: 13363.946754693985\n",
      "Epoch: 819500,Train Loss: 0.5500868558883667,Time: 13371.989918470383\n",
      "Epoch: 820000,Train Loss: 0.5500868558883667,Time: 13380.477855205536\n",
      "Epoch: 820500,Train Loss: 0.5500868558883667,Time: 13388.722882270813\n",
      "Epoch: 821000,Train Loss: 0.5500867962837219,Time: 13396.980956554413\n",
      "Epoch: 821500,Train Loss: 0.5500867962837219,Time: 13405.086396932602\n",
      "Epoch: 822000,Train Loss: 0.5500867962837219,Time: 13413.32188987732\n",
      "Epoch: 822500,Train Loss: 0.5500867962837219,Time: 13421.53148317337\n",
      "Epoch: 823000,Train Loss: 0.5500867366790771,Time: 13429.936820983887\n",
      "Epoch: 823500,Train Loss: 0.5500867962837219,Time: 13438.227371692657\n",
      "Epoch: 824000,Train Loss: 0.5500867962837219,Time: 13446.33094716072\n",
      "Epoch: 824500,Train Loss: 0.5500867962837219,Time: 13454.261199712753\n",
      "Epoch: 825000,Train Loss: 0.5500867366790771,Time: 13462.754521846771\n",
      "Epoch: 825500,Train Loss: 0.5500867366790771,Time: 13471.06717467308\n",
      "Epoch: 826000,Train Loss: 0.5500867366790771,Time: 13479.314078569412\n",
      "Epoch: 826500,Train Loss: 0.5500866770744324,Time: 13487.60109949112\n",
      "Epoch: 827000,Train Loss: 0.5500866770744324,Time: 13495.952528238297\n",
      "Epoch: 827500,Train Loss: 0.5500866770744324,Time: 13503.99279499054\n",
      "Epoch: 828000,Train Loss: 0.5500866770744324,Time: 13512.472151517868\n",
      "Epoch: 828500,Train Loss: 0.5500866770744324,Time: 13520.41145992279\n",
      "Epoch: 829000,Train Loss: 0.5500866770744324,Time: 13528.732652425766\n",
      "Epoch: 829500,Train Loss: 0.5500866770744324,Time: 13536.73177599907\n",
      "Epoch: 830000,Train Loss: 0.5500866770744324,Time: 13544.80656504631\n",
      "Epoch: 830500,Train Loss: 0.5500866770744324,Time: 13553.059203863144\n",
      "Epoch: 831000,Train Loss: 0.5500866770744324,Time: 13561.205420970917\n",
      "Epoch: 831500,Train Loss: 0.5500866174697876,Time: 13569.370407819748\n",
      "Epoch: 832000,Train Loss: 0.5500865578651428,Time: 13577.38682794571\n",
      "Epoch: 832500,Train Loss: 0.5500865578651428,Time: 13585.24194073677\n",
      "Epoch: 833000,Train Loss: 0.5500865578651428,Time: 13593.271720647812\n",
      "Epoch: 833500,Train Loss: 0.5500865578651428,Time: 13601.187057495117\n",
      "Epoch: 834000,Train Loss: 0.5500865578651428,Time: 13609.364081382751\n",
      "Epoch: 834500,Train Loss: 0.5500865578651428,Time: 13617.728861093521\n",
      "Epoch: 835000,Train Loss: 0.5500865578651428,Time: 13625.90369462967\n",
      "Epoch: 835500,Train Loss: 0.5500865578651428,Time: 13633.768120765686\n",
      "Epoch: 836000,Train Loss: 0.5500865578651428,Time: 13641.857675790787\n",
      "Epoch: 836500,Train Loss: 0.5500865578651428,Time: 13649.838469266891\n",
      "Epoch: 837000,Train Loss: 0.5500865578651428,Time: 13658.117577791214\n",
      "Epoch: 837500,Train Loss: 0.5500865578651428,Time: 13666.222525835037\n",
      "Epoch: 838000,Train Loss: 0.5500865578651428,Time: 13674.807473897934\n",
      "Epoch: 838500,Train Loss: 0.550086498260498,Time: 13682.810508728027\n",
      "Epoch: 839000,Train Loss: 0.550086498260498,Time: 13690.994451522827\n",
      "Epoch: 839500,Train Loss: 0.5500864386558533,Time: 13698.97709441185\n",
      "Epoch: 840000,Train Loss: 0.5500864386558533,Time: 13707.292792320251\n",
      "Epoch: 840500,Train Loss: 0.5500864386558533,Time: 13715.401446342468\n",
      "Epoch: 841000,Train Loss: 0.5500864386558533,Time: 13723.716552257538\n",
      "Epoch: 841500,Train Loss: 0.5500864386558533,Time: 13731.648650884628\n",
      "Epoch: 842000,Train Loss: 0.5500864386558533,Time: 13739.814444065094\n",
      "Epoch: 842500,Train Loss: 0.5500864386558533,Time: 13748.014244794846\n",
      "Epoch: 843000,Train Loss: 0.5500864386558533,Time: 13756.31352519989\n",
      "Epoch: 843500,Train Loss: 0.5500864386558533,Time: 13764.663769960403\n",
      "Epoch: 844000,Train Loss: 0.5500864386558533,Time: 13772.838653802872\n",
      "Epoch: 844500,Train Loss: 0.5500863790512085,Time: 13780.805932998657\n",
      "Epoch: 845000,Train Loss: 0.5500863194465637,Time: 13789.385825395584\n",
      "Epoch: 845500,Train Loss: 0.5500863790512085,Time: 13797.686745166779\n",
      "Epoch: 846000,Train Loss: 0.5500863194465637,Time: 13805.854083538055\n",
      "Epoch: 846500,Train Loss: 0.5500863194465637,Time: 13814.218729019165\n",
      "Epoch: 847000,Train Loss: 0.5500863194465637,Time: 13822.551886081696\n",
      "Epoch: 847500,Train Loss: 0.5500863194465637,Time: 13830.552402973175\n",
      "Epoch: 848000,Train Loss: 0.5500863194465637,Time: 13839.013697862625\n",
      "Epoch: 848500,Train Loss: 0.550086259841919,Time: 13847.158917665482\n",
      "Epoch: 849000,Train Loss: 0.5500863194465637,Time: 13855.36676120758\n",
      "Epoch: 849500,Train Loss: 0.550086259841919,Time: 13863.39317703247\n",
      "Epoch: 850000,Train Loss: 0.550086259841919,Time: 13871.697182655334\n",
      "Epoch: 850500,Train Loss: 0.550086259841919,Time: 13880.128146648407\n",
      "Epoch: 851000,Train Loss: 0.550086259841919,Time: 13888.433815479279\n",
      "Epoch: 851500,Train Loss: 0.550086259841919,Time: 13896.395445108414\n",
      "Epoch: 852000,Train Loss: 0.5500862002372742,Time: 13904.55171585083\n",
      "Epoch: 852500,Train Loss: 0.5500862002372742,Time: 13912.66816687584\n",
      "Epoch: 853000,Train Loss: 0.5500862002372742,Time: 13920.866903305054\n",
      "Epoch: 853500,Train Loss: 0.5500862002372742,Time: 13928.90355014801\n",
      "Epoch: 854000,Train Loss: 0.5500862002372742,Time: 13936.939091444016\n",
      "Epoch: 854500,Train Loss: 0.5500862002372742,Time: 13944.866255760193\n",
      "Epoch: 855000,Train Loss: 0.5500862002372742,Time: 13953.185059070587\n",
      "Epoch: 855500,Train Loss: 0.5500861406326294,Time: 13961.30664229393\n",
      "Epoch: 856000,Train Loss: 0.5500862002372742,Time: 13969.457971334457\n",
      "Epoch: 856500,Train Loss: 0.5500861406326294,Time: 13977.47152543068\n",
      "Epoch: 857000,Train Loss: 0.5500860810279846,Time: 13985.742554664612\n",
      "Epoch: 857500,Train Loss: 0.5500860810279846,Time: 13993.814343452454\n",
      "Epoch: 858000,Train Loss: 0.5500860810279846,Time: 14002.335261106491\n",
      "Epoch: 858500,Train Loss: 0.5500860810279846,Time: 14010.669735908508\n",
      "Epoch: 859000,Train Loss: 0.5500860810279846,Time: 14018.72409749031\n",
      "Epoch: 859500,Train Loss: 0.5500860810279846,Time: 14026.887828111649\n",
      "Epoch: 860000,Train Loss: 0.5500860810279846,Time: 14035.0052587986\n",
      "Epoch: 860500,Train Loss: 0.5500860810279846,Time: 14042.967557668686\n",
      "Epoch: 861000,Train Loss: 0.5500860810279846,Time: 14051.334882736206\n",
      "Epoch: 861500,Train Loss: 0.5500860810279846,Time: 14059.14091205597\n",
      "Epoch: 862000,Train Loss: 0.5500860810279846,Time: 14067.461333274841\n",
      "Epoch: 862500,Train Loss: 0.5500860810279846,Time: 14075.625824689865\n",
      "Epoch: 863000,Train Loss: 0.5500860810279846,Time: 14083.653707504272\n",
      "Epoch: 863500,Train Loss: 0.5500860214233398,Time: 14091.750520706177\n",
      "Epoch: 864000,Train Loss: 0.5500859618186951,Time: 14099.987211942673\n",
      "Epoch: 864500,Train Loss: 0.5500860214233398,Time: 14108.171476364136\n",
      "Epoch: 865000,Train Loss: 0.5500859618186951,Time: 14116.3327460289\n",
      "Epoch: 865500,Train Loss: 0.5500859618186951,Time: 14124.616328716278\n",
      "Epoch: 866000,Train Loss: 0.5500860214233398,Time: 14132.778693199158\n",
      "Epoch: 866500,Train Loss: 0.5500859618186951,Time: 14140.615931510925\n",
      "Epoch: 867000,Train Loss: 0.5500859618186951,Time: 14148.75960278511\n",
      "Epoch: 867500,Train Loss: 0.5500859618186951,Time: 14156.727005958557\n",
      "Epoch: 868000,Train Loss: 0.5500859618186951,Time: 14165.085067749023\n",
      "Epoch: 868500,Train Loss: 0.5500859618186951,Time: 14173.23289346695\n",
      "Epoch: 869000,Train Loss: 0.5500859618186951,Time: 14181.454859495163\n",
      "Epoch: 869500,Train Loss: 0.5500859618186951,Time: 14189.37982082367\n",
      "Epoch: 870000,Train Loss: 0.5500859022140503,Time: 14197.529955625534\n",
      "Epoch: 870500,Train Loss: 0.5500859022140503,Time: 14206.000829219818\n",
      "Epoch: 871000,Train Loss: 0.5500859022140503,Time: 14214.266713380814\n",
      "Epoch: 871500,Train Loss: 0.5500859022140503,Time: 14222.291447877884\n",
      "Epoch: 872000,Train Loss: 0.5500858426094055,Time: 14230.368376016617\n",
      "Epoch: 872500,Train Loss: 0.5500858426094055,Time: 14238.622799158096\n",
      "Epoch: 873000,Train Loss: 0.5500858426094055,Time: 14246.778464794159\n",
      "Epoch: 873500,Train Loss: 0.5500858426094055,Time: 14254.87645983696\n",
      "Epoch: 874000,Train Loss: 0.5500858426094055,Time: 14263.052109241486\n",
      "Epoch: 874500,Train Loss: 0.5500858426094055,Time: 14271.019159793854\n",
      "Epoch: 875000,Train Loss: 0.5500858426094055,Time: 14279.17096877098\n",
      "Epoch: 875500,Train Loss: 0.5500857830047607,Time: 14287.615955591202\n",
      "Epoch: 876000,Train Loss: 0.5500857830047607,Time: 14295.667397499084\n",
      "Epoch: 876500,Train Loss: 0.5500857830047607,Time: 14303.52079796791\n",
      "Epoch: 877000,Train Loss: 0.5500857830047607,Time: 14311.562242507935\n",
      "Epoch: 877500,Train Loss: 0.5500857830047607,Time: 14319.549769639969\n",
      "Epoch: 878000,Train Loss: 0.550085723400116,Time: 14327.727898359299\n",
      "Epoch: 878500,Train Loss: 0.550085723400116,Time: 14335.672559976578\n",
      "Epoch: 879000,Train Loss: 0.550085723400116,Time: 14344.328873157501\n",
      "Epoch: 879500,Train Loss: 0.550085723400116,Time: 14352.994184017181\n",
      "Epoch: 880000,Train Loss: 0.550085723400116,Time: 14361.655145645142\n",
      "Epoch: 880500,Train Loss: 0.550085723400116,Time: 14369.61970615387\n",
      "Epoch: 881000,Train Loss: 0.550085723400116,Time: 14378.003215551376\n",
      "Epoch: 881500,Train Loss: 0.550085723400116,Time: 14385.95723247528\n",
      "Epoch: 882000,Train Loss: 0.550085723400116,Time: 14394.340193748474\n",
      "Epoch: 882500,Train Loss: 0.550085723400116,Time: 14402.780372619629\n",
      "Epoch: 883000,Train Loss: 0.550085723400116,Time: 14411.19107222557\n",
      "Epoch: 883500,Train Loss: 0.550085723400116,Time: 14419.17180609703\n",
      "Epoch: 884000,Train Loss: 0.5500856041908264,Time: 14427.559786319733\n",
      "Epoch: 884500,Train Loss: 0.5500856041908264,Time: 14435.62333393097\n",
      "Epoch: 885000,Train Loss: 0.5500856041908264,Time: 14443.683732032776\n",
      "Epoch: 885500,Train Loss: 0.5500856041908264,Time: 14451.795097589493\n",
      "Epoch: 886000,Train Loss: 0.5500856041908264,Time: 14459.726989746094\n",
      "Epoch: 886500,Train Loss: 0.5500856041908264,Time: 14468.149884939194\n",
      "Epoch: 887000,Train Loss: 0.5500856041908264,Time: 14476.207613945007\n",
      "Epoch: 887500,Train Loss: 0.5500856041908264,Time: 14484.399609327316\n",
      "Epoch: 888000,Train Loss: 0.5500856041908264,Time: 14492.97266292572\n",
      "Epoch: 888500,Train Loss: 0.5500856041908264,Time: 14501.579958438873\n",
      "Epoch: 889000,Train Loss: 0.5500855445861816,Time: 14509.710537433624\n",
      "Epoch: 889500,Train Loss: 0.5500855445861816,Time: 14518.006573677063\n",
      "Epoch: 890000,Train Loss: 0.5500855445861816,Time: 14525.961347818375\n",
      "Epoch: 890500,Train Loss: 0.5500855445861816,Time: 14534.176293849945\n",
      "Epoch: 891000,Train Loss: 0.5500855445861816,Time: 14542.375408887863\n",
      "Epoch: 891500,Train Loss: 0.5500855445861816,Time: 14551.080387353897\n",
      "Epoch: 892000,Train Loss: 0.5500855445861816,Time: 14559.620372056961\n",
      "Epoch: 892500,Train Loss: 0.5500855445861816,Time: 14568.059121847153\n",
      "Epoch: 893000,Train Loss: 0.5500855445861816,Time: 14576.343765735626\n",
      "Epoch: 893500,Train Loss: 0.5500854849815369,Time: 14584.593039751053\n",
      "Epoch: 894000,Train Loss: 0.5500854849815369,Time: 14592.459956407547\n",
      "Epoch: 894500,Train Loss: 0.5500854849815369,Time: 14600.599731206894\n",
      "Epoch: 895000,Train Loss: 0.5500854849815369,Time: 14608.79836678505\n",
      "Epoch: 895500,Train Loss: 0.5500854849815369,Time: 14617.14096736908\n",
      "Epoch: 896000,Train Loss: 0.5500854849815369,Time: 14625.153983354568\n",
      "Epoch: 896500,Train Loss: 0.5500854849815369,Time: 14633.297689676285\n",
      "Epoch: 897000,Train Loss: 0.5500854253768921,Time: 14641.349304437637\n",
      "Epoch: 897500,Train Loss: 0.5500854253768921,Time: 14649.84692811966\n",
      "Epoch: 898000,Train Loss: 0.5500854253768921,Time: 14658.001328229904\n",
      "Epoch: 898500,Train Loss: 0.5500854253768921,Time: 14666.418830394745\n",
      "Epoch: 899000,Train Loss: 0.5500854253768921,Time: 14674.527312040329\n",
      "Epoch: 899500,Train Loss: 0.5500854253768921,Time: 14682.855522871017\n",
      "Epoch: 900000,Train Loss: 0.5500854253768921,Time: 14690.91933631897\n",
      "Epoch: 900500,Train Loss: 0.5500853657722473,Time: 14699.02836894989\n",
      "Epoch: 901000,Train Loss: 0.5500853657722473,Time: 14706.987466573715\n",
      "Epoch: 901500,Train Loss: 0.5500853657722473,Time: 14715.444265842438\n",
      "Epoch: 902000,Train Loss: 0.5500853657722473,Time: 14723.526163339615\n",
      "Epoch: 902500,Train Loss: 0.5500853657722473,Time: 14731.659009218216\n",
      "Epoch: 903000,Train Loss: 0.5500853657722473,Time: 14739.655131340027\n",
      "Epoch: 903500,Train Loss: 0.5500853061676025,Time: 14748.33968782425\n",
      "Epoch: 904000,Train Loss: 0.5500853061676025,Time: 14756.425205469131\n",
      "Epoch: 904500,Train Loss: 0.5500853061676025,Time: 14764.523738384247\n",
      "Epoch: 905000,Train Loss: 0.5500853061676025,Time: 14772.475098848343\n",
      "Epoch: 905500,Train Loss: 0.5500853061676025,Time: 14780.583331346512\n",
      "Epoch: 906000,Train Loss: 0.5500853061676025,Time: 14788.611088991165\n",
      "Epoch: 906500,Train Loss: 0.5500853061676025,Time: 14796.973710536957\n",
      "Epoch: 907000,Train Loss: 0.5500853061676025,Time: 14805.337347507477\n",
      "Epoch: 907500,Train Loss: 0.5500853061676025,Time: 14813.53200507164\n",
      "Epoch: 908000,Train Loss: 0.5500852465629578,Time: 14821.672312021255\n",
      "Epoch: 908500,Train Loss: 0.5500852465629578,Time: 14829.646530866623\n",
      "Epoch: 909000,Train Loss: 0.5500852465629578,Time: 14837.574355602264\n",
      "Epoch: 909500,Train Loss: 0.5500852465629578,Time: 14846.014599084854\n",
      "Epoch: 910000,Train Loss: 0.5500852465629578,Time: 14853.885491132736\n",
      "Epoch: 910500,Train Loss: 0.5500852465629578,Time: 14862.11947107315\n",
      "Epoch: 911000,Train Loss: 0.5500852465629578,Time: 14869.94368481636\n",
      "Epoch: 911500,Train Loss: 0.5500852465629578,Time: 14878.08393573761\n",
      "Epoch: 912000,Train Loss: 0.5500852465629578,Time: 14886.029259204865\n",
      "Epoch: 912500,Train Loss: 0.550085186958313,Time: 14894.03283572197\n",
      "Epoch: 913000,Train Loss: 0.5500852465629578,Time: 14902.027799367905\n",
      "Epoch: 913500,Train Loss: 0.550085186958313,Time: 14910.319784402847\n",
      "Epoch: 914000,Train Loss: 0.550085186958313,Time: 14918.329434633255\n",
      "Epoch: 914500,Train Loss: 0.550085186958313,Time: 14926.452658176422\n",
      "Epoch: 915000,Train Loss: 0.550085186958313,Time: 14934.31696844101\n",
      "Epoch: 915500,Train Loss: 0.550085186958313,Time: 14942.511565685272\n",
      "Epoch: 916000,Train Loss: 0.550085186958313,Time: 14950.514190912247\n",
      "Epoch: 916500,Train Loss: 0.5500851273536682,Time: 14958.774604320526\n",
      "Epoch: 917000,Train Loss: 0.5500851273536682,Time: 14966.946608304977\n",
      "Epoch: 917500,Train Loss: 0.5500851273536682,Time: 14975.234185695648\n",
      "Epoch: 918000,Train Loss: 0.5500851273536682,Time: 14983.155368328094\n",
      "Epoch: 918500,Train Loss: 0.5500851273536682,Time: 14991.400270938873\n",
      "Epoch: 919000,Train Loss: 0.5500851273536682,Time: 14999.669195652008\n",
      "Epoch: 919500,Train Loss: 0.5500850677490234,Time: 15007.833084344864\n",
      "Epoch: 920000,Train Loss: 0.5500850677490234,Time: 15016.054723501205\n",
      "Epoch: 920500,Train Loss: 0.5500850677490234,Time: 15024.27027797699\n",
      "Epoch: 921000,Train Loss: 0.5500851273536682,Time: 15032.255091905594\n",
      "Epoch: 921500,Train Loss: 0.5500850081443787,Time: 15040.624977111816\n",
      "Epoch: 922000,Train Loss: 0.5500850081443787,Time: 15048.91289639473\n",
      "Epoch: 922500,Train Loss: 0.5500850081443787,Time: 15057.013902664185\n",
      "Epoch: 923000,Train Loss: 0.5500850081443787,Time: 15064.8614320755\n",
      "Epoch: 923500,Train Loss: 0.5500850081443787,Time: 15073.18291926384\n",
      "Epoch: 924000,Train Loss: 0.5500850081443787,Time: 15081.4482421875\n",
      "Epoch: 924500,Train Loss: 0.5500850081443787,Time: 15089.665931463242\n",
      "Epoch: 925000,Train Loss: 0.5500850081443787,Time: 15097.680076122284\n",
      "Epoch: 925500,Train Loss: 0.5500850081443787,Time: 15106.218049287796\n",
      "Epoch: 926000,Train Loss: 0.5500850081443787,Time: 15114.386701107025\n",
      "Epoch: 926500,Train Loss: 0.5500850081443787,Time: 15122.504998207092\n",
      "Epoch: 927000,Train Loss: 0.5500850081443787,Time: 15130.487092018127\n",
      "Epoch: 927500,Train Loss: 0.5500850081443787,Time: 15138.799530267715\n",
      "Epoch: 928000,Train Loss: 0.5500849485397339,Time: 15146.829486608505\n",
      "Epoch: 928500,Train Loss: 0.5500849485397339,Time: 15154.946484565735\n",
      "Epoch: 929000,Train Loss: 0.5500849485397339,Time: 15162.957520484924\n",
      "Epoch: 929500,Train Loss: 0.5500849485397339,Time: 15171.308735609055\n",
      "Epoch: 930000,Train Loss: 0.5500849485397339,Time: 15179.18592262268\n",
      "Epoch: 930500,Train Loss: 0.5500849485397339,Time: 15187.275297403336\n",
      "Epoch: 931000,Train Loss: 0.5500848889350891,Time: 15195.525326251984\n",
      "Epoch: 931500,Train Loss: 0.5500848889350891,Time: 15204.02262043953\n",
      "Epoch: 932000,Train Loss: 0.5500848889350891,Time: 15212.01453757286\n",
      "Epoch: 932500,Train Loss: 0.5500848889350891,Time: 15220.505631923676\n",
      "Epoch: 933000,Train Loss: 0.5500848889350891,Time: 15228.33824801445\n",
      "Epoch: 933500,Train Loss: 0.5500848889350891,Time: 15236.573406457901\n",
      "Epoch: 934000,Train Loss: 0.5500848889350891,Time: 15244.478045463562\n",
      "Epoch: 934500,Train Loss: 0.5500848889350891,Time: 15252.652463197708\n",
      "Epoch: 935000,Train Loss: 0.5500848889350891,Time: 15260.681546211243\n",
      "Epoch: 935500,Train Loss: 0.5500848889350891,Time: 15269.014293432236\n",
      "Epoch: 936000,Train Loss: 0.5500848889350891,Time: 15277.106676578522\n",
      "Epoch: 936500,Train Loss: 0.5500848293304443,Time: 15285.37675499916\n",
      "Epoch: 937000,Train Loss: 0.5500847697257996,Time: 15293.208919286728\n",
      "Epoch: 937500,Train Loss: 0.5500847697257996,Time: 15301.484275579453\n",
      "Epoch: 938000,Train Loss: 0.5500847697257996,Time: 15309.456359148026\n",
      "Epoch: 938500,Train Loss: 0.5500847697257996,Time: 15317.674053668976\n",
      "Epoch: 939000,Train Loss: 0.5500847697257996,Time: 15325.90370965004\n",
      "Epoch: 939500,Train Loss: 0.5500847697257996,Time: 15334.335575819016\n",
      "Epoch: 940000,Train Loss: 0.5500847697257996,Time: 15342.631211996078\n",
      "Epoch: 940500,Train Loss: 0.5500847697257996,Time: 15350.92766571045\n",
      "Epoch: 941000,Train Loss: 0.5500847697257996,Time: 15359.182760715485\n",
      "Epoch: 941500,Train Loss: 0.5500847697257996,Time: 15367.587778806686\n",
      "Epoch: 942000,Train Loss: 0.5500847697257996,Time: 15375.653636217117\n",
      "Epoch: 942500,Train Loss: 0.5500847697257996,Time: 15384.002722024918\n",
      "Epoch: 943000,Train Loss: 0.5500847697257996,Time: 15391.93957400322\n",
      "Epoch: 943500,Train Loss: 0.5500847697257996,Time: 15400.311795949936\n",
      "Epoch: 944000,Train Loss: 0.55008465051651,Time: 15408.784786701202\n",
      "Epoch: 944500,Train Loss: 0.55008465051651,Time: 15416.907143592834\n",
      "Epoch: 945000,Train Loss: 0.55008465051651,Time: 15424.92533659935\n",
      "Epoch: 945500,Train Loss: 0.55008465051651,Time: 15432.962047338486\n",
      "Epoch: 946000,Train Loss: 0.55008465051651,Time: 15441.355702877045\n",
      "Epoch: 946500,Train Loss: 0.55008465051651,Time: 15449.78584933281\n",
      "Epoch: 947000,Train Loss: 0.55008465051651,Time: 15457.947201490402\n",
      "Epoch: 947500,Train Loss: 0.55008465051651,Time: 15466.186624288559\n",
      "Epoch: 948000,Train Loss: 0.55008465051651,Time: 15474.177470445633\n",
      "Epoch: 948500,Train Loss: 0.55008465051651,Time: 15482.279862642288\n",
      "Epoch: 949000,Train Loss: 0.55008465051651,Time: 15490.23136138916\n",
      "Epoch: 949500,Train Loss: 0.55008465051651,Time: 15498.46342086792\n",
      "Epoch: 950000,Train Loss: 0.55008465051651,Time: 15506.56668806076\n",
      "Epoch: 950500,Train Loss: 0.55008465051651,Time: 15514.84917140007\n",
      "Epoch: 951000,Train Loss: 0.55008465051651,Time: 15523.030725479126\n",
      "Epoch: 951500,Train Loss: 0.55008465051651,Time: 15531.307270765305\n",
      "Epoch: 952000,Train Loss: 0.5500845313072205,Time: 15539.712507724762\n",
      "Epoch: 952500,Train Loss: 0.5500844717025757,Time: 15548.012021780014\n",
      "Epoch: 953000,Train Loss: 0.5500844717025757,Time: 15556.315673828125\n",
      "Epoch: 953500,Train Loss: 0.5500844717025757,Time: 15564.686072826385\n",
      "Epoch: 954000,Train Loss: 0.5500844717025757,Time: 15572.910987615585\n",
      "Epoch: 954500,Train Loss: 0.5500844717025757,Time: 15581.352882146835\n",
      "Epoch: 955000,Train Loss: 0.5500844717025757,Time: 15589.453409671783\n",
      "Epoch: 955500,Train Loss: 0.5500844717025757,Time: 15597.66067481041\n",
      "Epoch: 956000,Train Loss: 0.5500844717025757,Time: 15605.504883527756\n",
      "Epoch: 956500,Train Loss: 0.5500844717025757,Time: 15613.64570426941\n",
      "Epoch: 957000,Train Loss: 0.5500844717025757,Time: 15622.015827178955\n",
      "Epoch: 957500,Train Loss: 0.5500844717025757,Time: 15630.13272023201\n",
      "Epoch: 958000,Train Loss: 0.5500844717025757,Time: 15638.320606946945\n",
      "Epoch: 958500,Train Loss: 0.5500844717025757,Time: 15646.393181800842\n",
      "Epoch: 959000,Train Loss: 0.5500844717025757,Time: 15654.611920595169\n",
      "Epoch: 959500,Train Loss: 0.5500844717025757,Time: 15662.812337875366\n",
      "Epoch: 960000,Train Loss: 0.5500844717025757,Time: 15671.209543943405\n",
      "Epoch: 960500,Train Loss: 0.5500844717025757,Time: 15679.141827344894\n",
      "Epoch: 961000,Train Loss: 0.5500844120979309,Time: 15687.189002037048\n",
      "Epoch: 961500,Train Loss: 0.5500843524932861,Time: 15695.53764796257\n",
      "Epoch: 962000,Train Loss: 0.5500843524932861,Time: 15703.681780576706\n",
      "Epoch: 962500,Train Loss: 0.5500843524932861,Time: 15711.828794717789\n",
      "Epoch: 963000,Train Loss: 0.5500843524932861,Time: 15719.909149885178\n",
      "Epoch: 963500,Train Loss: 0.5500843524932861,Time: 15727.898050308228\n",
      "Epoch: 964000,Train Loss: 0.5500843524932861,Time: 15736.133555173874\n",
      "Epoch: 964500,Train Loss: 0.5500843524932861,Time: 15744.229300498962\n",
      "Epoch: 965000,Train Loss: 0.5500843524932861,Time: 15752.287454605103\n",
      "Epoch: 965500,Train Loss: 0.5500843524932861,Time: 15760.115585565567\n",
      "Epoch: 966000,Train Loss: 0.5500843524932861,Time: 15768.254728794098\n",
      "Epoch: 966500,Train Loss: 0.5500843524932861,Time: 15776.26604104042\n",
      "Epoch: 967000,Train Loss: 0.5500843524932861,Time: 15784.456770420074\n",
      "Epoch: 967500,Train Loss: 0.5500843524932861,Time: 15792.533926010132\n",
      "Epoch: 968000,Train Loss: 0.5500843524932861,Time: 15801.238624572754\n",
      "Epoch: 968500,Train Loss: 0.5500842928886414,Time: 15809.220618009567\n",
      "Epoch: 969000,Train Loss: 0.5500842332839966,Time: 15817.3163459301\n",
      "Epoch: 969500,Train Loss: 0.5500842332839966,Time: 15825.38248348236\n",
      "Epoch: 970000,Train Loss: 0.5500842332839966,Time: 15833.768185377121\n",
      "Epoch: 970500,Train Loss: 0.5500842332839966,Time: 15841.870387077332\n",
      "Epoch: 971000,Train Loss: 0.5500842332839966,Time: 15850.120627641678\n",
      "Epoch: 971500,Train Loss: 0.5500842332839966,Time: 15858.23085474968\n",
      "Epoch: 972000,Train Loss: 0.5500842332839966,Time: 15866.553627490997\n",
      "Epoch: 972500,Train Loss: 0.5500842332839966,Time: 15874.572871208191\n",
      "Epoch: 973000,Train Loss: 0.5500842332839966,Time: 15882.91445350647\n",
      "Epoch: 973500,Train Loss: 0.5500842332839966,Time: 15891.031550168991\n",
      "Epoch: 974000,Train Loss: 0.5500842332839966,Time: 15899.20310997963\n",
      "Epoch: 974500,Train Loss: 0.5500842332839966,Time: 15907.344531297684\n",
      "Epoch: 975000,Train Loss: 0.5500842332839966,Time: 15915.58671760559\n",
      "Epoch: 975500,Train Loss: 0.5500842332839966,Time: 15923.832081794739\n",
      "Epoch: 976000,Train Loss: 0.5500842332839966,Time: 15931.90369963646\n",
      "Epoch: 976500,Train Loss: 0.5500842332839966,Time: 15940.116329431534\n",
      "Epoch: 977000,Train Loss: 0.5500841736793518,Time: 15948.276331186295\n",
      "Epoch: 977500,Train Loss: 0.550084114074707,Time: 15956.268024921417\n",
      "Epoch: 978000,Train Loss: 0.550084114074707,Time: 15964.390755653381\n",
      "Epoch: 978500,Train Loss: 0.550084114074707,Time: 15972.546015739441\n",
      "Epoch: 979000,Train Loss: 0.550084114074707,Time: 15981.028723478317\n",
      "Epoch: 979500,Train Loss: 0.550084114074707,Time: 15989.001666545868\n",
      "Epoch: 980000,Train Loss: 0.550084114074707,Time: 15997.35237121582\n",
      "Epoch: 980500,Train Loss: 0.550084114074707,Time: 16005.348153352737\n",
      "Epoch: 981000,Train Loss: 0.550084114074707,Time: 16013.512230873108\n",
      "Epoch: 981500,Train Loss: 0.550084114074707,Time: 16021.73567533493\n",
      "Epoch: 982000,Train Loss: 0.550084114074707,Time: 16029.862349033356\n",
      "Epoch: 982500,Train Loss: 0.550084114074707,Time: 16037.898225784302\n",
      "Epoch: 983000,Train Loss: 0.550084114074707,Time: 16046.439173936844\n",
      "Epoch: 983500,Train Loss: 0.550084114074707,Time: 16054.408955335617\n",
      "Epoch: 984000,Train Loss: 0.550084114074707,Time: 16062.72351694107\n",
      "Epoch: 984500,Train Loss: 0.5500840544700623,Time: 16070.787885189056\n",
      "Epoch: 985000,Train Loss: 0.550084114074707,Time: 16079.089147567749\n",
      "Epoch: 985500,Train Loss: 0.5500840544700623,Time: 16087.359023332596\n",
      "Epoch: 986000,Train Loss: 0.5500840544700623,Time: 16095.8457198143\n",
      "Epoch: 986500,Train Loss: 0.5500840544700623,Time: 16103.769328832626\n",
      "Epoch: 987000,Train Loss: 0.5500840544700623,Time: 16112.052976608276\n",
      "Epoch: 987500,Train Loss: 0.5500840544700623,Time: 16119.97464466095\n",
      "Epoch: 988000,Train Loss: 0.5500840544700623,Time: 16128.06864953041\n",
      "Epoch: 988500,Train Loss: 0.5500839948654175,Time: 16136.175699710846\n",
      "Epoch: 989000,Train Loss: 0.5500839948654175,Time: 16144.382803440094\n",
      "Epoch: 989500,Train Loss: 0.5500839948654175,Time: 16152.492447853088\n",
      "Epoch: 990000,Train Loss: 0.5500839948654175,Time: 16160.654968261719\n",
      "Epoch: 990500,Train Loss: 0.5500839948654175,Time: 16168.661759376526\n",
      "Epoch: 991000,Train Loss: 0.5500839948654175,Time: 16177.099779129028\n",
      "Epoch: 991500,Train Loss: 0.5500839948654175,Time: 16185.06248664856\n",
      "Epoch: 992000,Train Loss: 0.5500839948654175,Time: 16193.329877138138\n",
      "Epoch: 992500,Train Loss: 0.5500839352607727,Time: 16201.332552194595\n",
      "Epoch: 993000,Train Loss: 0.5500839352607727,Time: 16209.587755203247\n",
      "Epoch: 993500,Train Loss: 0.5500839352607727,Time: 16217.574481964111\n",
      "Epoch: 994000,Train Loss: 0.5500839352607727,Time: 16225.772237062454\n",
      "Epoch: 994500,Train Loss: 0.5500839352607727,Time: 16233.908100366592\n",
      "Epoch: 995000,Train Loss: 0.5500839352607727,Time: 16242.14379286766\n",
      "Epoch: 995500,Train Loss: 0.5500839352607727,Time: 16250.154643297195\n",
      "Epoch: 996000,Train Loss: 0.5500839352607727,Time: 16258.489057302475\n",
      "Epoch: 996500,Train Loss: 0.5500839352607727,Time: 16266.42772603035\n",
      "Epoch: 997000,Train Loss: 0.5500838756561279,Time: 16274.787638902664\n",
      "Epoch: 997500,Train Loss: 0.5500838756561279,Time: 16282.81891965866\n",
      "Epoch: 998000,Train Loss: 0.5500838756561279,Time: 16291.643305540085\n",
      "Epoch: 998500,Train Loss: 0.5500838160514832,Time: 16300.008660078049\n",
      "Epoch: 999000,Train Loss: 0.5500838160514832,Time: 16308.575253009796\n",
      "Epoch: 999500,Train Loss: 0.5500838160514832,Time: 16316.51617717743\n",
      "Epoch: 1000000,Train Loss: 0.5500838160514832,Time: 16324.797225475311\n",
      "Epoch: 1000500,Train Loss: 0.5500838160514832,Time: 16332.912282705307\n",
      "Epoch: 1001000,Train Loss: 0.5500838160514832,Time: 16341.1176404953\n",
      "Epoch: 1001500,Train Loss: 0.5500838160514832,Time: 16349.04539179802\n",
      "Epoch: 1002000,Train Loss: 0.5500838160514832,Time: 16357.244366407394\n",
      "Epoch: 1002500,Train Loss: 0.5500838160514832,Time: 16365.625803232193\n",
      "Epoch: 1003000,Train Loss: 0.5500838160514832,Time: 16374.030116081238\n",
      "Epoch: 1003500,Train Loss: 0.5500838160514832,Time: 16382.110589504242\n",
      "Epoch: 1004000,Train Loss: 0.5500837564468384,Time: 16390.735201597214\n",
      "Epoch: 1004500,Train Loss: 0.5500838160514832,Time: 16398.84558415413\n",
      "Epoch: 1005000,Train Loss: 0.5500837564468384,Time: 16407.092321634293\n",
      "Epoch: 1005500,Train Loss: 0.5500838160514832,Time: 16415.341278791428\n",
      "Epoch: 1006000,Train Loss: 0.5500838160514832,Time: 16423.545333385468\n",
      "Epoch: 1006500,Train Loss: 0.5500838160514832,Time: 16431.6556122303\n",
      "Epoch: 1007000,Train Loss: 0.5500837564468384,Time: 16439.81059908867\n",
      "Epoch: 1007500,Train Loss: 0.5500837564468384,Time: 16448.315505743027\n",
      "Epoch: 1008000,Train Loss: 0.5500836968421936,Time: 16456.76407289505\n",
      "Epoch: 1008500,Train Loss: 0.5500836968421936,Time: 16464.875858783722\n",
      "Epoch: 1009000,Train Loss: 0.5500836968421936,Time: 16473.358182430267\n",
      "Epoch: 1009500,Train Loss: 0.5500836968421936,Time: 16481.757727861404\n",
      "Epoch: 1010000,Train Loss: 0.5500836968421936,Time: 16490.302322149277\n",
      "Epoch: 1010500,Train Loss: 0.5500836968421936,Time: 16498.272073745728\n",
      "Epoch: 1011000,Train Loss: 0.5500836968421936,Time: 16506.383285999298\n",
      "Epoch: 1011500,Train Loss: 0.5500836968421936,Time: 16514.511697292328\n",
      "Epoch: 1012000,Train Loss: 0.5500836968421936,Time: 16522.884543418884\n",
      "Epoch: 1012500,Train Loss: 0.5500836968421936,Time: 16530.927537441254\n",
      "Epoch: 1013000,Train Loss: 0.5500836968421936,Time: 16539.07234597206\n",
      "Epoch: 1013500,Train Loss: 0.5500836968421936,Time: 16547.14403653145\n",
      "Epoch: 1014000,Train Loss: 0.5500836968421936,Time: 16555.3567135334\n",
      "Epoch: 1014500,Train Loss: 0.5500836372375488,Time: 16563.485184192657\n",
      "Epoch: 1015000,Train Loss: 0.5500836372375488,Time: 16571.76835489273\n",
      "Epoch: 1015500,Train Loss: 0.5500836372375488,Time: 16579.579708337784\n",
      "Epoch: 1016000,Train Loss: 0.5500836372375488,Time: 16588.020681858063\n",
      "Epoch: 1016500,Train Loss: 0.5500836372375488,Time: 16596.110471010208\n",
      "Epoch: 1017000,Train Loss: 0.5500836372375488,Time: 16604.163753032684\n",
      "Epoch: 1017500,Train Loss: 0.550083577632904,Time: 16612.520253419876\n",
      "Epoch: 1018000,Train Loss: 0.550083577632904,Time: 16620.770939350128\n",
      "Epoch: 1018500,Train Loss: 0.550083577632904,Time: 16628.916471481323\n",
      "Epoch: 1019000,Train Loss: 0.550083577632904,Time: 16637.216259241104\n",
      "Epoch: 1019500,Train Loss: 0.550083577632904,Time: 16645.169919252396\n",
      "Epoch: 1020000,Train Loss: 0.5500835180282593,Time: 16653.482304096222\n",
      "Epoch: 1020500,Train Loss: 0.550083577632904,Time: 16661.561547994614\n",
      "Epoch: 1021000,Train Loss: 0.550083577632904,Time: 16669.772638320923\n",
      "Epoch: 1021500,Train Loss: 0.5500835180282593,Time: 16677.73183965683\n",
      "Epoch: 1022000,Train Loss: 0.5500835180282593,Time: 16686.096331357956\n",
      "Epoch: 1022500,Train Loss: 0.5500835180282593,Time: 16694.062797546387\n",
      "Epoch: 1023000,Train Loss: 0.5500835180282593,Time: 16702.442974805832\n",
      "Epoch: 1023500,Train Loss: 0.5500835180282593,Time: 16710.497816562653\n",
      "Epoch: 1024000,Train Loss: 0.5500835180282593,Time: 16718.739802598953\n",
      "Epoch: 1024500,Train Loss: 0.5500834584236145,Time: 16726.835778474808\n",
      "Epoch: 1025000,Train Loss: 0.5500834584236145,Time: 16735.015765190125\n",
      "Epoch: 1025500,Train Loss: 0.5500834584236145,Time: 16743.102278470993\n",
      "Epoch: 1026000,Train Loss: 0.5500835180282593,Time: 16751.53879714012\n",
      "Epoch: 1026500,Train Loss: 0.5500834584236145,Time: 16759.47012066841\n",
      "Epoch: 1027000,Train Loss: 0.5500834584236145,Time: 16767.715017557144\n",
      "Epoch: 1027500,Train Loss: 0.5500834584236145,Time: 16775.60007405281\n",
      "Epoch: 1028000,Train Loss: 0.5500834584236145,Time: 16783.706472158432\n",
      "Epoch: 1028500,Train Loss: 0.5500833988189697,Time: 16791.864906072617\n",
      "Epoch: 1029000,Train Loss: 0.5500833988189697,Time: 16799.85444045067\n",
      "Epoch: 1029500,Train Loss: 0.5500833988189697,Time: 16808.328663110733\n",
      "Epoch: 1030000,Train Loss: 0.5500834584236145,Time: 16816.575003147125\n",
      "Epoch: 1030500,Train Loss: 0.5500833988189697,Time: 16825.434307813644\n",
      "Epoch: 1031000,Train Loss: 0.5500833988189697,Time: 16833.439442157745\n",
      "Epoch: 1031500,Train Loss: 0.5500833988189697,Time: 16841.46802663803\n",
      "Epoch: 1032000,Train Loss: 0.5500833988189697,Time: 16849.331332445145\n",
      "Epoch: 1032500,Train Loss: 0.5500833988189697,Time: 16857.695635318756\n",
      "Epoch: 1033000,Train Loss: 0.5500833988189697,Time: 16865.86878156662\n",
      "Epoch: 1033500,Train Loss: 0.5500833988189697,Time: 16873.990853071213\n",
      "Epoch: 1034000,Train Loss: 0.5500833988189697,Time: 16882.33635020256\n",
      "Epoch: 1034500,Train Loss: 0.5500833988189697,Time: 16890.582486867905\n",
      "Epoch: 1035000,Train Loss: 0.5500833988189697,Time: 16898.586534023285\n",
      "Epoch: 1035500,Train Loss: 0.5500833988189697,Time: 16907.374450206757\n",
      "Epoch: 1036000,Train Loss: 0.5500833988189697,Time: 16915.363039016724\n",
      "Epoch: 1036500,Train Loss: 0.5500833988189697,Time: 16923.598525047302\n",
      "Epoch: 1037000,Train Loss: 0.550083339214325,Time: 16931.932713270187\n",
      "Epoch: 1037500,Train Loss: 0.5500832796096802,Time: 16940.08394551277\n",
      "Epoch: 1038000,Train Loss: 0.550083339214325,Time: 16948.322032928467\n",
      "Epoch: 1038500,Train Loss: 0.5500832796096802,Time: 16956.526313066483\n",
      "Epoch: 1039000,Train Loss: 0.5500832796096802,Time: 16964.531205892563\n",
      "Epoch: 1039500,Train Loss: 0.5500832796096802,Time: 16972.7760181427\n",
      "Epoch: 1040000,Train Loss: 0.5500832796096802,Time: 16980.830948591232\n",
      "Epoch: 1040500,Train Loss: 0.5500832796096802,Time: 16989.276201486588\n",
      "Epoch: 1041000,Train Loss: 0.5500832796096802,Time: 16997.240253925323\n",
      "Epoch: 1041500,Train Loss: 0.5500832796096802,Time: 17005.464242219925\n",
      "Epoch: 1042000,Train Loss: 0.5500832796096802,Time: 17013.911723852158\n",
      "Epoch: 1042500,Train Loss: 0.5500832796096802,Time: 17022.135049819946\n",
      "Epoch: 1043000,Train Loss: 0.5500832796096802,Time: 17030.667734384537\n",
      "Epoch: 1043500,Train Loss: 0.5500832796096802,Time: 17039.33061313629\n",
      "Epoch: 1044000,Train Loss: 0.5500832796096802,Time: 17047.34133553505\n",
      "Epoch: 1044500,Train Loss: 0.5500832796096802,Time: 17055.54675102234\n",
      "Epoch: 1045000,Train Loss: 0.5500832796096802,Time: 17063.5197994709\n",
      "Epoch: 1045500,Train Loss: 0.5500832796096802,Time: 17071.79880475998\n",
      "Epoch: 1046000,Train Loss: 0.5500832796096802,Time: 17079.97143626213\n",
      "Epoch: 1046500,Train Loss: 0.5500831604003906,Time: 17088.23223876953\n",
      "Epoch: 1047000,Train Loss: 0.5500831604003906,Time: 17096.099350452423\n",
      "Epoch: 1047500,Train Loss: 0.5500831604003906,Time: 17104.319211244583\n",
      "Epoch: 1048000,Train Loss: 0.5500831604003906,Time: 17112.32990837097\n",
      "Epoch: 1048500,Train Loss: 0.5500831604003906,Time: 17120.630039930344\n",
      "Epoch: 1049000,Train Loss: 0.5500831604003906,Time: 17128.78875684738\n",
      "Epoch: 1049500,Train Loss: 0.5500831604003906,Time: 17137.046248674393\n",
      "Epoch: 1050000,Train Loss: 0.5500831604003906,Time: 17145.020206212997\n",
      "Epoch: 1050500,Train Loss: 0.5500831604003906,Time: 17153.081102609634\n",
      "Epoch: 1051000,Train Loss: 0.5500831604003906,Time: 17161.077615737915\n",
      "Epoch: 1051500,Train Loss: 0.5500831604003906,Time: 17169.26436972618\n",
      "Epoch: 1052000,Train Loss: 0.5500831604003906,Time: 17177.288116693497\n",
      "Epoch: 1052500,Train Loss: 0.5500831604003906,Time: 17185.585829734802\n",
      "Epoch: 1053000,Train Loss: 0.5500831604003906,Time: 17193.92627120018\n",
      "Epoch: 1053500,Train Loss: 0.5500831604003906,Time: 17202.11898779869\n",
      "Epoch: 1054000,Train Loss: 0.5500831604003906,Time: 17210.22246980667\n",
      "Epoch: 1054500,Train Loss: 0.5500831604003906,Time: 17218.778478860855\n",
      "Epoch: 1055000,Train Loss: 0.5500831604003906,Time: 17226.748448610306\n",
      "Epoch: 1055500,Train Loss: 0.5500831007957458,Time: 17234.963413715363\n",
      "Epoch: 1056000,Train Loss: 0.5500831007957458,Time: 17243.12193250656\n",
      "Epoch: 1056500,Train Loss: 0.5500831007957458,Time: 17251.292229890823\n",
      "Epoch: 1057000,Train Loss: 0.5500830411911011,Time: 17259.35484480858\n",
      "Epoch: 1057500,Train Loss: 0.5500831007957458,Time: 17267.632132053375\n",
      "Epoch: 1058000,Train Loss: 0.5500830411911011,Time: 17275.541375398636\n",
      "Epoch: 1058500,Train Loss: 0.5500830411911011,Time: 17283.795397758484\n",
      "Epoch: 1059000,Train Loss: 0.5500830411911011,Time: 17291.873762130737\n",
      "Epoch: 1059500,Train Loss: 0.5500830411911011,Time: 17300.067677497864\n",
      "Epoch: 1060000,Train Loss: 0.5500830411911011,Time: 17308.093899965286\n",
      "Epoch: 1060500,Train Loss: 0.5500830411911011,Time: 17316.875435829163\n",
      "Epoch: 1061000,Train Loss: 0.5500830411911011,Time: 17325.148559570312\n",
      "Epoch: 1061500,Train Loss: 0.5500830411911011,Time: 17333.342331647873\n",
      "Epoch: 1062000,Train Loss: 0.5500830411911011,Time: 17341.22403740883\n",
      "Epoch: 1062500,Train Loss: 0.5500830411911011,Time: 17349.273375988007\n",
      "Epoch: 1063000,Train Loss: 0.5500830411911011,Time: 17357.49810051918\n",
      "Epoch: 1063500,Train Loss: 0.5500830411911011,Time: 17365.87975358963\n",
      "Epoch: 1064000,Train Loss: 0.5500830411911011,Time: 17373.96897625923\n",
      "Epoch: 1064500,Train Loss: 0.5500830411911011,Time: 17382.010996580124\n",
      "Epoch: 1065000,Train Loss: 0.5500829815864563,Time: 17390.00993824005\n",
      "Epoch: 1065500,Train Loss: 0.5500829815864563,Time: 17398.2243206501\n",
      "Epoch: 1066000,Train Loss: 0.5500829815864563,Time: 17406.38631463051\n",
      "Epoch: 1066500,Train Loss: 0.5500829219818115,Time: 17414.53747177124\n",
      "Epoch: 1067000,Train Loss: 0.5500829219818115,Time: 17422.480676412582\n",
      "Epoch: 1067500,Train Loss: 0.5500829219818115,Time: 17430.671462774277\n",
      "Epoch: 1068000,Train Loss: 0.5500829219818115,Time: 17439.381272554398\n",
      "Epoch: 1068500,Train Loss: 0.5500829219818115,Time: 17447.534073114395\n",
      "Epoch: 1069000,Train Loss: 0.5500829219818115,Time: 17455.50422644615\n",
      "Epoch: 1069500,Train Loss: 0.5500829219818115,Time: 17463.793158769608\n",
      "Epoch: 1070000,Train Loss: 0.5500829219818115,Time: 17471.816256046295\n",
      "Epoch: 1070500,Train Loss: 0.5500829219818115,Time: 17480.070032835007\n",
      "Epoch: 1071000,Train Loss: 0.5500829219818115,Time: 17488.086201667786\n",
      "Epoch: 1071500,Train Loss: 0.5500829219818115,Time: 17496.24199962616\n",
      "Epoch: 1072000,Train Loss: 0.5500829219818115,Time: 17504.17845106125\n",
      "Epoch: 1072500,Train Loss: 0.5500829219818115,Time: 17512.404504060745\n",
      "Epoch: 1073000,Train Loss: 0.5500828623771667,Time: 17520.328078508377\n",
      "Epoch: 1073500,Train Loss: 0.5500829219818115,Time: 17528.551872253418\n",
      "Epoch: 1074000,Train Loss: 0.5500828623771667,Time: 17536.41428375244\n",
      "Epoch: 1074500,Train Loss: 0.5500829219818115,Time: 17544.651443004608\n",
      "Epoch: 1075000,Train Loss: 0.5500828623771667,Time: 17552.7210252285\n",
      "Epoch: 1075500,Train Loss: 0.5500828623771667,Time: 17560.878153324127\n",
      "Epoch: 1076000,Train Loss: 0.5500828623771667,Time: 17568.774513721466\n",
      "Epoch: 1076500,Train Loss: 0.5500828623771667,Time: 17576.91579580307\n",
      "Epoch: 1077000,Train Loss: 0.5500828623771667,Time: 17585.179062128067\n",
      "Epoch: 1077500,Train Loss: 0.550082802772522,Time: 17593.49213886261\n",
      "Epoch: 1078000,Train Loss: 0.550082802772522,Time: 17601.545657873154\n",
      "Epoch: 1078500,Train Loss: 0.550082802772522,Time: 17609.890719413757\n",
      "Epoch: 1079000,Train Loss: 0.550082802772522,Time: 17617.919064760208\n",
      "Epoch: 1079500,Train Loss: 0.550082802772522,Time: 17626.463742017746\n",
      "Epoch: 1080000,Train Loss: 0.550082802772522,Time: 17634.97190117836\n",
      "Epoch: 1080500,Train Loss: 0.550082802772522,Time: 17643.078207969666\n",
      "Epoch: 1081000,Train Loss: 0.5500827431678772,Time: 17650.936858177185\n",
      "Epoch: 1081500,Train Loss: 0.5500827431678772,Time: 17659.17286849022\n",
      "Epoch: 1082000,Train Loss: 0.5500827431678772,Time: 17667.296481609344\n",
      "Epoch: 1082500,Train Loss: 0.5500827431678772,Time: 17675.78154182434\n",
      "Epoch: 1083000,Train Loss: 0.5500827431678772,Time: 17683.995564460754\n",
      "Epoch: 1083500,Train Loss: 0.5500827431678772,Time: 17692.45317697525\n",
      "Epoch: 1084000,Train Loss: 0.5500827431678772,Time: 17700.470756053925\n",
      "Epoch: 1084500,Train Loss: 0.5500827431678772,Time: 17708.606776952744\n",
      "Epoch: 1085000,Train Loss: 0.5500827431678772,Time: 17716.716933965683\n",
      "Epoch: 1085500,Train Loss: 0.5500826835632324,Time: 17725.069635152817\n",
      "Epoch: 1086000,Train Loss: 0.5500826835632324,Time: 17733.223279714584\n",
      "Epoch: 1086500,Train Loss: 0.5500826835632324,Time: 17741.504243850708\n",
      "Epoch: 1087000,Train Loss: 0.5500826835632324,Time: 17749.492592573166\n",
      "Epoch: 1087500,Train Loss: 0.5500826835632324,Time: 17757.840226650238\n",
      "Epoch: 1088000,Train Loss: 0.5500826835632324,Time: 17765.95617890358\n",
      "Epoch: 1088500,Train Loss: 0.5500826835632324,Time: 17774.243796110153\n",
      "Epoch: 1089000,Train Loss: 0.5500826835632324,Time: 17782.226299762726\n",
      "Epoch: 1089500,Train Loss: 0.5500826835632324,Time: 17790.40601992607\n",
      "Epoch: 1090000,Train Loss: 0.5500826835632324,Time: 17798.60865020752\n",
      "Epoch: 1090500,Train Loss: 0.5500826835632324,Time: 17806.957136392593\n",
      "Epoch: 1091000,Train Loss: 0.5500826239585876,Time: 17815.067393302917\n",
      "Epoch: 1091500,Train Loss: 0.5500826239585876,Time: 17823.25716638565\n",
      "Epoch: 1092000,Train Loss: 0.5500826835632324,Time: 17831.30849981308\n",
      "Epoch: 1092500,Train Loss: 0.5500826239585876,Time: 17840.341282606125\n",
      "Epoch: 1093000,Train Loss: 0.5500826239585876,Time: 17848.334002256393\n",
      "Epoch: 1093500,Train Loss: 0.5500826239585876,Time: 17856.59793829918\n",
      "Epoch: 1094000,Train Loss: 0.5500826239585876,Time: 17864.685267210007\n",
      "Epoch: 1094500,Train Loss: 0.5500826239585876,Time: 17872.918522834778\n",
      "Epoch: 1095000,Train Loss: 0.5500826239585876,Time: 17880.900253534317\n",
      "Epoch: 1095500,Train Loss: 0.5500826239585876,Time: 17889.24886059761\n",
      "Epoch: 1096000,Train Loss: 0.5500825643539429,Time: 17897.39625453949\n",
      "Epoch: 1096500,Train Loss: 0.5500825643539429,Time: 17905.706310749054\n",
      "Epoch: 1097000,Train Loss: 0.5500826239585876,Time: 17913.735636472702\n",
      "Epoch: 1097500,Train Loss: 0.5500825643539429,Time: 17921.900196313858\n",
      "Epoch: 1098000,Train Loss: 0.5500825643539429,Time: 17929.9384868145\n",
      "Epoch: 1098500,Train Loss: 0.5500825643539429,Time: 17938.146524190903\n",
      "Epoch: 1099000,Train Loss: 0.5500825643539429,Time: 17945.998349666595\n",
      "Epoch: 1099500,Train Loss: 0.5500825643539429,Time: 17954.3044090271\n",
      "Epoch: 1100000,Train Loss: 0.5500825643539429,Time: 17962.568635225296\n",
      "Epoch: 1100500,Train Loss: 0.5500825643539429,Time: 17970.52947998047\n",
      "Epoch: 1101000,Train Loss: 0.5500825643539429,Time: 17978.632755041122\n",
      "Epoch: 1101500,Train Loss: 0.5500825643539429,Time: 17986.57661151886\n",
      "Epoch: 1102000,Train Loss: 0.5500825047492981,Time: 17994.77742576599\n",
      "Epoch: 1102500,Train Loss: 0.5500825643539429,Time: 18002.845643281937\n",
      "Epoch: 1103000,Train Loss: 0.5500825643539429,Time: 18011.065335273743\n",
      "Epoch: 1103500,Train Loss: 0.5500825047492981,Time: 18019.168194293976\n",
      "Epoch: 1104000,Train Loss: 0.5500825047492981,Time: 18027.357199907303\n",
      "Epoch: 1104500,Train Loss: 0.5500825047492981,Time: 18035.426277399063\n",
      "Epoch: 1105000,Train Loss: 0.5500825047492981,Time: 18043.840834140778\n",
      "Epoch: 1105500,Train Loss: 0.5500825047492981,Time: 18051.819464683533\n",
      "Epoch: 1106000,Train Loss: 0.5500825047492981,Time: 18060.01841688156\n",
      "Epoch: 1106500,Train Loss: 0.5500825047492981,Time: 18068.077519655228\n",
      "Epoch: 1107000,Train Loss: 0.5500825047492981,Time: 18076.209226608276\n",
      "Epoch: 1107500,Train Loss: 0.5500824451446533,Time: 18084.425457000732\n",
      "Epoch: 1108000,Train Loss: 0.5500824451446533,Time: 18092.65540242195\n",
      "Epoch: 1108500,Train Loss: 0.5500824451446533,Time: 18100.554827451706\n",
      "Epoch: 1109000,Train Loss: 0.5500824451446533,Time: 18108.69521522522\n",
      "Epoch: 1109500,Train Loss: 0.5500824451446533,Time: 18116.66669178009\n",
      "Epoch: 1110000,Train Loss: 0.5500824451446533,Time: 18125.06055831909\n",
      "Epoch: 1110500,Train Loss: 0.5500824451446533,Time: 18133.043903827667\n",
      "Epoch: 1111000,Train Loss: 0.5500824451446533,Time: 18141.83679127693\n",
      "Epoch: 1111500,Train Loss: 0.5500824451446533,Time: 18149.87889790535\n",
      "Epoch: 1112000,Train Loss: 0.5500824451446533,Time: 18158.10492014885\n",
      "Epoch: 1112500,Train Loss: 0.5500824451446533,Time: 18166.254745721817\n",
      "Epoch: 1113000,Train Loss: 0.5500824451446533,Time: 18174.57291674614\n",
      "Epoch: 1113500,Train Loss: 0.5500824451446533,Time: 18182.7273709774\n",
      "Epoch: 1114000,Train Loss: 0.5500824451446533,Time: 18191.10752105713\n",
      "Epoch: 1114500,Train Loss: 0.5500823855400085,Time: 18199.02174091339\n",
      "Epoch: 1115000,Train Loss: 0.5500823855400085,Time: 18207.360726356506\n",
      "Epoch: 1115500,Train Loss: 0.5500824451446533,Time: 18215.336841583252\n",
      "Epoch: 1116000,Train Loss: 0.5500823855400085,Time: 18223.49395799637\n",
      "Epoch: 1116500,Train Loss: 0.5500824451446533,Time: 18231.60826396942\n",
      "Epoch: 1117000,Train Loss: 0.5500823855400085,Time: 18239.975427865982\n",
      "Epoch: 1117500,Train Loss: 0.5500823855400085,Time: 18247.838958501816\n",
      "Epoch: 1118000,Train Loss: 0.5500823259353638,Time: 18255.99484062195\n",
      "Epoch: 1118500,Train Loss: 0.5500823259353638,Time: 18264.247688531876\n",
      "Epoch: 1119000,Train Loss: 0.5500823259353638,Time: 18272.351596593857\n",
      "Epoch: 1119500,Train Loss: 0.5500823259353638,Time: 18280.69487786293\n",
      "Epoch: 1120000,Train Loss: 0.5500823259353638,Time: 18288.889477968216\n",
      "Epoch: 1120500,Train Loss: 0.5500823259353638,Time: 18297.19127345085\n",
      "Epoch: 1121000,Train Loss: 0.5500823259353638,Time: 18305.692810058594\n",
      "Epoch: 1121500,Train Loss: 0.5500823259353638,Time: 18313.8300614357\n",
      "Epoch: 1122000,Train Loss: 0.5500823259353638,Time: 18321.975217342377\n",
      "Epoch: 1122500,Train Loss: 0.5500823259353638,Time: 18329.970580101013\n",
      "Epoch: 1123000,Train Loss: 0.5500823259353638,Time: 18338.577953100204\n",
      "Epoch: 1123500,Train Loss: 0.5500823259353638,Time: 18346.844282627106\n",
      "Epoch: 1124000,Train Loss: 0.5500823259353638,Time: 18355.148616552353\n",
      "Epoch: 1124500,Train Loss: 0.5500823259353638,Time: 18363.14982318878\n",
      "Epoch: 1125000,Train Loss: 0.5500823259353638,Time: 18371.395362854004\n",
      "Epoch: 1125500,Train Loss: 0.5500823259353638,Time: 18379.44727921486\n",
      "Epoch: 1126000,Train Loss: 0.5500823259353638,Time: 18387.675348997116\n",
      "Epoch: 1126500,Train Loss: 0.5500823259353638,Time: 18395.765513420105\n",
      "Epoch: 1127000,Train Loss: 0.5500823259353638,Time: 18404.113884449005\n",
      "Epoch: 1127500,Train Loss: 0.5500823259353638,Time: 18412.65067434311\n",
      "Epoch: 1128000,Train Loss: 0.5500823259353638,Time: 18420.74327802658\n",
      "Epoch: 1128500,Train Loss: 0.5500823259353638,Time: 18429.142732858658\n",
      "Epoch: 1129000,Train Loss: 0.550082266330719,Time: 18437.74423480034\n",
      "Epoch: 1129500,Train Loss: 0.550082266330719,Time: 18446.116686582565\n",
      "Epoch: 1130000,Train Loss: 0.550082266330719,Time: 18454.23366165161\n",
      "Epoch: 1130500,Train Loss: 0.550082266330719,Time: 18462.195021152496\n",
      "Epoch: 1131000,Train Loss: 0.5500822067260742,Time: 18470.55600309372\n",
      "Epoch: 1131500,Train Loss: 0.550082266330719,Time: 18478.683683395386\n",
      "Epoch: 1132000,Train Loss: 0.5500822067260742,Time: 18486.888830900192\n",
      "Epoch: 1132500,Train Loss: 0.5500822067260742,Time: 18494.913985729218\n",
      "Epoch: 1133000,Train Loss: 0.5500822067260742,Time: 18503.144042491913\n",
      "Epoch: 1133500,Train Loss: 0.5500822067260742,Time: 18511.328435897827\n",
      "Epoch: 1134000,Train Loss: 0.5500822067260742,Time: 18519.805574417114\n",
      "Epoch: 1134500,Train Loss: 0.5500822067260742,Time: 18527.83522105217\n",
      "Epoch: 1135000,Train Loss: 0.5500822067260742,Time: 18535.948608636856\n",
      "Epoch: 1135500,Train Loss: 0.5500822067260742,Time: 18543.97149538994\n",
      "Epoch: 1136000,Train Loss: 0.5500822067260742,Time: 18552.3911550045\n",
      "Epoch: 1136500,Train Loss: 0.5500822067260742,Time: 18560.456930160522\n",
      "Epoch: 1137000,Train Loss: 0.5500822067260742,Time: 18568.616793870926\n",
      "Epoch: 1137500,Train Loss: 0.5500822067260742,Time: 18576.549178361893\n",
      "Epoch: 1138000,Train Loss: 0.5500822067260742,Time: 18584.698098421097\n",
      "Epoch: 1138500,Train Loss: 0.5500822067260742,Time: 18592.779438972473\n",
      "Epoch: 1139000,Train Loss: 0.5500822067260742,Time: 18601.118670225143\n",
      "Epoch: 1139500,Train Loss: 0.5500822067260742,Time: 18609.12692642212\n",
      "Epoch: 1140000,Train Loss: 0.5500822067260742,Time: 18617.267339468002\n",
      "Epoch: 1140500,Train Loss: 0.5500822067260742,Time: 18625.224641561508\n",
      "Epoch: 1141000,Train Loss: 0.5500822067260742,Time: 18633.391217708588\n",
      "Epoch: 1141500,Train Loss: 0.5500822067260742,Time: 18641.362196922302\n",
      "Epoch: 1142000,Train Loss: 0.5500822067260742,Time: 18649.69573020935\n",
      "Epoch: 1142500,Train Loss: 0.5500822067260742,Time: 18657.883519887924\n",
      "Epoch: 1143000,Train Loss: 0.5500822067260742,Time: 18666.290192842484\n",
      "Epoch: 1143500,Train Loss: 0.5500822067260742,Time: 18674.248492717743\n",
      "Epoch: 1144000,Train Loss: 0.5500822067260742,Time: 18682.582397699356\n",
      "Epoch: 1144500,Train Loss: 0.5500820279121399,Time: 18690.542613983154\n",
      "Epoch: 1145000,Train Loss: 0.5500820279121399,Time: 18698.841396570206\n",
      "Epoch: 1145500,Train Loss: 0.5500820279121399,Time: 18707.788394212723\n",
      "Epoch: 1146000,Train Loss: 0.5500820279121399,Time: 18715.998778820038\n",
      "Epoch: 1146500,Train Loss: 0.5500820279121399,Time: 18724.389464855194\n",
      "Epoch: 1147000,Train Loss: 0.5500820279121399,Time: 18732.73898124695\n",
      "Epoch: 1147500,Train Loss: 0.5500820279121399,Time: 18740.96704006195\n",
      "Epoch: 1148000,Train Loss: 0.5500820279121399,Time: 18749.408281326294\n",
      "Epoch: 1148500,Train Loss: 0.5500820279121399,Time: 18757.844057798386\n",
      "Epoch: 1149000,Train Loss: 0.5500820279121399,Time: 18766.18707227707\n",
      "Epoch: 1149500,Train Loss: 0.5500820279121399,Time: 18774.38053917885\n",
      "Epoch: 1150000,Train Loss: 0.5500820279121399,Time: 18782.477879285812\n",
      "Epoch: 1150500,Train Loss: 0.5500820279121399,Time: 18790.518226385117\n",
      "Epoch: 1151000,Train Loss: 0.5500820279121399,Time: 18798.726679563522\n",
      "Epoch: 1151500,Train Loss: 0.5500820279121399,Time: 18806.721361637115\n",
      "Epoch: 1152000,Train Loss: 0.5500820279121399,Time: 18814.91098833084\n",
      "Epoch: 1152500,Train Loss: 0.5500820279121399,Time: 18822.86358523369\n",
      "Epoch: 1153000,Train Loss: 0.5500819683074951,Time: 18831.035836696625\n",
      "Epoch: 1153500,Train Loss: 0.5500819683074951,Time: 18839.17882657051\n",
      "Epoch: 1154000,Train Loss: 0.5500819683074951,Time: 18847.40541577339\n",
      "Epoch: 1154500,Train Loss: 0.5500819683074951,Time: 18855.731536388397\n",
      "Epoch: 1155000,Train Loss: 0.5500819683074951,Time: 18863.928929567337\n",
      "Epoch: 1155500,Train Loss: 0.5500819683074951,Time: 18871.82314801216\n",
      "Epoch: 1156000,Train Loss: 0.5500819683074951,Time: 18880.185458421707\n",
      "Epoch: 1156500,Train Loss: 0.5500819683074951,Time: 18888.378997802734\n",
      "Epoch: 1157000,Train Loss: 0.5500819683074951,Time: 18896.607673168182\n",
      "Epoch: 1157500,Train Loss: 0.5500819683074951,Time: 18904.636184453964\n",
      "Epoch: 1158000,Train Loss: 0.5500819683074951,Time: 18912.90113592148\n",
      "Epoch: 1158500,Train Loss: 0.5500819683074951,Time: 18921.082166194916\n",
      "Epoch: 1159000,Train Loss: 0.5500819087028503,Time: 18929.498259305954\n",
      "Epoch: 1159500,Train Loss: 0.5500819683074951,Time: 18937.6275472641\n",
      "Epoch: 1160000,Train Loss: 0.5500819683074951,Time: 18946.058623552322\n",
      "Epoch: 1160500,Train Loss: 0.5500819683074951,Time: 18954.15473651886\n",
      "Epoch: 1161000,Train Loss: 0.5500819087028503,Time: 18962.83731818199\n",
      "Epoch: 1161500,Train Loss: 0.5500819087028503,Time: 18971.127382993698\n",
      "Epoch: 1162000,Train Loss: 0.5500819087028503,Time: 18979.4755756855\n",
      "Epoch: 1162500,Train Loss: 0.5500819087028503,Time: 18987.580571889877\n",
      "Epoch: 1163000,Train Loss: 0.5500819087028503,Time: 18996.095402240753\n",
      "Epoch: 1163500,Train Loss: 0.5500819087028503,Time: 19004.21024632454\n",
      "Epoch: 1164000,Train Loss: 0.5500818490982056,Time: 19012.371708631516\n",
      "Epoch: 1164500,Train Loss: 0.5500819087028503,Time: 19020.46883916855\n",
      "Epoch: 1165000,Train Loss: 0.5500818490982056,Time: 19028.66547203064\n",
      "Epoch: 1165500,Train Loss: 0.5500818490982056,Time: 19036.692499399185\n",
      "Epoch: 1166000,Train Loss: 0.5500818490982056,Time: 19045.630207538605\n",
      "Epoch: 1166500,Train Loss: 0.5500818490982056,Time: 19053.623804807663\n",
      "Epoch: 1167000,Train Loss: 0.5500817894935608,Time: 19061.9632062912\n",
      "Epoch: 1167500,Train Loss: 0.5500818490982056,Time: 19069.953258275986\n",
      "Epoch: 1168000,Train Loss: 0.5500818490982056,Time: 19078.55843448639\n",
      "Epoch: 1168500,Train Loss: 0.5500818490982056,Time: 19086.478063583374\n",
      "Epoch: 1169000,Train Loss: 0.5500818490982056,Time: 19094.804140090942\n",
      "Epoch: 1169500,Train Loss: 0.5500818490982056,Time: 19102.704734802246\n",
      "Epoch: 1170000,Train Loss: 0.5500818490982056,Time: 19110.900785684586\n",
      "Epoch: 1170500,Train Loss: 0.5500818490982056,Time: 19118.947194099426\n",
      "Epoch: 1171000,Train Loss: 0.5500817894935608,Time: 19127.271918058395\n",
      "Epoch: 1171500,Train Loss: 0.5500817894935608,Time: 19135.405597686768\n",
      "Epoch: 1172000,Train Loss: 0.5500817894935608,Time: 19143.926271677017\n",
      "Epoch: 1172500,Train Loss: 0.5500817894935608,Time: 19152.56897687912\n",
      "Epoch: 1173000,Train Loss: 0.5500817894935608,Time: 19160.98062825203\n",
      "Epoch: 1173500,Train Loss: 0.5500817894935608,Time: 19169.397656679153\n",
      "Epoch: 1174000,Train Loss: 0.5500817894935608,Time: 19177.815268993378\n",
      "Epoch: 1174500,Train Loss: 0.5500817894935608,Time: 19186.211529493332\n",
      "Epoch: 1175000,Train Loss: 0.5500817894935608,Time: 19194.302716493607\n",
      "Epoch: 1175500,Train Loss: 0.5500817894935608,Time: 19202.5235517025\n",
      "Epoch: 1176000,Train Loss: 0.5500817894935608,Time: 19210.56525850296\n",
      "Epoch: 1176500,Train Loss: 0.5500817894935608,Time: 19218.755559921265\n",
      "Epoch: 1177000,Train Loss: 0.5500817894935608,Time: 19226.7881295681\n",
      "Epoch: 1177500,Train Loss: 0.5500817894935608,Time: 19235.340450525284\n",
      "Epoch: 1178000,Train Loss: 0.5500817894935608,Time: 19243.277228355408\n",
      "Epoch: 1178500,Train Loss: 0.5500817894935608,Time: 19251.37136387825\n",
      "Epoch: 1179000,Train Loss: 0.5500817894935608,Time: 19259.456584692\n",
      "Epoch: 1179500,Train Loss: 0.5500817894935608,Time: 19267.517111063004\n",
      "Epoch: 1180000,Train Loss: 0.5500817894935608,Time: 19276.21659708023\n",
      "Epoch: 1180500,Train Loss: 0.5500817894935608,Time: 19284.90359187126\n",
      "Epoch: 1181000,Train Loss: 0.5500816702842712,Time: 19292.94249868393\n",
      "Epoch: 1181500,Train Loss: 0.550081729888916,Time: 19301.14113497734\n",
      "Epoch: 1182000,Train Loss: 0.5500816702842712,Time: 19309.104437351227\n",
      "Epoch: 1182500,Train Loss: 0.5500817894935608,Time: 19317.1681432724\n",
      "Epoch: 1183000,Train Loss: 0.5500816702842712,Time: 19324.991639137268\n",
      "Epoch: 1183500,Train Loss: 0.5500816702842712,Time: 19333.020064353943\n",
      "Epoch: 1184000,Train Loss: 0.5500816702842712,Time: 19341.537527561188\n",
      "Epoch: 1184500,Train Loss: 0.5500816702842712,Time: 19350.39979314804\n",
      "Epoch: 1185000,Train Loss: 0.5500816702842712,Time: 19358.53749704361\n",
      "Epoch: 1185500,Train Loss: 0.5500816702842712,Time: 19367.00804758072\n",
      "Epoch: 1186000,Train Loss: 0.5500816702842712,Time: 19375.306707143784\n",
      "Epoch: 1186500,Train Loss: 0.5500816702842712,Time: 19383.815816640854\n",
      "Epoch: 1187000,Train Loss: 0.5500816702842712,Time: 19392.403327941895\n",
      "Epoch: 1187500,Train Loss: 0.5500816702842712,Time: 19400.78117108345\n",
      "Epoch: 1188000,Train Loss: 0.5500816702842712,Time: 19409.05581665039\n",
      "Epoch: 1188500,Train Loss: 0.5500816702842712,Time: 19417.226657390594\n",
      "Epoch: 1189000,Train Loss: 0.5500816702842712,Time: 19425.23588514328\n",
      "Epoch: 1189500,Train Loss: 0.5500816702842712,Time: 19433.551865577698\n",
      "Epoch: 1190000,Train Loss: 0.5500816702842712,Time: 19441.771535873413\n",
      "Epoch: 1190500,Train Loss: 0.5500816702842712,Time: 19450.16040802002\n",
      "Epoch: 1191000,Train Loss: 0.5500816702842712,Time: 19458.17052578926\n",
      "Epoch: 1191500,Train Loss: 0.5500816702842712,Time: 19466.3425488472\n",
      "Epoch: 1192000,Train Loss: 0.5500816702842712,Time: 19474.296805620193\n",
      "Epoch: 1192500,Train Loss: 0.5500816702842712,Time: 19482.21629834175\n",
      "Epoch: 1193000,Train Loss: 0.5500816702842712,Time: 19490.36185526848\n",
      "Epoch: 1193500,Train Loss: 0.5500816702842712,Time: 19498.491407871246\n",
      "Epoch: 1194000,Train Loss: 0.5500816702842712,Time: 19506.55421447754\n",
      "Epoch: 1194500,Train Loss: 0.5500816702842712,Time: 19514.649850845337\n",
      "Epoch: 1195000,Train Loss: 0.5500816106796265,Time: 19522.9017932415\n",
      "Epoch: 1195500,Train Loss: 0.5500815510749817,Time: 19531.238704442978\n",
      "Epoch: 1196000,Train Loss: 0.5500816106796265,Time: 19539.230835199356\n",
      "Epoch: 1196500,Train Loss: 0.5500816106796265,Time: 19547.712977170944\n",
      "Epoch: 1197000,Train Loss: 0.5500815510749817,Time: 19555.688024044037\n",
      "Epoch: 1197500,Train Loss: 0.5500815510749817,Time: 19563.962800502777\n",
      "Epoch: 1198000,Train Loss: 0.5500816106796265,Time: 19572.039139032364\n",
      "Epoch: 1198500,Train Loss: 0.5500815510749817,Time: 19580.185433387756\n",
      "Epoch: 1199000,Train Loss: 0.5500815510749817,Time: 19588.16499900818\n",
      "Epoch: 1199500,Train Loss: 0.5500815510749817,Time: 19596.318571805954\n",
      "Epoch: 1200000,Train Loss: 0.5500815510749817,Time: 19604.49335885048\n",
      "Epoch: 1200500,Train Loss: 0.5500815510749817,Time: 19612.789778232574\n",
      "Epoch: 1201000,Train Loss: 0.5500815510749817,Time: 19620.70213317871\n",
      "Epoch: 1201500,Train Loss: 0.5500815510749817,Time: 19628.95026731491\n",
      "Epoch: 1202000,Train Loss: 0.5500815510749817,Time: 19637.22831916809\n",
      "Epoch: 1202500,Train Loss: 0.5500815510749817,Time: 19645.576856136322\n",
      "Epoch: 1203000,Train Loss: 0.5500815510749817,Time: 19653.81538581848\n",
      "Epoch: 1203500,Train Loss: 0.5500815510749817,Time: 19662.11141204834\n",
      "Epoch: 1204000,Train Loss: 0.5500815510749817,Time: 19670.146784305573\n",
      "Epoch: 1204500,Train Loss: 0.5500815510749817,Time: 19678.346795797348\n",
      "Epoch: 1205000,Train Loss: 0.5500815510749817,Time: 19686.55969810486\n",
      "Epoch: 1205500,Train Loss: 0.5500815510749817,Time: 19694.785474538803\n",
      "Epoch: 1206000,Train Loss: 0.5500815510749817,Time: 19702.81780743599\n",
      "Epoch: 1206500,Train Loss: 0.5500815510749817,Time: 19711.183406591415\n",
      "Epoch: 1207000,Train Loss: 0.5500815510749817,Time: 19719.34399175644\n",
      "Epoch: 1207500,Train Loss: 0.5500815510749817,Time: 19727.61432313919\n",
      "Epoch: 1208000,Train Loss: 0.5500814318656921,Time: 19735.747091531754\n",
      "Epoch: 1208500,Train Loss: 0.5500815510749817,Time: 19744.099375724792\n",
      "Epoch: 1209000,Train Loss: 0.5500814318656921,Time: 19752.63761639595\n",
      "Epoch: 1209500,Train Loss: 0.5500815510749817,Time: 19760.962818145752\n",
      "Epoch: 1210000,Train Loss: 0.5500815510749817,Time: 19768.896425962448\n",
      "Epoch: 1210500,Train Loss: 0.5500815510749817,Time: 19777.14768218994\n",
      "Epoch: 1211000,Train Loss: 0.5500815510749817,Time: 19785.079752206802\n",
      "Epoch: 1211500,Train Loss: 0.5500814318656921,Time: 19793.391598701477\n",
      "Epoch: 1212000,Train Loss: 0.5500814318656921,Time: 19801.392973661423\n",
      "Epoch: 1212500,Train Loss: 0.5500814318656921,Time: 19809.653919935226\n",
      "Epoch: 1213000,Train Loss: 0.5500814318656921,Time: 19817.70914697647\n",
      "Epoch: 1213500,Train Loss: 0.5500814318656921,Time: 19825.950586795807\n",
      "Epoch: 1214000,Train Loss: 0.5500814318656921,Time: 19833.753425836563\n",
      "Epoch: 1214500,Train Loss: 0.5500814318656921,Time: 19841.938542842865\n",
      "Epoch: 1215000,Train Loss: 0.5500814318656921,Time: 19850.172598838806\n",
      "Epoch: 1215500,Train Loss: 0.5500814318656921,Time: 19858.391650676727\n",
      "Epoch: 1216000,Train Loss: 0.5500814318656921,Time: 19866.780783176422\n",
      "Epoch: 1216500,Train Loss: 0.5500814318656921,Time: 19875.257380962372\n",
      "Epoch: 1217000,Train Loss: 0.5500814318656921,Time: 19883.52972459793\n",
      "Epoch: 1217500,Train Loss: 0.5500814318656921,Time: 19892.203604459763\n",
      "Epoch: 1218000,Train Loss: 0.5500814318656921,Time: 19900.237983942032\n",
      "Epoch: 1218500,Train Loss: 0.5500814318656921,Time: 19908.51966881752\n",
      "Epoch: 1219000,Train Loss: 0.5500814318656921,Time: 19916.749982595444\n",
      "Epoch: 1219500,Train Loss: 0.5500814318656921,Time: 19924.826018810272\n",
      "Epoch: 1220000,Train Loss: 0.5500814318656921,Time: 19932.971093177795\n",
      "Epoch: 1220500,Train Loss: 0.5500814318656921,Time: 19941.25891304016\n",
      "Epoch: 1221000,Train Loss: 0.5500814318656921,Time: 19949.139080524445\n",
      "Epoch: 1221500,Train Loss: 0.5500814318656921,Time: 19957.244971752167\n",
      "Epoch: 1222000,Train Loss: 0.5500814318656921,Time: 19965.293686389923\n",
      "Epoch: 1222500,Train Loss: 0.5500813126564026,Time: 19973.591409683228\n",
      "Epoch: 1223000,Train Loss: 0.5500813722610474,Time: 19981.831117153168\n",
      "Epoch: 1223500,Train Loss: 0.5500813126564026,Time: 19989.989703655243\n",
      "Epoch: 1224000,Train Loss: 0.5500813126564026,Time: 19998.10728740692\n",
      "Epoch: 1224500,Train Loss: 0.5500813126564026,Time: 20006.545741796494\n",
      "Epoch: 1225000,Train Loss: 0.5500813126564026,Time: 20014.881113529205\n",
      "Epoch: 1225500,Train Loss: 0.5500813126564026,Time: 20023.11785006523\n",
      "Epoch: 1226000,Train Loss: 0.5500813126564026,Time: 20031.0208902359\n",
      "Epoch: 1226500,Train Loss: 0.5500813126564026,Time: 20039.55344581604\n",
      "Epoch: 1227000,Train Loss: 0.5500813126564026,Time: 20047.71761226654\n",
      "Epoch: 1227500,Train Loss: 0.5500813126564026,Time: 20056.083552122116\n",
      "Epoch: 1228000,Train Loss: 0.5500813126564026,Time: 20064.14557981491\n",
      "Epoch: 1228500,Train Loss: 0.5500813126564026,Time: 20072.647366523743\n",
      "Epoch: 1229000,Train Loss: 0.5500813126564026,Time: 20080.898819446564\n",
      "Epoch: 1229500,Train Loss: 0.5500813126564026,Time: 20089.20131254196\n",
      "Epoch: 1230000,Train Loss: 0.5500813126564026,Time: 20097.298028945923\n",
      "Epoch: 1230500,Train Loss: 0.5500813126564026,Time: 20105.51487326622\n",
      "Epoch: 1231000,Train Loss: 0.5500813126564026,Time: 20113.88453602791\n",
      "Epoch: 1231500,Train Loss: 0.5500813126564026,Time: 20122.39549422264\n",
      "Epoch: 1232000,Train Loss: 0.5500813126564026,Time: 20130.414110660553\n",
      "Epoch: 1232500,Train Loss: 0.5500813126564026,Time: 20138.904601097107\n",
      "Epoch: 1233000,Train Loss: 0.5500813126564026,Time: 20147.14794421196\n",
      "Epoch: 1233500,Train Loss: 0.5500813126564026,Time: 20155.534496068954\n",
      "Epoch: 1234000,Train Loss: 0.5500813126564026,Time: 20163.566746234894\n",
      "Epoch: 1234500,Train Loss: 0.5500813126564026,Time: 20172.35115623474\n",
      "Epoch: 1235000,Train Loss: 0.5500812530517578,Time: 20180.45205116272\n",
      "Epoch: 1235500,Train Loss: 0.5500812530517578,Time: 20189.119151830673\n",
      "Epoch: 1236000,Train Loss: 0.5500812530517578,Time: 20197.108924150467\n",
      "Epoch: 1236500,Train Loss: 0.5500812530517578,Time: 20205.308374643326\n",
      "Epoch: 1237000,Train Loss: 0.5500812530517578,Time: 20213.466254472733\n",
      "Epoch: 1237500,Train Loss: 0.5500812530517578,Time: 20221.83672785759\n",
      "Epoch: 1238000,Train Loss: 0.5500812530517578,Time: 20229.939860105515\n",
      "Epoch: 1238500,Train Loss: 0.5500812530517578,Time: 20238.150628566742\n",
      "Epoch: 1239000,Train Loss: 0.5500812530517578,Time: 20246.30535840988\n",
      "Epoch: 1239500,Train Loss: 0.5500812530517578,Time: 20254.605842351913\n",
      "Epoch: 1240000,Train Loss: 0.5500812530517578,Time: 20262.67849969864\n",
      "Epoch: 1240500,Train Loss: 0.5500812530517578,Time: 20270.886697769165\n",
      "Epoch: 1241000,Train Loss: 0.5500812530517578,Time: 20278.722675561905\n",
      "Epoch: 1241500,Train Loss: 0.5500812530517578,Time: 20287.361453533173\n",
      "Epoch: 1242000,Train Loss: 0.5500812530517578,Time: 20295.97700238228\n",
      "Epoch: 1242500,Train Loss: 0.5500812530517578,Time: 20303.887328624725\n",
      "Epoch: 1243000,Train Loss: 0.550081193447113,Time: 20311.98031258583\n",
      "Epoch: 1243500,Train Loss: 0.550081193447113,Time: 20319.95295572281\n",
      "Epoch: 1244000,Train Loss: 0.550081193447113,Time: 20328.087844610214\n",
      "Epoch: 1244500,Train Loss: 0.550081193447113,Time: 20336.242138385773\n",
      "Epoch: 1245000,Train Loss: 0.550081193447113,Time: 20344.513734579086\n",
      "Epoch: 1245500,Train Loss: 0.550081193447113,Time: 20352.56450176239\n",
      "Epoch: 1246000,Train Loss: 0.550081193447113,Time: 20360.9085791111\n",
      "Epoch: 1246500,Train Loss: 0.550081193447113,Time: 20368.810854673386\n",
      "Epoch: 1247000,Train Loss: 0.550081193447113,Time: 20377.11969089508\n",
      "Epoch: 1247500,Train Loss: 0.5500811338424683,Time: 20385.268783807755\n",
      "Epoch: 1248000,Train Loss: 0.5500811338424683,Time: 20393.640724420547\n",
      "Epoch: 1248500,Train Loss: 0.5500811338424683,Time: 20401.736475467682\n",
      "Epoch: 1249000,Train Loss: 0.5500811338424683,Time: 20409.78383088112\n",
      "Epoch: 1249500,Train Loss: 0.5500811338424683,Time: 20418.049968242645\n",
      "Epoch: 1250000,Train Loss: 0.5500811338424683,Time: 20426.22037744522\n",
      "Epoch: 1250500,Train Loss: 0.5500811338424683,Time: 20434.19308590889\n",
      "Epoch: 1251000,Train Loss: 0.5500811338424683,Time: 20442.64558982849\n",
      "Epoch: 1251500,Train Loss: 0.5500811338424683,Time: 20451.112100362778\n",
      "Epoch: 1252000,Train Loss: 0.5500811338424683,Time: 20459.36189007759\n",
      "Epoch: 1252500,Train Loss: 0.5500811338424683,Time: 20467.439187288284\n",
      "Epoch: 1253000,Train Loss: 0.5500811338424683,Time: 20475.50475001335\n",
      "Epoch: 1253500,Train Loss: 0.5500811338424683,Time: 20483.49973845482\n",
      "Epoch: 1254000,Train Loss: 0.5500811338424683,Time: 20491.730137109756\n",
      "Epoch: 1254500,Train Loss: 0.5500811338424683,Time: 20499.934913635254\n",
      "Epoch: 1255000,Train Loss: 0.5500811338424683,Time: 20508.088597536087\n",
      "Epoch: 1255500,Train Loss: 0.5500811338424683,Time: 20516.09745645523\n",
      "Epoch: 1256000,Train Loss: 0.5500810742378235,Time: 20524.418429374695\n",
      "Epoch: 1256500,Train Loss: 0.5500811338424683,Time: 20532.4038271904\n",
      "Epoch: 1257000,Train Loss: 0.5500811338424683,Time: 20540.594358444214\n",
      "Epoch: 1257500,Train Loss: 0.5500811338424683,Time: 20548.851482868195\n",
      "Epoch: 1258000,Train Loss: 0.5500811338424683,Time: 20557.620901823044\n",
      "Epoch: 1258500,Train Loss: 0.5500810742378235,Time: 20565.777702093124\n",
      "Epoch: 1259000,Train Loss: 0.5500810742378235,Time: 20574.2217130661\n",
      "Epoch: 1259500,Train Loss: 0.5500810742378235,Time: 20582.70451068878\n",
      "Epoch: 1260000,Train Loss: 0.5500810742378235,Time: 20591.27404665947\n",
      "Epoch: 1260500,Train Loss: 0.5500810742378235,Time: 20599.413908958435\n",
      "Epoch: 1261000,Train Loss: 0.5500810146331787,Time: 20607.82945227623\n",
      "Epoch: 1261500,Train Loss: 0.5500810742378235,Time: 20615.784641981125\n",
      "Epoch: 1262000,Train Loss: 0.5500810742378235,Time: 20624.366045475006\n",
      "Epoch: 1262500,Train Loss: 0.5500810742378235,Time: 20632.361500501633\n",
      "Epoch: 1263000,Train Loss: 0.5500810146331787,Time: 20640.503144025803\n",
      "Epoch: 1263500,Train Loss: 0.5500810742378235,Time: 20648.589209079742\n",
      "Epoch: 1264000,Train Loss: 0.5500810742378235,Time: 20656.808683633804\n",
      "Epoch: 1264500,Train Loss: 0.5500810742378235,Time: 20664.818288564682\n",
      "Epoch: 1265000,Train Loss: 0.5500810146331787,Time: 20673.06117081642\n",
      "Epoch: 1265500,Train Loss: 0.5500810146331787,Time: 20681.067831993103\n",
      "Epoch: 1266000,Train Loss: 0.5500810146331787,Time: 20689.37686419487\n",
      "Epoch: 1266500,Train Loss: 0.5500810146331787,Time: 20697.44301056862\n",
      "Epoch: 1267000,Train Loss: 0.5500810146331787,Time: 20705.590282201767\n",
      "Epoch: 1267500,Train Loss: 0.5500810146331787,Time: 20713.606562137604\n",
      "Epoch: 1268000,Train Loss: 0.5500810146331787,Time: 20721.899689674377\n",
      "Epoch: 1268500,Train Loss: 0.5500810146331787,Time: 20730.29292654991\n",
      "Epoch: 1269000,Train Loss: 0.5500810146331787,Time: 20738.464824676514\n",
      "Epoch: 1269500,Train Loss: 0.5500810146331787,Time: 20746.39601111412\n",
      "Epoch: 1270000,Train Loss: 0.5500810146331787,Time: 20754.714597463608\n",
      "Epoch: 1270500,Train Loss: 0.5500810146331787,Time: 20762.82960176468\n",
      "Epoch: 1271000,Train Loss: 0.5500810146331787,Time: 20771.028148651123\n",
      "Epoch: 1271500,Train Loss: 0.5500810146331787,Time: 20779.11303949356\n",
      "Epoch: 1272000,Train Loss: 0.5500809550285339,Time: 20787.280765533447\n",
      "Epoch: 1272500,Train Loss: 0.5500809550285339,Time: 20795.18316435814\n",
      "Epoch: 1273000,Train Loss: 0.5500809550285339,Time: 20803.34272789955\n",
      "Epoch: 1273500,Train Loss: 0.5500809550285339,Time: 20811.385271549225\n",
      "Epoch: 1274000,Train Loss: 0.5500809550285339,Time: 20819.468994140625\n",
      "Epoch: 1274500,Train Loss: 0.5500809550285339,Time: 20827.482672214508\n",
      "Epoch: 1275000,Train Loss: 0.5500809550285339,Time: 20835.964431762695\n",
      "Epoch: 1275500,Train Loss: 0.5500809550285339,Time: 20844.248960971832\n",
      "Epoch: 1276000,Train Loss: 0.5500809550285339,Time: 20852.65578508377\n",
      "Epoch: 1276500,Train Loss: 0.5500809550285339,Time: 20860.599613428116\n",
      "Epoch: 1277000,Train Loss: 0.5500809550285339,Time: 20868.697590827942\n",
      "Epoch: 1277500,Train Loss: 0.5500809550285339,Time: 20876.750586748123\n",
      "Epoch: 1278000,Train Loss: 0.5500809550285339,Time: 20884.872979164124\n",
      "Epoch: 1278500,Train Loss: 0.5500809550285339,Time: 20893.096529960632\n",
      "Epoch: 1279000,Train Loss: 0.5500808954238892,Time: 20901.351836681366\n",
      "Epoch: 1279500,Train Loss: 0.5500808954238892,Time: 20909.279308080673\n",
      "Epoch: 1280000,Train Loss: 0.5500808954238892,Time: 20917.428770065308\n",
      "Epoch: 1280500,Train Loss: 0.5500808954238892,Time: 20925.38477373123\n",
      "Epoch: 1281000,Train Loss: 0.5500808954238892,Time: 20933.251683950424\n",
      "Epoch: 1281500,Train Loss: 0.5500808954238892,Time: 20941.33049750328\n",
      "Epoch: 1282000,Train Loss: 0.5500808954238892,Time: 20949.590322971344\n",
      "Epoch: 1282500,Train Loss: 0.5500808954238892,Time: 20958.284967660904\n",
      "Epoch: 1283000,Train Loss: 0.5500808954238892,Time: 20966.73798942566\n",
      "Epoch: 1283500,Train Loss: 0.5500808954238892,Time: 20975.00656557083\n",
      "Epoch: 1284000,Train Loss: 0.5500808358192444,Time: 20983.492726802826\n",
      "Epoch: 1284500,Train Loss: 0.5500808358192444,Time: 20991.49555516243\n",
      "Epoch: 1285000,Train Loss: 0.5500808358192444,Time: 20999.643662929535\n",
      "Epoch: 1285500,Train Loss: 0.5500808358192444,Time: 21007.631637334824\n",
      "Epoch: 1286000,Train Loss: 0.5500808358192444,Time: 21015.786573171616\n",
      "Epoch: 1286500,Train Loss: 0.5500808358192444,Time: 21023.744383573532\n",
      "Epoch: 1287000,Train Loss: 0.5500808358192444,Time: 21031.918167352676\n",
      "Epoch: 1287500,Train Loss: 0.5500808358192444,Time: 21039.920876026154\n",
      "Epoch: 1288000,Train Loss: 0.5500808358192444,Time: 21048.13438630104\n",
      "Epoch: 1288500,Train Loss: 0.5500808358192444,Time: 21056.04239845276\n",
      "Epoch: 1289000,Train Loss: 0.5500808358192444,Time: 21064.252923488617\n",
      "Epoch: 1289500,Train Loss: 0.5500808358192444,Time: 21072.38471508026\n",
      "Epoch: 1290000,Train Loss: 0.5500808358192444,Time: 21080.76881623268\n",
      "Epoch: 1290500,Train Loss: 0.5500808358192444,Time: 21089.007333040237\n",
      "Epoch: 1291000,Train Loss: 0.5500808358192444,Time: 21097.272135019302\n",
      "Epoch: 1291500,Train Loss: 0.5500808358192444,Time: 21105.23144674301\n",
      "Epoch: 1292000,Train Loss: 0.5500808358192444,Time: 21113.335363149643\n",
      "Epoch: 1292500,Train Loss: 0.5500808358192444,Time: 21121.23185491562\n",
      "Epoch: 1293000,Train Loss: 0.5500808358192444,Time: 21129.607305765152\n",
      "Epoch: 1293500,Train Loss: 0.5500808358192444,Time: 21137.696358203888\n",
      "Epoch: 1294000,Train Loss: 0.5500808358192444,Time: 21146.067848205566\n",
      "Epoch: 1294500,Train Loss: 0.5500808358192444,Time: 21154.228430986404\n",
      "Epoch: 1295000,Train Loss: 0.5500807762145996,Time: 21162.421302318573\n",
      "Epoch: 1295500,Train Loss: 0.5500807166099548,Time: 21170.897275209427\n",
      "Epoch: 1296000,Train Loss: 0.5500807166099548,Time: 21179.22457242012\n",
      "Epoch: 1296500,Train Loss: 0.5500807166099548,Time: 21187.215763807297\n",
      "Epoch: 1297000,Train Loss: 0.5500807166099548,Time: 21195.476999998093\n",
      "Epoch: 1297500,Train Loss: 0.5500807166099548,Time: 21203.53769993782\n",
      "Epoch: 1298000,Train Loss: 0.5500807166099548,Time: 21211.790189027786\n",
      "Epoch: 1298500,Train Loss: 0.5500807166099548,Time: 21219.63613319397\n",
      "Epoch: 1299000,Train Loss: 0.5500807166099548,Time: 21227.891617059708\n",
      "Epoch: 1299500,Train Loss: 0.5500807166099548,Time: 21235.929699659348\n",
      "Epoch: 1300000,Train Loss: 0.5500807166099548,Time: 21244.16711783409\n",
      "Epoch: 1300500,Train Loss: 0.5500807166099548,Time: 21252.335874080658\n",
      "Epoch: 1301000,Train Loss: 0.5500807166099548,Time: 21260.655880212784\n",
      "Epoch: 1301500,Train Loss: 0.5500807166099548,Time: 21268.84308719635\n",
      "Epoch: 1302000,Train Loss: 0.5500807166099548,Time: 21276.997371673584\n",
      "Epoch: 1302500,Train Loss: 0.5500807166099548,Time: 21285.189207792282\n",
      "Epoch: 1303000,Train Loss: 0.5500807166099548,Time: 21293.372300624847\n",
      "Epoch: 1303500,Train Loss: 0.5500807166099548,Time: 21301.29600763321\n",
      "Epoch: 1304000,Train Loss: 0.5500807166099548,Time: 21309.598115682602\n",
      "Epoch: 1304500,Train Loss: 0.5500807166099548,Time: 21317.481069803238\n",
      "Epoch: 1305000,Train Loss: 0.5500807166099548,Time: 21325.811813116074\n",
      "Epoch: 1305500,Train Loss: 0.5500807166099548,Time: 21333.750865459442\n",
      "Epoch: 1306000,Train Loss: 0.5500807166099548,Time: 21342.237817287445\n",
      "Epoch: 1306500,Train Loss: 0.5500807166099548,Time: 21350.707995414734\n",
      "Epoch: 1307000,Train Loss: 0.5500807166099548,Time: 21358.90375995636\n",
      "Epoch: 1307500,Train Loss: 0.5500807166099548,Time: 21366.829099178314\n",
      "Epoch: 1308000,Train Loss: 0.5500807166099548,Time: 21375.23114991188\n",
      "Epoch: 1308500,Train Loss: 0.5500807166099548,Time: 21383.443219184875\n",
      "Epoch: 1309000,Train Loss: 0.5500807166099548,Time: 21391.777999401093\n",
      "Epoch: 1309500,Train Loss: 0.5500805974006653,Time: 21399.964913129807\n",
      "Epoch: 1310000,Train Loss: 0.5500807166099548,Time: 21408.311215639114\n",
      "Epoch: 1310500,Train Loss: 0.5500807166099548,Time: 21416.22313261032\n",
      "Epoch: 1311000,Train Loss: 0.5500805974006653,Time: 21424.55501151085\n",
      "Epoch: 1311500,Train Loss: 0.5500806570053101,Time: 21432.62433886528\n",
      "Epoch: 1312000,Train Loss: 0.5500806570053101,Time: 21440.92094540596\n",
      "Epoch: 1312500,Train Loss: 0.5500805974006653,Time: 21448.91659641266\n",
      "Epoch: 1313000,Train Loss: 0.5500805974006653,Time: 21457.32800745964\n",
      "Epoch: 1313500,Train Loss: 0.5500805974006653,Time: 21465.89255452156\n",
      "Epoch: 1314000,Train Loss: 0.5500806570053101,Time: 21474.361676216125\n",
      "Epoch: 1314500,Train Loss: 0.5500806570053101,Time: 21483.03561067581\n",
      "Epoch: 1315000,Train Loss: 0.5500805974006653,Time: 21490.914957761765\n",
      "Epoch: 1315500,Train Loss: 0.5500805974006653,Time: 21499.20312690735\n",
      "Epoch: 1316000,Train Loss: 0.5500805974006653,Time: 21507.200323581696\n",
      "Epoch: 1316500,Train Loss: 0.5500805974006653,Time: 21515.665811538696\n",
      "Epoch: 1317000,Train Loss: 0.5500805974006653,Time: 21523.808754205704\n",
      "Epoch: 1317500,Train Loss: 0.5500806570053101,Time: 21532.08003592491\n",
      "Epoch: 1318000,Train Loss: 0.5500805974006653,Time: 21540.19549036026\n",
      "Epoch: 1318500,Train Loss: 0.5500806570053101,Time: 21548.521135807037\n",
      "Epoch: 1319000,Train Loss: 0.5500806570053101,Time: 21556.458243846893\n",
      "Epoch: 1319500,Train Loss: 0.5500805974006653,Time: 21564.7723801136\n",
      "Epoch: 1320000,Train Loss: 0.5500805974006653,Time: 21572.808506965637\n",
      "Epoch: 1320500,Train Loss: 0.5500805974006653,Time: 21581.12367582321\n",
      "Epoch: 1321000,Train Loss: 0.5500805974006653,Time: 21589.138506412506\n",
      "Epoch: 1321500,Train Loss: 0.5500805974006653,Time: 21597.565741300583\n",
      "Epoch: 1322000,Train Loss: 0.5500805974006653,Time: 21605.7372879982\n",
      "Epoch: 1322500,Train Loss: 0.5500805974006653,Time: 21614.12480854988\n",
      "Epoch: 1323000,Train Loss: 0.5500805974006653,Time: 21622.09576678276\n",
      "Epoch: 1323500,Train Loss: 0.5500805974006653,Time: 21630.49835395813\n",
      "Epoch: 1324000,Train Loss: 0.5500805974006653,Time: 21638.481041908264\n",
      "Epoch: 1324500,Train Loss: 0.5500805974006653,Time: 21646.704770326614\n",
      "Epoch: 1325000,Train Loss: 0.5500805974006653,Time: 21655.47384905815\n",
      "Epoch: 1325500,Train Loss: 0.5500805974006653,Time: 21663.88392329216\n",
      "Epoch: 1326000,Train Loss: 0.5500805974006653,Time: 21671.89791917801\n",
      "Epoch: 1326500,Train Loss: 0.5500805974006653,Time: 21680.10416507721\n",
      "Epoch: 1327000,Train Loss: 0.5500805974006653,Time: 21688.353066921234\n",
      "Epoch: 1327500,Train Loss: 0.5500805974006653,Time: 21696.836050748825\n",
      "Epoch: 1328000,Train Loss: 0.5500805974006653,Time: 21705.1989634037\n",
      "Epoch: 1328500,Train Loss: 0.5500805974006653,Time: 21713.304886817932\n",
      "Epoch: 1329000,Train Loss: 0.5500805974006653,Time: 21721.21135377884\n",
      "Epoch: 1329500,Train Loss: 0.5500805974006653,Time: 21729.41527199745\n",
      "Epoch: 1330000,Train Loss: 0.5500805974006653,Time: 21737.57325720787\n",
      "Epoch: 1330500,Train Loss: 0.5500805974006653,Time: 21746.15935921669\n",
      "Epoch: 1331000,Train Loss: 0.5500805974006653,Time: 21754.422609090805\n",
      "Epoch: 1331500,Train Loss: 0.5500805974006653,Time: 21763.033190965652\n",
      "Epoch: 1332000,Train Loss: 0.5500805974006653,Time: 21771.213657855988\n",
      "Epoch: 1332500,Train Loss: 0.5500805974006653,Time: 21779.411107063293\n",
      "Epoch: 1333000,Train Loss: 0.5500805974006653,Time: 21787.478660821915\n",
      "Epoch: 1333500,Train Loss: 0.5500805974006653,Time: 21795.627329826355\n",
      "Epoch: 1334000,Train Loss: 0.5500805974006653,Time: 21803.55570936203\n",
      "Epoch: 1334500,Train Loss: 0.5500804781913757,Time: 21811.682945013046\n",
      "Epoch: 1335000,Train Loss: 0.5500804781913757,Time: 21819.578502893448\n",
      "Epoch: 1335500,Train Loss: 0.5500804781913757,Time: 21827.68111371994\n",
      "Epoch: 1336000,Train Loss: 0.5500804781913757,Time: 21835.728479623795\n",
      "Epoch: 1336500,Train Loss: 0.5500804781913757,Time: 21843.99555540085\n",
      "Epoch: 1337000,Train Loss: 0.5500804781913757,Time: 21852.1578810215\n",
      "Epoch: 1337500,Train Loss: 0.5500804781913757,Time: 21860.403079271317\n",
      "Epoch: 1338000,Train Loss: 0.5500804781913757,Time: 21868.706238031387\n",
      "Epoch: 1338500,Train Loss: 0.5500804781913757,Time: 21876.977147579193\n",
      "Epoch: 1339000,Train Loss: 0.5500804781913757,Time: 21885.01864886284\n",
      "Epoch: 1339500,Train Loss: 0.5500804781913757,Time: 21893.53321290016\n",
      "Epoch: 1340000,Train Loss: 0.5500804781913757,Time: 21901.55788064003\n",
      "Epoch: 1340500,Train Loss: 0.5500804781913757,Time: 21909.79634666443\n",
      "Epoch: 1341000,Train Loss: 0.5500804781913757,Time: 21917.866273641586\n",
      "Epoch: 1341500,Train Loss: 0.5500804781913757,Time: 21926.05457997322\n",
      "Epoch: 1342000,Train Loss: 0.5500804781913757,Time: 21934.047936677933\n",
      "Epoch: 1342500,Train Loss: 0.5500804781913757,Time: 21942.304794073105\n",
      "Epoch: 1343000,Train Loss: 0.5500804781913757,Time: 21950.418365716934\n",
      "Epoch: 1343500,Train Loss: 0.5500804781913757,Time: 21958.87309551239\n",
      "Epoch: 1344000,Train Loss: 0.5500804781913757,Time: 21966.84767794609\n",
      "Epoch: 1344500,Train Loss: 0.5500804781913757,Time: 21975.019728183746\n",
      "Epoch: 1345000,Train Loss: 0.5500804781913757,Time: 21982.98776292801\n",
      "Epoch: 1345500,Train Loss: 0.5500804781913757,Time: 21991.282917499542\n",
      "Epoch: 1346000,Train Loss: 0.5500804781913757,Time: 21999.34559392929\n",
      "Epoch: 1346500,Train Loss: 0.5500804781913757,Time: 22007.512742757797\n",
      "Epoch: 1347000,Train Loss: 0.5500804781913757,Time: 22015.3936316967\n",
      "Epoch: 1347500,Train Loss: 0.5500804781913757,Time: 22023.435654640198\n",
      "Epoch: 1348000,Train Loss: 0.5500804781913757,Time: 22031.340220928192\n",
      "Epoch: 1348500,Train Loss: 0.5500803589820862,Time: 22039.521666288376\n",
      "Epoch: 1349000,Train Loss: 0.550080418586731,Time: 22047.722002267838\n",
      "Epoch: 1349500,Train Loss: 0.550080418586731,Time: 22056.19273877144\n",
      "Epoch: 1350000,Train Loss: 0.5500804781913757,Time: 22064.30650115013\n",
      "Epoch: 1350500,Train Loss: 0.5500804781913757,Time: 22072.60809278488\n",
      "Epoch: 1351000,Train Loss: 0.550080418586731,Time: 22080.643963098526\n",
      "Epoch: 1351500,Train Loss: 0.5500803589820862,Time: 22088.83688020706\n",
      "Epoch: 1352000,Train Loss: 0.5500803589820862,Time: 22096.861308813095\n",
      "Epoch: 1352500,Train Loss: 0.5500803589820862,Time: 22104.97460103035\n",
      "Epoch: 1353000,Train Loss: 0.5500803589820862,Time: 22113.01056909561\n",
      "Epoch: 1353500,Train Loss: 0.5500803589820862,Time: 22121.155136823654\n",
      "Epoch: 1354000,Train Loss: 0.5500804781913757,Time: 22129.38032388687\n",
      "Epoch: 1354500,Train Loss: 0.5500804781913757,Time: 22137.470026493073\n",
      "Epoch: 1355000,Train Loss: 0.5500803589820862,Time: 22145.43689250946\n",
      "Epoch: 1355500,Train Loss: 0.5500804781913757,Time: 22153.49378490448\n",
      "Epoch: 1356000,Train Loss: 0.5500804781913757,Time: 22161.563053369522\n",
      "Epoch: 1356500,Train Loss: 0.5500803589820862,Time: 22169.75767350197\n",
      "Epoch: 1357000,Train Loss: 0.5500803589820862,Time: 22177.73108458519\n",
      "Epoch: 1357500,Train Loss: 0.5500803589820862,Time: 22186.079208374023\n",
      "Epoch: 1358000,Train Loss: 0.5500803589820862,Time: 22194.453000068665\n",
      "Epoch: 1358500,Train Loss: 0.5500803589820862,Time: 22202.586162805557\n",
      "Epoch: 1359000,Train Loss: 0.5500803589820862,Time: 22210.54999399185\n",
      "Epoch: 1359500,Train Loss: 0.5500803589820862,Time: 22218.738361120224\n",
      "Epoch: 1360000,Train Loss: 0.5500803589820862,Time: 22226.668568611145\n",
      "Epoch: 1360500,Train Loss: 0.5500803589820862,Time: 22234.73137331009\n",
      "Epoch: 1361000,Train Loss: 0.5500803589820862,Time: 22242.61620283127\n",
      "Epoch: 1361500,Train Loss: 0.5500803589820862,Time: 22250.755519866943\n",
      "Epoch: 1362000,Train Loss: 0.5500803589820862,Time: 22258.64877653122\n",
      "Epoch: 1362500,Train Loss: 0.5500803589820862,Time: 22266.726422309875\n",
      "Epoch: 1363000,Train Loss: 0.5500803589820862,Time: 22274.622060775757\n",
      "Epoch: 1363500,Train Loss: 0.5500803589820862,Time: 22282.929527044296\n",
      "Epoch: 1364000,Train Loss: 0.5500803589820862,Time: 22290.98848104477\n",
      "Epoch: 1364500,Train Loss: 0.5500803589820862,Time: 22299.334706544876\n",
      "Epoch: 1365000,Train Loss: 0.5500803589820862,Time: 22307.68438720703\n",
      "Epoch: 1365500,Train Loss: 0.5500803589820862,Time: 22316.159492731094\n",
      "Epoch: 1366000,Train Loss: 0.5500803589820862,Time: 22324.42764878273\n",
      "Epoch: 1366500,Train Loss: 0.5500803589820862,Time: 22332.60351538658\n",
      "Epoch: 1367000,Train Loss: 0.5500803589820862,Time: 22340.461161136627\n",
      "Epoch: 1367500,Train Loss: 0.5500802993774414,Time: 22348.56324672699\n",
      "Epoch: 1368000,Train Loss: 0.5500802993774414,Time: 22356.39560651779\n",
      "Epoch: 1368500,Train Loss: 0.5500803589820862,Time: 22364.527845144272\n",
      "Epoch: 1369000,Train Loss: 0.5500802993774414,Time: 22372.719601154327\n",
      "Epoch: 1369500,Train Loss: 0.5500802397727966,Time: 22381.04840707779\n",
      "Epoch: 1370000,Train Loss: 0.5500803589820862,Time: 22389.206785917282\n",
      "Epoch: 1370500,Train Loss: 0.5500803589820862,Time: 22397.58123779297\n",
      "Epoch: 1371000,Train Loss: 0.5500802993774414,Time: 22405.99750638008\n",
      "Epoch: 1371500,Train Loss: 0.5500802993774414,Time: 22414.34913277626\n",
      "Epoch: 1372000,Train Loss: 0.5500802397727966,Time: 22422.543488025665\n",
      "Epoch: 1372500,Train Loss: 0.5500802397727966,Time: 22430.851397514343\n",
      "Epoch: 1373000,Train Loss: 0.5500802397727966,Time: 22438.90270280838\n",
      "Epoch: 1373500,Train Loss: 0.5500802397727966,Time: 22447.338211774826\n",
      "Epoch: 1374000,Train Loss: 0.5500802993774414,Time: 22455.15984773636\n",
      "Epoch: 1374500,Train Loss: 0.5500802993774414,Time: 22463.2457716465\n",
      "Epoch: 1375000,Train Loss: 0.5500802397727966,Time: 22471.508988142014\n",
      "Epoch: 1375500,Train Loss: 0.5500802397727966,Time: 22479.808434724808\n",
      "Epoch: 1376000,Train Loss: 0.5500802397727966,Time: 22487.883503437042\n",
      "Epoch: 1376500,Train Loss: 0.5500802397727966,Time: 22496.73856139183\n",
      "Epoch: 1377000,Train Loss: 0.5500802397727966,Time: 22505.077876091003\n",
      "Epoch: 1377500,Train Loss: 0.5500802397727966,Time: 22513.449773311615\n",
      "Epoch: 1378000,Train Loss: 0.5500802397727966,Time: 22521.487486600876\n",
      "Epoch: 1378500,Train Loss: 0.5500802397727966,Time: 22530.26761484146\n",
      "Epoch: 1379000,Train Loss: 0.5500802397727966,Time: 22538.48903632164\n",
      "Epoch: 1379500,Train Loss: 0.5500802397727966,Time: 22546.847344636917\n",
      "Epoch: 1380000,Train Loss: 0.5500802397727966,Time: 22555.002805948257\n",
      "Epoch: 1380500,Train Loss: 0.5500802397727966,Time: 22563.28208899498\n",
      "Epoch: 1381000,Train Loss: 0.5500802397727966,Time: 22571.521347284317\n",
      "Epoch: 1381500,Train Loss: 0.5500802397727966,Time: 22579.800411462784\n",
      "Epoch: 1382000,Train Loss: 0.5500802397727966,Time: 22587.762758016586\n",
      "Epoch: 1382500,Train Loss: 0.5500802397727966,Time: 22595.98674273491\n",
      "Epoch: 1383000,Train Loss: 0.5500802397727966,Time: 22603.917882442474\n",
      "Epoch: 1383500,Train Loss: 0.5500802397727966,Time: 22612.12476158142\n",
      "Epoch: 1384000,Train Loss: 0.5500801801681519,Time: 22620.09675335884\n",
      "Epoch: 1384500,Train Loss: 0.5500802397727966,Time: 22628.304203271866\n",
      "Epoch: 1385000,Train Loss: 0.5500802397727966,Time: 22636.240130901337\n",
      "Epoch: 1385500,Train Loss: 0.5500802397727966,Time: 22644.266530513763\n",
      "Epoch: 1386000,Train Loss: 0.5500802397727966,Time: 22652.64039826393\n",
      "Epoch: 1386500,Train Loss: 0.5500802397727966,Time: 22660.67528152466\n",
      "Epoch: 1387000,Train Loss: 0.5500801801681519,Time: 22669.41721343994\n",
      "Epoch: 1387500,Train Loss: 0.5500801801681519,Time: 22677.700147390366\n",
      "Epoch: 1388000,Train Loss: 0.5500801205635071,Time: 22686.02241706848\n",
      "Epoch: 1388500,Train Loss: 0.5500801801681519,Time: 22694.090502500534\n",
      "Epoch: 1389000,Train Loss: 0.5500801801681519,Time: 22702.472720384598\n",
      "Epoch: 1389500,Train Loss: 0.5500801801681519,Time: 22710.36496090889\n",
      "Epoch: 1390000,Train Loss: 0.5500801801681519,Time: 22718.83420777321\n",
      "Epoch: 1390500,Train Loss: 0.5500801801681519,Time: 22726.96164369583\n",
      "Epoch: 1391000,Train Loss: 0.5500801801681519,Time: 22735.465102672577\n",
      "Epoch: 1391500,Train Loss: 0.5500801801681519,Time: 22743.960815668106\n",
      "Epoch: 1392000,Train Loss: 0.5500801801681519,Time: 22752.253660678864\n",
      "Epoch: 1392500,Train Loss: 0.5500801205635071,Time: 22760.352828264236\n",
      "Epoch: 1393000,Train Loss: 0.5500801205635071,Time: 22768.812336444855\n",
      "Epoch: 1393500,Train Loss: 0.5500801205635071,Time: 22776.85356616974\n",
      "Epoch: 1394000,Train Loss: 0.5500801205635071,Time: 22784.97855067253\n",
      "Epoch: 1394500,Train Loss: 0.5500801801681519,Time: 22793.09458231926\n",
      "Epoch: 1395000,Train Loss: 0.5500801801681519,Time: 22801.711215019226\n",
      "Epoch: 1395500,Train Loss: 0.5500801205635071,Time: 22809.98424768448\n",
      "Epoch: 1396000,Train Loss: 0.5500801205635071,Time: 22818.22852706909\n",
      "Epoch: 1396500,Train Loss: 0.5500801801681519,Time: 22826.745718240738\n",
      "Epoch: 1397000,Train Loss: 0.5500801205635071,Time: 22834.84081220627\n",
      "Epoch: 1397500,Train Loss: 0.5500801205635071,Time: 22843.49938249588\n",
      "Epoch: 1398000,Train Loss: 0.5500801205635071,Time: 22852.157009363174\n",
      "Epoch: 1398500,Train Loss: 0.5500801205635071,Time: 22860.445297718048\n",
      "Epoch: 1399000,Train Loss: 0.5500801205635071,Time: 22868.65561556816\n",
      "Epoch: 1399500,Train Loss: 0.5500801205635071,Time: 22876.585513353348\n",
      "Epoch: 1400000,Train Loss: 0.5500801205635071,Time: 22884.863085269928\n",
      "Epoch: 1400500,Train Loss: 0.5500801205635071,Time: 22893.088047027588\n",
      "Epoch: 1401000,Train Loss: 0.5500801205635071,Time: 22901.54115462303\n",
      "Epoch: 1401500,Train Loss: 0.5500801205635071,Time: 22909.592591762543\n",
      "Epoch: 1402000,Train Loss: 0.5500800609588623,Time: 22917.749856948853\n",
      "Epoch: 1402500,Train Loss: 0.5500801205635071,Time: 22925.66924595833\n",
      "Epoch: 1403000,Train Loss: 0.5500800609588623,Time: 22933.99692964554\n",
      "Epoch: 1403500,Train Loss: 0.5500801205635071,Time: 22942.099863052368\n",
      "Epoch: 1404000,Train Loss: 0.5500800609588623,Time: 22950.550790786743\n",
      "Epoch: 1404500,Train Loss: 0.5500800609588623,Time: 22958.489372968674\n",
      "Epoch: 1405000,Train Loss: 0.5500800609588623,Time: 22967.04650259018\n",
      "Epoch: 1405500,Train Loss: 0.5500800609588623,Time: 22974.981883764267\n",
      "Epoch: 1406000,Train Loss: 0.5500800609588623,Time: 22983.41999912262\n",
      "Epoch: 1406500,Train Loss: 0.5500800609588623,Time: 22991.529175519943\n",
      "Epoch: 1407000,Train Loss: 0.5500800609588623,Time: 22999.955911636353\n",
      "Epoch: 1407500,Train Loss: 0.5500800609588623,Time: 23007.95623230934\n",
      "Epoch: 1408000,Train Loss: 0.5500800609588623,Time: 23016.10905766487\n",
      "Epoch: 1408500,Train Loss: 0.5500800609588623,Time: 23023.94161438942\n",
      "Epoch: 1409000,Train Loss: 0.5500800609588623,Time: 23032.245539188385\n",
      "Epoch: 1409500,Train Loss: 0.5500800609588623,Time: 23040.55126142502\n",
      "Epoch: 1410000,Train Loss: 0.5500800013542175,Time: 23048.836918115616\n",
      "Epoch: 1410500,Train Loss: 0.5500800609588623,Time: 23056.99343395233\n",
      "Epoch: 1411000,Train Loss: 0.5500800013542175,Time: 23065.140268564224\n",
      "Epoch: 1411500,Train Loss: 0.5500800609588623,Time: 23073.090832710266\n",
      "Epoch: 1412000,Train Loss: 0.5500800013542175,Time: 23081.360810518265\n",
      "Epoch: 1412500,Train Loss: 0.5500800013542175,Time: 23089.218873739243\n",
      "Epoch: 1413000,Train Loss: 0.5500800013542175,Time: 23097.259439229965\n",
      "Epoch: 1413500,Train Loss: 0.5500800013542175,Time: 23105.29157590866\n",
      "Epoch: 1414000,Train Loss: 0.5500800013542175,Time: 23113.394003629684\n",
      "Epoch: 1414500,Train Loss: 0.5500800013542175,Time: 23121.46229815483\n",
      "Epoch: 1415000,Train Loss: 0.5500799417495728,Time: 23130.057020187378\n",
      "Epoch: 1415500,Train Loss: 0.5500800609588623,Time: 23138.13808774948\n",
      "Epoch: 1416000,Train Loss: 0.5500800013542175,Time: 23146.536287546158\n",
      "Epoch: 1416500,Train Loss: 0.5500800013542175,Time: 23154.7204144001\n",
      "Epoch: 1417000,Train Loss: 0.5500799417495728,Time: 23163.557111024857\n",
      "Epoch: 1417500,Train Loss: 0.5500800013542175,Time: 23171.433341503143\n",
      "Epoch: 1418000,Train Loss: 0.5500799417495728,Time: 23179.652006864548\n",
      "Epoch: 1418500,Train Loss: 0.5500799417495728,Time: 23188.269574403763\n",
      "Epoch: 1419000,Train Loss: 0.5500799417495728,Time: 23196.74716901779\n",
      "Epoch: 1419500,Train Loss: 0.5500800013542175,Time: 23204.96708059311\n",
      "Epoch: 1420000,Train Loss: 0.5500799417495728,Time: 23213.152160167694\n",
      "Epoch: 1420500,Train Loss: 0.5500800013542175,Time: 23221.172960281372\n",
      "Epoch: 1421000,Train Loss: 0.5500799417495728,Time: 23229.716668844223\n",
      "Epoch: 1421500,Train Loss: 0.5500799417495728,Time: 23237.73185157776\n",
      "Epoch: 1422000,Train Loss: 0.5500799417495728,Time: 23246.4114882946\n",
      "Epoch: 1422500,Train Loss: 0.5500800013542175,Time: 23254.516131401062\n",
      "Epoch: 1423000,Train Loss: 0.5500799417495728,Time: 23262.748391628265\n",
      "Epoch: 1423500,Train Loss: 0.5500799417495728,Time: 23270.777288913727\n",
      "Epoch: 1424000,Train Loss: 0.5500799417495728,Time: 23279.019783735275\n",
      "Epoch: 1424500,Train Loss: 0.5500799417495728,Time: 23287.2997238636\n",
      "Epoch: 1425000,Train Loss: 0.5500799417495728,Time: 23295.494572877884\n",
      "Epoch: 1425500,Train Loss: 0.5500799417495728,Time: 23303.484026670456\n",
      "Epoch: 1426000,Train Loss: 0.5500799417495728,Time: 23311.876423597336\n",
      "Epoch: 1426500,Train Loss: 0.5500799417495728,Time: 23319.672807455063\n",
      "Epoch: 1427000,Train Loss: 0.5500799417495728,Time: 23327.742804288864\n",
      "Epoch: 1427500,Train Loss: 0.5500799417495728,Time: 23335.841596364975\n",
      "Epoch: 1428000,Train Loss: 0.5500799417495728,Time: 23344.002987861633\n",
      "Epoch: 1428500,Train Loss: 0.5500799417495728,Time: 23352.37033009529\n",
      "Epoch: 1429000,Train Loss: 0.5500799417495728,Time: 23360.600292921066\n",
      "Epoch: 1429500,Train Loss: 0.5500799417495728,Time: 23368.932445526123\n",
      "Epoch: 1430000,Train Loss: 0.550079882144928,Time: 23377.268340826035\n",
      "Epoch: 1430500,Train Loss: 0.5500799417495728,Time: 23385.41277861595\n",
      "Epoch: 1431000,Train Loss: 0.550079882144928,Time: 23393.719601392746\n",
      "Epoch: 1431500,Train Loss: 0.5500799417495728,Time: 23401.815621852875\n",
      "Epoch: 1432000,Train Loss: 0.5500799417495728,Time: 23409.980131864548\n",
      "Epoch: 1432500,Train Loss: 0.5500799417495728,Time: 23417.96226286888\n",
      "Epoch: 1433000,Train Loss: 0.550079882144928,Time: 23426.063126802444\n",
      "Epoch: 1433500,Train Loss: 0.5500799417495728,Time: 23434.133702754974\n",
      "Epoch: 1434000,Train Loss: 0.550079882144928,Time: 23442.169141292572\n",
      "Epoch: 1434500,Train Loss: 0.550079882144928,Time: 23450.038677692413\n",
      "Epoch: 1435000,Train Loss: 0.550079882144928,Time: 23458.222299575806\n",
      "Epoch: 1435500,Train Loss: 0.550079882144928,Time: 23466.499507665634\n",
      "Epoch: 1436000,Train Loss: 0.550079882144928,Time: 23475.02983236313\n",
      "Epoch: 1436500,Train Loss: 0.550079882144928,Time: 23483.187626600266\n",
      "Epoch: 1437000,Train Loss: 0.550079882144928,Time: 23491.522288560867\n",
      "Epoch: 1437500,Train Loss: 0.550079882144928,Time: 23499.98273062706\n",
      "Epoch: 1438000,Train Loss: 0.550079882144928,Time: 23508.507113218307\n",
      "Epoch: 1438500,Train Loss: 0.550079882144928,Time: 23516.613821983337\n",
      "Epoch: 1439000,Train Loss: 0.550079882144928,Time: 23524.917350530624\n",
      "Epoch: 1439500,Train Loss: 0.550079882144928,Time: 23533.09688782692\n",
      "Epoch: 1440000,Train Loss: 0.550079882144928,Time: 23541.84734749794\n",
      "Epoch: 1440500,Train Loss: 0.550079882144928,Time: 23550.31613636017\n",
      "Epoch: 1441000,Train Loss: 0.550079882144928,Time: 23558.59568786621\n",
      "Epoch: 1441500,Train Loss: 0.550079882144928,Time: 23566.82228732109\n",
      "Epoch: 1442000,Train Loss: 0.5500798225402832,Time: 23574.97989487648\n",
      "Epoch: 1442500,Train Loss: 0.550079882144928,Time: 23583.136491775513\n",
      "Epoch: 1443000,Train Loss: 0.550079882144928,Time: 23591.178027153015\n",
      "Epoch: 1443500,Train Loss: 0.550079882144928,Time: 23599.269229650497\n",
      "Epoch: 1444000,Train Loss: 0.550079882144928,Time: 23607.82016301155\n",
      "Epoch: 1444500,Train Loss: 0.550079882144928,Time: 23615.99204325676\n",
      "Epoch: 1445000,Train Loss: 0.550079882144928,Time: 23624.118220567703\n",
      "Epoch: 1445500,Train Loss: 0.550079882144928,Time: 23632.038150787354\n",
      "Epoch: 1446000,Train Loss: 0.5500797629356384,Time: 23640.5185611248\n",
      "Epoch: 1446500,Train Loss: 0.5500798225402832,Time: 23648.645226955414\n",
      "Epoch: 1447000,Train Loss: 0.5500798225402832,Time: 23656.95961689949\n",
      "Epoch: 1447500,Train Loss: 0.5500797629356384,Time: 23664.80002808571\n",
      "Epoch: 1448000,Train Loss: 0.5500797629356384,Time: 23672.95424580574\n",
      "Epoch: 1448500,Train Loss: 0.5500797629356384,Time: 23680.93946480751\n",
      "Epoch: 1449000,Train Loss: 0.5500797629356384,Time: 23689.047537088394\n",
      "Epoch: 1449500,Train Loss: 0.5500798225402832,Time: 23697.200712442398\n",
      "Epoch: 1450000,Train Loss: 0.5500798225402832,Time: 23705.56105184555\n",
      "Epoch: 1450500,Train Loss: 0.5500798225402832,Time: 23713.64304804802\n",
      "Epoch: 1451000,Train Loss: 0.5500798225402832,Time: 23722.139415740967\n",
      "Epoch: 1451500,Train Loss: 0.5500797629356384,Time: 23730.22232556343\n",
      "Epoch: 1452000,Train Loss: 0.5500797629356384,Time: 23738.447332143784\n",
      "Epoch: 1452500,Train Loss: 0.5500797629356384,Time: 23746.503699064255\n",
      "Epoch: 1453000,Train Loss: 0.5500797629356384,Time: 23754.721850156784\n",
      "Epoch: 1453500,Train Loss: 0.5500797629356384,Time: 23762.743426561356\n",
      "Epoch: 1454000,Train Loss: 0.5500797629356384,Time: 23770.94787788391\n",
      "Epoch: 1454500,Train Loss: 0.5500797629356384,Time: 23779.319898366928\n",
      "Epoch: 1455000,Train Loss: 0.5500797629356384,Time: 23787.615121364594\n",
      "Epoch: 1455500,Train Loss: 0.5500797629356384,Time: 23795.624168872833\n",
      "Epoch: 1456000,Train Loss: 0.5500797629356384,Time: 23803.79219508171\n",
      "Epoch: 1456500,Train Loss: 0.5500797629356384,Time: 23811.746431827545\n",
      "Epoch: 1457000,Train Loss: 0.5500797629356384,Time: 23819.940858840942\n",
      "Epoch: 1457500,Train Loss: 0.5500797629356384,Time: 23828.186537742615\n",
      "Epoch: 1458000,Train Loss: 0.5500797629356384,Time: 23836.10498547554\n",
      "Epoch: 1458500,Train Loss: 0.5500797629356384,Time: 23844.449599266052\n",
      "Epoch: 1459000,Train Loss: 0.5500797629356384,Time: 23852.45691871643\n",
      "Epoch: 1459500,Train Loss: 0.5500797629356384,Time: 23860.932681322098\n",
      "Epoch: 1460000,Train Loss: 0.5500797629356384,Time: 23869.09424805641\n",
      "Epoch: 1460500,Train Loss: 0.5500797629356384,Time: 23877.375783920288\n",
      "Epoch: 1461000,Train Loss: 0.5500797629356384,Time: 23885.564914941788\n",
      "Epoch: 1461500,Train Loss: 0.5500797629356384,Time: 23893.810976982117\n",
      "Epoch: 1462000,Train Loss: 0.5500797629356384,Time: 23901.982511758804\n",
      "Epoch: 1462500,Train Loss: 0.5500797629356384,Time: 23910.11601114273\n",
      "Epoch: 1463000,Train Loss: 0.5500797629356384,Time: 23918.065129995346\n",
      "Epoch: 1463500,Train Loss: 0.5500797629356384,Time: 23926.59686112404\n",
      "Epoch: 1464000,Train Loss: 0.5500797629356384,Time: 23934.959774017334\n",
      "Epoch: 1464500,Train Loss: 0.5500797629356384,Time: 23943.173015356064\n",
      "Epoch: 1465000,Train Loss: 0.5500797629356384,Time: 23951.53530573845\n",
      "Epoch: 1465500,Train Loss: 0.5500797629356384,Time: 23959.99876999855\n",
      "Epoch: 1466000,Train Loss: 0.5500797629356384,Time: 23968.199402570724\n",
      "Epoch: 1466500,Train Loss: 0.5500797629356384,Time: 23976.3191986084\n",
      "Epoch: 1467000,Train Loss: 0.5500797033309937,Time: 23984.37358188629\n",
      "Epoch: 1467500,Train Loss: 0.5500797629356384,Time: 23992.48790693283\n",
      "Epoch: 1468000,Train Loss: 0.5500797629356384,Time: 24000.503769397736\n",
      "Epoch: 1468500,Train Loss: 0.5500797629356384,Time: 24008.643818616867\n",
      "Epoch: 1469000,Train Loss: 0.5500797629356384,Time: 24016.455057621002\n",
      "Epoch: 1469500,Train Loss: 0.5500797629356384,Time: 24024.667962551117\n",
      "Epoch: 1470000,Train Loss: 0.5500796437263489,Time: 24032.76951265335\n",
      "Epoch: 1470500,Train Loss: 0.5500797629356384,Time: 24040.946611881256\n",
      "Epoch: 1471000,Train Loss: 0.5500796437263489,Time: 24049.071817159653\n",
      "Epoch: 1471500,Train Loss: 0.5500796437263489,Time: 24057.238243818283\n",
      "Epoch: 1472000,Train Loss: 0.5500796437263489,Time: 24065.260636806488\n",
      "Epoch: 1472500,Train Loss: 0.5500797629356384,Time: 24073.514721155167\n",
      "Epoch: 1473000,Train Loss: 0.5500796437263489,Time: 24081.74183034897\n",
      "Epoch: 1473500,Train Loss: 0.5500797629356384,Time: 24090.096090316772\n",
      "Epoch: 1474000,Train Loss: 0.5500796437263489,Time: 24098.14979505539\n",
      "Epoch: 1474500,Train Loss: 0.5500796437263489,Time: 24106.391153097153\n",
      "Epoch: 1475000,Train Loss: 0.5500796437263489,Time: 24114.379539966583\n",
      "Epoch: 1475500,Train Loss: 0.5500796437263489,Time: 24122.78210401535\n",
      "Epoch: 1476000,Train Loss: 0.5500796437263489,Time: 24131.017343521118\n",
      "Epoch: 1476500,Train Loss: 0.5500796437263489,Time: 24139.72001338005\n",
      "Epoch: 1477000,Train Loss: 0.5500796437263489,Time: 24147.74338579178\n",
      "Epoch: 1477500,Train Loss: 0.5500796437263489,Time: 24156.150555849075\n",
      "Epoch: 1478000,Train Loss: 0.5500796437263489,Time: 24163.994713306427\n",
      "Epoch: 1478500,Train Loss: 0.5500796437263489,Time: 24172.31477689743\n",
      "Epoch: 1479000,Train Loss: 0.5500796437263489,Time: 24180.57243013382\n",
      "Epoch: 1479500,Train Loss: 0.5500796437263489,Time: 24188.96277809143\n",
      "Epoch: 1480000,Train Loss: 0.5500796437263489,Time: 24197.377183914185\n",
      "Epoch: 1480500,Train Loss: 0.5500796437263489,Time: 24205.853760242462\n",
      "Epoch: 1481000,Train Loss: 0.5500796437263489,Time: 24214.020564079285\n",
      "Epoch: 1481500,Train Loss: 0.5500796437263489,Time: 24222.22592663765\n",
      "Epoch: 1482000,Train Loss: 0.5500796437263489,Time: 24230.396048784256\n",
      "Epoch: 1482500,Train Loss: 0.5500796437263489,Time: 24238.774120807648\n",
      "Epoch: 1483000,Train Loss: 0.5500796437263489,Time: 24246.72362923622\n",
      "Epoch: 1483500,Train Loss: 0.5500796437263489,Time: 24255.11419057846\n",
      "Epoch: 1484000,Train Loss: 0.5500796437263489,Time: 24263.237959623337\n",
      "Epoch: 1484500,Train Loss: 0.5500796437263489,Time: 24271.455790519714\n",
      "Epoch: 1485000,Train Loss: 0.5500796437263489,Time: 24279.569213867188\n",
      "Epoch: 1485500,Train Loss: 0.5500796437263489,Time: 24287.76475930214\n",
      "Epoch: 1486000,Train Loss: 0.5500796437263489,Time: 24295.900617599487\n",
      "Epoch: 1486500,Train Loss: 0.5500796437263489,Time: 24304.131724834442\n",
      "Epoch: 1487000,Train Loss: 0.5500796437263489,Time: 24312.305852890015\n",
      "Epoch: 1487500,Train Loss: 0.5500796437263489,Time: 24320.490052700043\n",
      "Epoch: 1488000,Train Loss: 0.5500796437263489,Time: 24328.392243623734\n",
      "Epoch: 1488500,Train Loss: 0.5500796437263489,Time: 24336.577275276184\n",
      "Epoch: 1489000,Train Loss: 0.5500796437263489,Time: 24345.126985311508\n",
      "Epoch: 1489500,Train Loss: 0.5500795841217041,Time: 24353.450482606888\n",
      "Epoch: 1490000,Train Loss: 0.5500795841217041,Time: 24361.643302679062\n",
      "Epoch: 1490500,Train Loss: 0.5500795841217041,Time: 24369.942851781845\n",
      "Epoch: 1491000,Train Loss: 0.5500795841217041,Time: 24378.375834941864\n",
      "Epoch: 1491500,Train Loss: 0.5500796437263489,Time: 24386.57551908493\n",
      "Epoch: 1492000,Train Loss: 0.5500796437263489,Time: 24394.55535387993\n",
      "Epoch: 1492500,Train Loss: 0.5500796437263489,Time: 24402.865763664246\n",
      "Epoch: 1493000,Train Loss: 0.5500796437263489,Time: 24410.877802848816\n",
      "Epoch: 1493500,Train Loss: 0.5500796437263489,Time: 24419.081733942032\n",
      "Epoch: 1494000,Train Loss: 0.5500796437263489,Time: 24427.371126890182\n",
      "Epoch: 1494500,Train Loss: 0.5500796437263489,Time: 24435.80518579483\n",
      "Epoch: 1495000,Train Loss: 0.5500795841217041,Time: 24443.799654722214\n",
      "Epoch: 1495500,Train Loss: 0.5500796437263489,Time: 24452.16371822357\n",
      "Epoch: 1496000,Train Loss: 0.5500795841217041,Time: 24460.39604616165\n",
      "Epoch: 1496500,Train Loss: 0.5500795841217041,Time: 24468.781789064407\n",
      "Epoch: 1497000,Train Loss: 0.5500794649124146,Time: 24477.061992168427\n",
      "Epoch: 1497500,Train Loss: 0.5500794649124146,Time: 24485.282583475113\n",
      "Epoch: 1498000,Train Loss: 0.5500794649124146,Time: 24493.445343494415\n",
      "Epoch: 1498500,Train Loss: 0.5500794649124146,Time: 24501.57107424736\n",
      "Epoch: 1499000,Train Loss: 0.5500794649124146,Time: 24509.634516000748\n",
      "Epoch: 1499500,Train Loss: 0.5500794649124146,Time: 24517.779670476913\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "\n",
    "# 开始训练\n",
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "index=500\n",
    "\n",
    "cnt=-index\n",
    "\n",
    "\n",
    "\n",
    "for e in range(1500000):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net.train()\n",
    "    for im, label in train_data:\n",
    "        im=im.cuda()\n",
    "        label=label.cuda()\n",
    "        # im = Variable(im)\n",
    "        # label = Variable(label)\n",
    "        # 前向传播x\n",
    "\n",
    "        out = net(im)\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        \n",
    "        # 反向传播\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 记录误差\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "\n",
    "    if e%index==0:\n",
    "        cnt+=index\n",
    "        endtime = time.time()\n",
    "        print('Epoch: {},Train Loss: {},Time: {}'.format(e,train_loss/len(train_data),endtime-starttime))\n",
    "        \n",
    "    losses.append(train_loss / len(train_data))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a line plot of `losses`\n",
    "plt.plot(losses)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5500794649124146"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f70518150a0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_fig=net.trainable_part.cpu().detach().numpy()\n",
    "plt.imshow(weight_fig, cmap='hot')  # 'hot' is a popular colormap for heatmaps\n",
    "plt.colorbar()  # Show color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_487009/2944170127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweight_fig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_part_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_fig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hot'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 'hot' is a popular colormap for heatmaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Show color scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "weight_fig=net.non_trainable_part_1.detach().numpy()\n",
    "plt.imshow(weight_fig, cmap='hot')  # 'hot' is a popular colormap for heatmaps\n",
    "plt.colorbar()  # Show color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_487009/313786931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweight_fig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_part_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_fig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hot'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 'hot' is a popular colormap for heatmaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Show color scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "weight_fig=net.non_trainable_part_2.detach().numpy()\n",
    "plt.imshow(weight_fig, cmap='hot')  # 'hot' is a popular colormap for heatmaps\n",
    "plt.colorbar()  # Show color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f7051743640>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAGdCAYAAABdD3qhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAABAWUlEQVR4nO3de3RU5b3H/88ESKLITIqEDMHIRdGAclGQEIpFSw4hUiUWESg/uRjhlEVcYrwAHgSqbVO1VkuhoK0YXEpBfj/FHsB4QhRoJYAEOBUOsoAGEgoTbk1CYklCsn9/UEZHcpvMntkh+/1aay+YvZ/9ne/sJOTL8zz72Q7DMAwBAACEQJjVCQAAAPug8AAAACFD4QEAAEKGwgMAAIQMhQcAAAgZCg8AABAyFB4AACBkKDwAAEDItLU6ATPU1tbqxIkT6tChgxwOh9XpAAD8ZBiGzp8/r9jYWIWFBe//xBcuXFBVVVXAccLDwxUZGWlCRvbTKgqPEydOKC4uzuo0AAABKioq0g033BCU2BcuXFCPHj3k8XgCjuV2u1VQUEDx0QytovDo0KGDJKlorORsZ3EyAAC/lVVLcf/fN/+eB0NVVZU8Ho+KigrkdDqbHaesrExxcT1UVVVF4dEMraLwuDy84mwnOcMtTgYA0GyhGC53Op0BFR4ITKsoPAAAaLqL/94COR/NReEBALAZCg8rBW3q8NKlS9W9e3dFRkYqISFBO3fubLD92rVrFR8fr8jISPXt21cbN24MVmoAAFu7aMKG5gpK4bFmzRplZGRo4cKF2r17t/r376/k5GSdOnWqzvbbtm3TxIkTlZaWpj179ig1NVWpqanat29fMNIDAAAWcRiGYZgdNCEhQXfddZeWLFki6dI6G3FxcXr88cc1d+7cK9qPHz9eFRUVWr9+vXffkCFDNGDAAC1fvrzR9ysrK5PL5VLpBCaXAsDVqKxKcq2WSktLgzbx0/u7ovTvcjqbf/dMWdl5uVw9g5pra2Z6j0dVVZXy8/OVlJT0zZuEhSkpKUl5eXl1npOXl+fTXpKSk5PrbV9ZWamysjKfDQCApmGoxUqmFx5nzpxRTU2NYmJifPbHxMTUu2iLx+Pxq31mZqZcLpd3Y/EwAACuDlfls1rmzZun0tJS71ZUVGR1SgCAqwY9HlYy/XbaTp06qU2bNiouLvbZX1xcLLfbXec5brfbr/YRERGKiIgwJ2EAgM1wO62VTO/xCA8P18CBA5Wbm+vdV1tbq9zcXCUmJtZ5TmJiok97ScrJyam3PQAAuDoFZQGxjIwMTZkyRYMGDdLgwYP1+uuvq6KiQtOmTZMkTZ48WV27dlVmZqYk6YknntDw4cP16quvavTo0Vq9erV27dqlN998MxjpAQBsrebfWyDno7mCUniMHz9ep0+f1oIFC+TxeDRgwABlZ2d7J5AWFhb6PPZ46NChWrVqlebPn6/nnntOvXr10rp163T77bcHIz0AgK3VKLDhEgqPQARlHY9QYx0PALi6hXYdj11yOq8LIE65XK5BrOPRTDyrBQBgM0wutVLrKjxKJbULMIaZS4KcNzEWAMAkFB5Wal2FBwAAjaLwsNJVuYAYAAC4OtHjAQCwGe5qsRKFBwDAZhhqsRJDLQAAIGQoPAAANmPNQ+KWLl2q7t27KzIyUgkJCdq5c2eD7deuXav4+HhFRkaqb9++2rhxo89xwzC0YMECdenSRddcc42SkpJ06NAhnza/+MUvNHToUF177bWKioqq830KCws1evRoXXvttercubOeeeYZXbwYvF4dCg8AgM2EvvBYs2aNMjIytHDhQu3evVv9+/dXcnKyTp06VWf7bdu2aeLEiUpLS9OePXuUmpqq1NRU7du3z9vm5Zdf1uLFi7V8+XLt2LFD7du3V3Jysi5cuOBtU1VVpXHjxmnmzJl1vk9NTY1Gjx6tqqoqbdu2TStXrlRWVpYWLFjg92dsqta1cmmK5GQdDwC46oR25dKP5HS2DyBOhVyuMX7lmpCQoLvuuktLliyRdOnhqXFxcXr88cc1d+7cK9qPHz9eFRUVWr9+vXffkCFDNGDAAC1fvlyGYSg2NlZPPfWUnn76aUmXrl1MTIyysrI0YcIEn3hZWVmaPXu2SkpKfPZ//PHH+tGPfqQTJ054H2uyfPlyzZkzR6dPn1Z4uPnLgdPjAQCwGXN6PMrKyny2ysrKOt+tqqpK+fn5SkpK8u4LCwtTUlKS8vLy6jwnLy/Pp70kJScne9sXFBTI4/H4tHG5XEpISKg3Zn3v07dvX2/Rcfl9ysrKtH///ibH8QeFBwDAZi7fTtvc7dLttHFxcXK5XN7t8hPXv+vMmTOqqanx+eUuSTExMfJ4PHWe4/F4Gmx/+U9/YvrzPt9+D7NxOy0AAM1QVFTkM9QSERFhYTZXD3o8AAA2Y85Qi9Pp9NnqKzw6deqkNm3aqLi42Gd/cXGx3G53nee43e4G21/+05+Y/rzPt9/DbBQeAACbCe1dLeHh4Ro4cKByc3O9+2pra5Wbm6vExMQ6z0lMTPRpL0k5OTne9j169JDb7fZpU1ZWph07dtQbs773+fLLL33ursnJyZHT6VSfPn2aHMcfDLUAAGwm9CuXZmRkaMqUKRo0aJAGDx6s119/XRUVFZo2bZokafLkyeratat3nsgTTzyh4cOH69VXX9Xo0aO1evVq7dq1S2+++aYkyeFwaPbs2fr5z3+uXr16qUePHnr++ecVGxur1NRU7/sWFhbq3LlzKiwsVE1Njfbu3StJuvnmm3Xddddp5MiR6tOnjx555BG9/PLL8ng8mj9/vmbNmhW0oSMKDwAAgmz8+PE6ffq0FixYII/HowEDBig7O9s7kbOwsFBhYd8MQgwdOlSrVq3S/Pnz9dxzz6lXr15at26dbr/9dm+bZ599VhUVFZoxY4ZKSko0bNgwZWdnKzIy0ttmwYIFWrlypff1HXfcIUn67LPPdM8996hNmzZav369Zs6cqcTERLVv315TpkzRCy+8ELRrwToe38U6HgAQcqFdx+NNOZ3XBhDna7lcM4Kaa2tGjwcAwGZ4Oq2VmFwKAABCpnX1ePSS1JJuo/7UpDh3mRQHAKBLPRaB9FrQ4xGI1lV4AADQqNDf1YJvMNQCAABChh4PAIDN0ONhJQoPAIDNcFeLlRhqAQAAIUOPBwDAZhhqsRKFBwDAZig8rEThAQCwGQoPKzHHAwAAhAw9HgAAm6HHw0oUHgAAm+F2Wisx1AIAAEKGHg8AgM1clNQmwPPRXBQeAACbofCwkulDLZmZmbrrrrvUoUMHde7cWampqTp48GCD52RlZcnhcPhskZGRZqcGAAAsZnrhsWXLFs2aNUvbt29XTk6OqqurNXLkSFVUVDR4ntPp1MmTJ73bsWPHzE4NAAB9c1dLIBuay/ShluzsbJ/XWVlZ6ty5s/Lz8/WDH/yg3vMcDofcbrfZ6QAA8B3c1WKloM/xKC0tlSR17NixwXbl5eXq1q2bamtrdeedd+qXv/ylbrvttjrbVlZWqrKy0vu6rKzs0l9ezJac7QPM+GyA53+j6yuppsRJOWBKGEnSHyebFwsAAH8F9Xba2tpazZ49W9///vd1++2319vu1ltv1YoVK/TRRx/p3XffVW1trYYOHarjx4/X2T4zM1Mul8u7xcXFBesjAABaHYZarBTUwmPWrFnat2+fVq9e3WC7xMRETZ48WQMGDNDw4cP1wQcfKDo6Wm+88Uad7efNm6fS0lLvVlRUFIz0AQCtEoWHlYI21JKenq7169dr69atuuGGG/w6t127drrjjjt0+PDhOo9HREQoIiLCjDQBALZzUYH9v5vCIxCm93gYhqH09HR9+OGH+vTTT9WjRw+/Y9TU1OjLL79Uly5dzE4PAABYyPQej1mzZmnVqlX66KOP1KFDB3k8HkmSy+XSNddcI0maPHmyunbtqszMTEnSCy+8oCFDhujmm29WSUmJXnnlFR07dkyPPfaY2ekBAGyvRoHdmcJdLYEwvfBYtmyZJOmee+7x2f/2229r6tSpkqTCwkKFhX3T2fLPf/5T06dPl8fj0fe+9z0NHDhQ27ZtU58+fcxODwBge9xOayXTCw/DMBpts3nzZp/Xr732ml577TWzUwEAAC0Mz2oBANjMRUmOAM9Hc1F4AABshsLDSkFdxwMAAODb6PEAANgMPR5WovAAANgMhYeVGGoBAAAhQ48HAMBmahRYjwfreASCwgMAYDOBDpUw1BIICg8AgM1QeFiJOR4AACBk6PEAANgMPR5Wal2Fx9RRUjurk/jGPx62OoM6xJkUp8ikOAAQcoFODmVyaSAYagEAACHTuno8AABo1EVJjT9JvX70eASCwgMAYDMUHlZiqAUAAIQMPR4AAJuhx8NKFB4AAJuh8LASQy0AACBk6PEAANhMjQLr8ag1KxFbovAAANgMhYeVKDwAADZzUYHNNKDwCARzPAAAQMjQ4wEAsBl6PKxE4QEAsBkKDysx1AIAAEKGHg8AgM3UKLBei0DuiAGFBwDAZi5KcgRwPoVHIBhqAQAAIdO6ejzGSLrW6iS+ZaE5Yf5xwJw4ktT1fZMCrTIpjiTdbWIsAGgUPR5Wal2FBwAAjaLwsBJDLQAAIGTo8QAA2ItRG1inBR0eAaHwAADYS60Cu5uW9cMCQuEBALCXmn9vgZyPZjN9jseiRYvkcDh8tvj4+AbPWbt2reLj4xUZGam+fftq48aNZqcFAABagKBMLr3tttt08uRJ7/bXv/613rbbtm3TxIkTlZaWpj179ig1NVWpqanat29fMFIDANhdjQkbmi0ohUfbtm3ldru9W6dOnept+9vf/lajRo3SM888o969e+vFF1/UnXfeqSVLlgQjNQCA3dWasKHZglJ4HDp0SLGxserZs6cmTZqkwsLCetvm5eUpKSnJZ19ycrLy8vLqPaeyslJlZWU+GwAAaPlMLzwSEhKUlZWl7OxsLVu2TAUFBbr77rt1/vz5Ott7PB7FxMT47IuJiZHH46n3PTIzM+VyubxbXFycqZ8BANCKWTTUsnTpUnXv3l2RkZFKSEjQzp07G2zf2PxHwzC0YMECdenSRddcc42SkpJ06NAhnzbnzp3TpEmT5HQ6FRUVpbS0NJWXl3uPHz169Ip5mQ6HQ9u3b2/eh2wC0wuPlJQUjRs3Tv369VNycrI2btyokpISvf++WWt1S/PmzVNpaal3KyoqMi02AKCVs2CoZc2aNcrIyNDChQu1e/du9e/fX8nJyTp16lSd7Zsy//Hll1/W4sWLtXz5cu3YsUPt27dXcnKyLly44G0zadIk7d+/Xzk5OVq/fr22bt2qGTNmXPF+mzZt8pmbOXDgQP8/ZBMFfeXSqKgo3XLLLTp8+HCdx91ut4qLi332FRcXy+121xszIiJCTqfTZwMAoKX6zW9+o+nTp2vatGnq06ePli9frmuvvVYrVqyos31j8x8Nw9Drr7+u+fPna8yYMerXr5/eeecdnThxQuvWrZMkHThwQNnZ2frjH/+ohIQEDRs2TL/73e+0evVqnThxwuf9rr/+ep+5me3atQvatQh64VFeXq4jR46oS5cudR5PTExUbm6uz76cnBwlJiYGOzUAgB3VKrBhln/3eHx3rmFlZWWdb1dVVaX8/Hyf+YxhYWFKSkqqdz5jY/MfCwoK5PF4fNq4XC4lJCR42+Tl5SkqKkqDBg3ytklKSlJYWJh27NjhE/uBBx5Q586dNWzYMP35z3+u58KZw/TC4+mnn9aWLVt09OhRbdu2TQ8++KDatGmjiRMnSpImT56sefPmeds/8cQTys7O1quvvqqvvvpKixYt0q5du5Senm52agAAmDbHIy4uzme+YWZmZp1vd+bMGdXU1Pg1n7Gx+Y+X/2ysTefOnX2Ot23bVh07dvS2ue666/Tqq69q7dq12rBhg4YNG6bU1NSgFh+mr1x6/PhxTZw4UWfPnlV0dLSGDRum7du3Kzo6WpJUWFiosLBv6p2hQ4dq1apVmj9/vp577jn16tVL69at0+233252agAAmKaoqMhnqD8iIsLCbJqnU6dOysjI8L6+6667dOLECb3yyit64IEHgvKephceq1evbvD45s2br9g3btw4jRs3zuxUAAC4kknPamnqHMNOnTqpTZs2fs1nbGz+4+U/i4uLfaYyFBcXa8CAAd423528evHiRZ07d67BeZQJCQnKyclp9HM1V9DneAAA0KKE+Hba8PBwDRw40Gc+Y21trXJzc+udz9jY/McePXrI7Xb7tCkrK9OOHTu8bRITE1VSUqL8/Hxvm08//VS1tbVKSEioN9+9e/fWOy/TDDwkDgBgLxY8JC4jI0NTpkzRoEGDNHjwYL3++uuqqKjQtGnTJF2a/9i1a1fvPJEnnnhCw4cP16uvvqrRo0dr9erV2rVrl958801JksPh0OzZs/Xzn/9cvXr1Uo8ePfT8888rNjZWqampkqTevXtr1KhRmj59upYvX67q6mqlp6drwoQJio2NlSStXLlS4eHhuuOOOyRJH3zwgVasWKE//vGPAVyghrWuwiNbUrjVSXzLXeaE6WpSHEnSepPi3G1SHACwgfHjx+v06dNasGCBPB6PBgwYoOzsbO/k0ObMf3z22WdVUVGhGTNmqKSkRMOGDVN2drYiIyO9bd577z2lp6drxIgRCgsL09ixY7V48WKf3F588UUdO3ZMbdu2VXx8vNasWaOHHnooaNfCYRiGEbToIVJWViaXy6XSCZKzJRUeAIAmKauSXKul0tLSoK3N5P1d8TfJ2SGAOOclV7/g5tqata4eDwAAGmPBUAu+weRSAAAQMvR4AADsxVBgt9Ne9RMUrEXhAQCwF4ZaLMVQCwAACBl6PAAA9kKPh6UoPAAA9mLSkuloHoZaAABAyNDjAQCwF4ZaLEXhAQCwFwoPS1F4AADshTkelmKOBwAACBl6PAAA9lKrwIZL6PEICIUHAMBeGGqxFEMtAAAgZOjxAADYC3e1WKp1FR5v3Cs5W85HetqRY0qcNqZEueQlo+V1cu11mNNvaeZ16jvZxGAAWhYKD0u1vN9CAACg1Wo53QMAAIQCk0stReEBALAXhlosxVALAAAIGXo8AAD2Qo+HpSg8AAD2YiiweRqGWYnYE4UHAMBe6PGwFHM8AABAyNDjAQCwF26ntRSFBwDAXhhqsRRDLQAAIGTo8QAA2As9Hpai8AAA2AtzPCzFUAsAAAgZ0wuP7t27y+FwXLHNmjWrzvZZWVlXtI2MjDQ7LQAALqkxYUOzmT7U8sUXX6im5puvyr59+/Qf//EfGjduXL3nOJ1OHTx40Pva4XCYnRYAAJfUKrDigaGWgJheeERHR/u8/tWvfqWbbrpJw4cPr/cch8Mht9ttdioAAFyJOR6WCuocj6qqKr377rt69NFHG+zFKC8vV7du3RQXF6cxY8Zo//79wUwLAABYJKh3taxbt04lJSWaOnVqvW1uvfVWrVixQv369VNpaal+/etfa+jQodq/f79uuOGGOs+prKxUZWWl93VZWdmlv8z8TAo38xME5tcPmxToWpPiSNK0lleqD5hqUqCvTYojSRdMisN0JaDl4XZaSwW1x+Ott95SSkqKYmNj622TmJioyZMna8CAARo+fLg++OADRUdH64033qj3nMzMTLlcLu8WFxcXjPQBAK1RrQkbmi1ohcexY8e0adMmPfbYY36d165dO91xxx06fPhwvW3mzZun0tJS71ZUVBRougAAIASCNtTy9ttvq3Pnzho9erRf59XU1OjLL7/UfffdV2+biIgIRUREBJoiAMCOGGqxVFAKj9raWr399tuaMmWK2rb1fYvJkyera9euyszMlCS98MILGjJkiG6++WaVlJTolVde0bFjx/zuKQEAoEkoPCwVlMJj06ZNKiws1KOPPnrFscLCQoWFfTPC889//lPTp0+Xx+PR9773PQ0cOFDbtm1Tnz59gpEaAACwkMMwDMPqJAJVVlYml8ul0p9IzhZ0V4tpd1mYeVdLa2bmXS1m4a4WoEnKqiTXaqm0tFROpzM473H5d8XLkvOaAOL8S3I9G9xcWzMeEgcAsBdWLrUUD4kDAAAhQ48HAMBeWDLdUhQeAAB74a4WS1F4AADshcLDUszxAAAAIUOPBwDAXpjjYSkKDwCAvTDUYimGWgAAQMjQ4wEAsBd6PCxF4QEAsBdDgc3TuOofNGKt1lV4uCRFWJ3EN/6RZU6crmZ+ptUmxYk1KY4k7TApzjMmxZGkBJPi/NCkOJL0dxNjAYBFWlfhAQBAYxhqsRSFBwDAXrid1lLc1QIAAEKGHg8AgL0w1GIpCg8AgL1QeFiKwgMAYC/M8bAUczwAAEDI0OMBALAXhlosReEBALCXWgVWPDDUEhCGWgAAQMjQ4wEAsBcml1qKwgMAYC/M8bAUQy0AACBkKDwAAPZSa8LWDEuXLlX37t0VGRmphIQE7dy5s8H2a9euVXx8vCIjI9W3b19t3LjR57hhGFqwYIG6dOmia665RklJSTp06JBPm3PnzmnSpElyOp2KiopSWlqaysvLfdr87W9/0913363IyEjFxcXp5Zdfbt4HbCIKDwCAvdSYsPlpzZo1ysjI0MKFC7V79271799fycnJOnXqVJ3tt23bpokTJyotLU179uxRamqqUlNTtW/fPm+bl19+WYsXL9by5cu1Y8cOtW/fXsnJybpw4YK3zaRJk7R//37l5ORo/fr12rp1q2bMmOE9XlZWppEjR6pbt27Kz8/XK6+8okWLFunNN9/0/0M2kcMwDCNo0UOkrKxMLpdLpTMlZ4TV2XzjH6+bE6ermZ9ptUlxYk2KI0k7TIrzjElxJCnBpDg/NCmOJP3dxFhAC1NWJblWS6WlpXI6ncF5j8u/KyZJzvAA4lRJrvf8yzUhIUF33XWXlixZIkmqra1VXFycHn/8cc2dO/eK9uPHj1dFRYXWr1/v3TdkyBANGDBAy5cvl2EYio2N1VNPPaWnn35a0qV8YmJilJWVpQkTJujAgQPq06ePvvjiCw0aNEiSlJ2drfvuu0/Hjx9XbGysli1bpv/6r/+Sx+NRePilizJ37lytW7dOX331VfMvUgPo8QAA2ItJPR5lZWU+W2VlZZ1vV1VVpfz8fCUlJXn3hYWFKSkpSXl5eXWek5eX59NekpKTk73tCwoK5PF4fNq4XC4lJCR42+Tl5SkqKspbdEhSUlKSwsLCtGPHDm+bH/zgB96i4/L7HDx4UP/85z/ru4IBofAAANiLSXM84uLi5HK5vFtmZmadb3fmzBnV1NQoJibGZ39MTIw8Hk+d53g8ngbbX/6zsTadO3f2Od62bVt17NjRp01dMb79HmZrXbfT/lNSAN1nZus62eoM6vCh1QkE0XirE6gDwyNAy2PSyqVFRUU+Qy0RES1orL8Fo8cDAIBmcDqdPlt9hUenTp3Upk0bFRcX++wvLi6W2+2u8xy3291g+8t/Ntbmu5NXL168qHPnzvm0qSvGt9/DbBQeAAB7CfFdLeHh4Ro4cKByc3O9+2pra5Wbm6vExMQ6z0lMTPRpL0k5OTne9j169JDb7fZpU1ZWph07dnjbJCYmqqSkRPn5+d42n376qWpra5WQkOBts3XrVlVXV/u8z6233qrvfe97/n3QJqLwAADYiwXreGRkZOgPf/iDVq5cqQMHDmjmzJmqqKjQtGnTJEmTJ0/WvHnzvO2feOIJZWdn69VXX9VXX32lRYsWadeuXUpPT5ckORwOzZ49Wz//+c/15z//WV9++aUmT56s2NhYpaamSpJ69+6tUaNGafr06dq5c6c+//xzpaena8KECYqNvXRr4k9+8hOFh4crLS1N+/fv15o1a/Tb3/5WGRkZ/n/IJmpdczwAAGiBxo8fr9OnT2vBggXyeDwaMGCAsrOzvRM5CwsLFRb2TV/A0KFDtWrVKs2fP1/PPfecevXqpXXr1un222/3tnn22WdVUVGhGTNmqKSkRMOGDVN2drYiIyO9bd577z2lp6drxIgRCgsL09ixY7V48WLvcZfLpf/5n//RrFmzNHDgQHXq1EkLFizwWevDbK1rHY8Jgd2bDQCwRkjX8fiR5GwXQJxqybU+uLm2Zn4PtWzdulX333+/YmNj5XA4tG7dOp/jTVnCtS7+LiULAECzWLRkOi7xu/CoqKhQ//79tXTp0jqPN2UJ1+/ydylZAABwdQpoqMXhcOjDDz/0TmRpyhKudfF3KdnvYqgFAK5uIR1qSTZhqOUThlqay9S7WpqyhOt3NWcp2crKyiuWqgUAoEkseEgcvmFq4dGUJVy/qzlLyWZmZvosUxsXF2dC9gAAINiuynU85s2bp9LSUu9WVFRkdUoAgKuFocAmll7194Jay9R1PL69hGuXLl28+4uLizVgwIA6z2nOUrIRERGsiQ8AaJ4aSY4Az0ezmdrj0ZQlXL+rOUvJAgDQbMzxsJTfPR7l5eU6fPiw93VBQYH27t2rjh076sYbb/Qu4dqrVy/16NFDzz//vM8SrpI0YsQIPfjgg96lXzMyMjRlyhQNGjRIgwcP1uuvv+6zlCwAAGgd/C48du3apXvvvdf7+vJ67lOmTFFWVlaTlnA9cuSIzpw5433d2FKyAACYJtBFwFhALCAsmQ4AsFxI1/EYIjkDmOFYdlFybWcdj+a6Ku9qAQAAV6fW9XTaN0qlFlR9jnEEMm36G6dNiXLJNuP/NSlSlElxJOmvJsYyR7pjkSlxzPzabTEpznmT4khSxWQTgwGhwlCLpVpX4QEAQGO4ndZSDLUAAICQoccDAGAvtQqs14KhloBQeAAA7KVWgQ21UHgEhKEWAAAQMvR4AADsJdDJoUwuDQiFBwDAXig8LEXhAQCwF+Z4WIo5HgAAIGTo8QAA2AtDLZai8AAA2AtDLZZiqAUAAIQMPR4AAHsJtMeCHo+AUHgAAOylRpIRwPkUHgFhqAUAAIQMPR4AAHthqMVSFB4AAHthqMVSDLUAAICQaV09Hv/pksKtTuIbH022OoM6THnI6gyuCkta4tcOgDno8bBU6yo8AABoDHM8LEXhAQCwl1oF1uMRyLlgjgcAAAgdejwAAPYS6LNa6PEICIUHAMBeakThYSGGWgAAQMjQ4wEAsBd6PCxF4QEAsBfmeFiKoRYAABAy9HgAAOyFoRZLUXgAAOyFwsNSDLUAAICQoccDAGAvhui1sBCFBwDAVmr+vQVyPprP76GWrVu36v7771dsbKwcDofWrVvnPVZdXa05c+aob9++at++vWJjYzV58mSdOHGiwZiLFi2Sw+Hw2eLj4/3+MAAANKbGhA3N53fhUVFRof79+2vp0qVXHPv666+1e/duPf/889q9e7c++OADHTx4UA888ECjcW+77TadPHnSu/31r3/1NzUAANDC+T3UkpKSopSUlDqPuVwu5eTk+OxbsmSJBg8erMLCQt144431J9K2rdxut7/pAADgl9p/b4Gcj+YL+hyP0tJSORwORUVFNdju0KFDio2NVWRkpBITE5WZmVlvoVJZWanKykrv67Kyskt/6SQpwqTEzXDepDhHTYojSX1NjGWWo1YnUAezvnZm6m51AnU4a1IcM683/39BI5jjYa2g3k574cIFzZkzRxMnTpTT6ay3XUJCgrKyspSdna1ly5apoKBAd999t86fr/tfo8zMTLlcLu8WFxcXrI8AAABMFLTCo7q6Wg8//LAMw9CyZcsabJuSkqJx48apX79+Sk5O1saNG1VSUqL333+/zvbz5s1TaWmpdysqKgrGRwAAtEK1JmxovqAMtVwuOo4dO6ZPP/20wd6OukRFRemWW27R4cOH6zweERGhiIiWNKYCALhaMNRiLdN7PC4XHYcOHdKmTZt0/fXX+x2jvLxcR44cUZcuXcxODwAAWMjvwqO8vFx79+7V3r17JUkFBQXau3evCgsLVV1drYceeki7du3Se++9p5qaGnk8Hnk8HlVVVXljjBgxQkuWLPG+fvrpp7VlyxYdPXpU27Zt04MPPqg2bdpo4sSJgX9CAAC+pVaBreHBUEtg/B5q2bVrl+69917v64yMDEnSlClTtGjRIv35z3+WJA0YMMDnvM8++0z33HOPJOnIkSM6c+aM99jx48c1ceJEnT17VtHR0Ro2bJi2b9+u6Ohof9MDAKBB3E5rLb8Lj3vuuUeGUf8i9w0du+zo0aM+r1evXu1vGgAA4CrEs1oAALbC5FJrUXgAAGyFwsNaFB4AAFthjoe1grpyKQAAwLfR4wEAsBWGWqxF4QEAsBWGWqzFUAsAAAgZejwAALZyeeXSQM5H81F4AABshTke1mKoBQAAhAyFBwDAVmpN2ILp3LlzmjRpkpxOp6KiopSWlqby8vIGz7lw4YJmzZql66+/Xtddd53Gjh2r4uJinzaFhYUaPXq0rr32WnXu3FnPPPOMLl686NNm8+bNuvPOOxUREaGbb75ZWVlZPscXLVokh8Phs8XHx/v1+VrXUEu1WlYpFWlSnPYmxZEuXSMzmNnX2NXEWC3NFybGOm1SnHYmxZHM+xfYrJ8VSTpkUpxeJsVBi9PSh1omTZqkkydPKicnR9XV1Zo2bZpmzJihVatW1XvOk08+qQ0bNmjt2rVyuVxKT0/Xj3/8Y33++eeXcq6p0ejRo+V2u7Vt2zadPHlSkydPVrt27fTLX/5S0qWnzY8ePVo//elP9d577yk3N1ePPfaYunTpouTkZO973Xbbbdq0aZP3ddu2/pUSDqMpT3Vr4crKyuRyuVT6n5IzwupsgqDIxFhxJsUx8yevjYmxWhozCw+3SXHMLDxaYiFrVoFG4RFSZVWSa7VUWloqp9MZnPf49++KbZKuCyBOuaShCk6uBw4cUJ8+ffTFF19o0KBBkqTs7Gzdd999On78uGJjY684p7S0VNHR0Vq1apUeeughSdJXX32l3r17Ky8vT0OGDNHHH3+sH/3oRzpx4oRiYmIkScuXL9ecOXN0+vRphYeHa86cOdqwYYP27dvnjT1hwgSVlJQoOztb0qUej3Xr1mnv3r3N/owtqX8AAICgqzFhky4VMt/eKisrA84tLy9PUVFR3qJDkpKSkhQWFqYdO3bUeU5+fr6qq6uVlJTk3RcfH68bb7xReXl53rh9+/b1Fh2SlJycrLKyMu3fv9/b5tsxLre5HOOyQ4cOKTY2Vj179tSkSZNUWFjo12ek8AAA2IpZczzi4uLkcrm8W2ZmZsC5eTwede7c2Wdf27Zt1bFjR3k8nnrPCQ8PV1RUlM/+mJgY7zkej8en6Lh8/PKxhtqUlZXpX//6lyQpISFBWVlZys7O1rJly1RQUKC7775b58+fb/JnbF1zPAAAaIRZczyKiop8hloiIuof6587d65eeumlBuMeOHAggKxCIyUlxfv3fv36KSEhQd26ddP777+vtLS0JsWg8AAAoBmcTmeT53g89dRTmjp1aoNtevbsKbfbrVOnTvnsv3jxos6dOye3u+6JXm63W1VVVSopKfHp9SguLvae43a7tXPnTp/zLt/18u02370Tpri4WE6nU9dcc02d7x0VFaVbbrlFhw8fbvCzfRuFBwDAVgwFdkNWc+7IiI6OVnR0dKPtEhMTVVJSovz8fA0cOFCS9Omnn6q2tlYJCQl1njNw4EC1a9dOubm5Gjt2rCTp4MGDKiwsVGJiojfuL37xC506dco7lJOTkyOn06k+ffp422zcuNEndk5OjjdGXcrLy3XkyBE98sgjjX62y5jjAQCwFbMmlwZD7969NWrUKE2fPl07d+7U559/rvT0dE2YMMF7R8s//vEPxcfHe3swXC6X0tLSlJGRoc8++0z5+fmaNm2aEhMTNWTIEEnSyJEj1adPHz3yyCP63//9X33yySeaP3++Zs2a5R0i+ulPf6q///3vevbZZ/XVV1/p97//vd5//309+eST3vyefvppbdmyRUePHtW2bdv04IMPqk2bNpo4cWKTPyM9HgAAtCDvvfee0tPTNWLECIWFhWns2LFavHix93h1dbUOHjyor7/+2rvvtdde87atrKxUcnKyfv/733uPt2nTRuvXr9fMmTOVmJio9u3ba8qUKXrhhRe8bXr06KENGzboySef1G9/+1vdcMMN+uMf/+izhsfx48c1ceJEnT17VtHR0Ro2bJi2b9/epN6cy1jH42rAOh5XL9bxaBrW8bC9UK7j8bECW5exQlKKgptra0aPBwDAVgJd9pyn0waGOR4AACBk6PEAANhKS39WS2tH4QEAsBUKD2sx1AIAAEKGHg8AgK0wudRaFB4AAFupVWDDJRQegaHwAADYCj0e1mpdhcf/qrV9IvPVvdS//9abFEeS+pgU5x8mxTHT/2NirD+ZFMesReQkKcqkOGZ+7cy65rEmxZGkD02MBVzl+DUNALAV7mqxFoUHAMBWKDysxe20AAAgZOjxAADYCpNLrUXhAQCwFYZarOX3UMvWrVt1//33KzY2Vg6HQ+vWrfM5PnXqVDkcDp9t1KhRjcZdunSpunfvrsjISCUkJGjnzp3+pgYAAFo4vwuPiooK9e/fX0uXLq23zahRo3Ty5Env9qc/NXwf4Jo1a5SRkaGFCxdq9+7d6t+/v5KTk3Xq1Cl/0wMAoEE1JmxoPr+HWlJSUpSSktJgm4iICLnd7ibH/M1vfqPp06dr2rRpkqTly5drw4YNWrFihebOnetvigAA1MtQYPM0DLMSsamg3NWyefNmde7cWbfeeqtmzpyps2fP1tu2qqpK+fn5SkpK+iapsDAlJSUpLy+vznMqKytVVlbmswEAgJbP9MJj1KhReuedd5Sbm6uXXnpJW7ZsUUpKimpq6u6cOnPmjGpqahQTE+OzPyYmRh6Pp85zMjMz5XK5vFtcnJlLMQIAWjOGWqxl+l0tEyZM8P69b9++6tevn2666SZt3rxZI0aMMOU95s2bp4yMDO/rsrIyig8AQJNwO621gr6AWM+ePdWpUycdPny4zuOdOnVSmzZtVFxc7LO/uLi43nkiERERcjqdPhsAAE1Bj4e1gl54HD9+XGfPnlWXLl3qPB4eHq6BAwcqNzfXu6+2tla5ublKTEwMdnoAACCE/C48ysvLtXfvXu3du1eSVFBQoL1796qwsFDl5eV65plntH37dh09elS5ubkaM2aMbr75ZiUnJ3tjjBgxQkuWLPG+zsjI0B/+8AetXLlSBw4c0MyZM1VRUeG9ywUAALPQ42Etv+d47Nq1S/fee6/39eW5FlOmTNGyZcv0t7/9TStXrlRJSYliY2M1cuRIvfjii4qIiPCec+TIEZ05c8b7evz48Tp9+rQWLFggj8ejAQMGKDs7+4oJpwAABIo5Htbyu/C45557ZBj138X8ySefNBrj6NGjV+xLT09Xenq6v+kAAICrCM9qAQDYCs9qsRaFBwDAVmoVWPHAUEtgWlfh0V1SuNVJtHD/Z1KcnibFkaQLJsW53qQ4ZtphYiwzr7lZWuLXzsxrDsB0ravwAACgEUwutRaFBwDAVpjjYa2gLyAGAABwGT0eAABbYajFWhQeAABbYajFWhQeAABbofCwFnM8AABAyNDjAQCwFeZ4WIvCAwBgK6xcai2GWgAAQMjQ4wEAsBUml1qLwgMAYCvM8bAWQy0AACBk6PEAANgKQy3WovAAANgKQy3WYqgFAACEDD0eAABbYajFWq2r8Ah0VRg7aGN1AnVoiV+zlnidWqLW/LUz87Px/dSiUHhYq3UVHgAANMJQYPM0DLMSsSnmeAAAgJChxwMAYCsMtViLwgMAYCsUHtZiqAUAAIQMPR4AAFthATFrUXgAAGyFoRZrMdQCAABChh4PAICtMNRiLQoPAICtMNRiLYZaAABAyNDjAQCwlUAf68VQS2AoPAAAtsIcD2tReAAAbKVGgc0zYI5HYPy+9lu3btX999+v2NhYORwOrVu3zue4w+Goc3vllVfqjblo0aIr2sfHx/v9YQAAQMvmd49HRUWF+vfvr0cffVQ//vGPrzh+8uRJn9cff/yx0tLSNHbs2Abj3nbbbdq0adM3ibWlMwYAYD56PKzl92/3lJQUpaSk1Hvc7Xb7vP7oo4907733qmfPng0n0rbtFecCAGA25nhYK6i30xYXF2vDhg1KS0trtO2hQ4cUGxurnj17atKkSSosLKy3bWVlpcrKynw2AADQ8gV1PGPlypXq0KFDnUMy35aQkKCsrCzdeuutOnnypH72s5/p7rvv1r59+9ShQ4cr2mdmZupnP/vZlYHCJLUxKXmEDl+zq1dr/tq15s9mcwy1WCuoPR4rVqzQpEmTFBkZ2WC7lJQUjRs3Tv369VNycrI2btyokpISvf/++3W2nzdvnkpLS71bUVFRMNIHALRCtSZsaL6g9Xj85S9/0cGDB7VmzRq/z42KitItt9yiw4cP13k8IiJCERERgaYIAABCLGg9Hm+99ZYGDhyo/v37+31ueXm5jhw5oi5dugQhMwCAnV1eubS5W7B7PM6dO6dJkybJ6XQqKipKaWlpKi8vb/CcCxcuaNasWbr++ut13XXXaezYsSouLvZpU1hYqNGjR+vaa69V586d9cwzz+jixYve4ydPntRPfvIT3XLLLQoLC9Ps2bPrfK+1a9cqPj5ekZGR6tu3rzZu3OjX5/O78CgvL9fevXu1d+9eSVJBQYH27t3rMxm0rKxMa9eu1WOPPVZnjBEjRmjJkiXe108//bS2bNmio0ePatu2bXrwwQfVpk0bTZw40d/0AABoUCBFR6APmGuKSZMmaf/+/crJydH69eu1detWzZgxo8FznnzySf33f/+31q5dqy1btujEiRM+8ytramo0evRoVVVVadu2bVq5cqWysrK0YMECb5vKykpFR0dr/vz59XYabNu2TRMnTlRaWpr27Nmj1NRUpaamat++fU3+fA7DMIwmt5a0efNm3XvvvVfsnzJlirKysiRJb775pmbPnq2TJ0/K5XJd0bZ79+6aOnWqFi1aJEmaMGGCtm7dqrNnzyo6OlrDhg3TL37xC910001NyqmsrEwul0ulEyRnuD+fBgDQEpRVSa7VUmlpqZxOZ3De49+/K+5RYPMMLkrarODkeuDAAfXp00dffPGFBg0aJEnKzs7Wfffdp+PHjys2NvaKc0pLSxUdHa1Vq1bpoYcekiR99dVX6t27t/Ly8jRkyBB9/PHH+tGPfqQTJ04oJiZGkrR8+XLNmTNHp0+fVni47y/Pe+65RwMGDNDrr7/us3/8+PGqqKjQ+vXrvfuGDBmiAQMGaPny5U36jH5f+3vuuUeN1SozZsxosDo7evSoz+vVq1f7mwYAAM1i1joe313KwYz5h3l5eYqKivIWHZKUlJSksLAw7dixQw8++OAV5+Tn56u6ulpJSUneffHx8brxxhu9hUdeXp769u3rLTokKTk5WTNnztT+/ft1xx13NDm/jIwMn33JyclXrGLekKDe1QIAQEtj1lBLXFycXC6Xd8vMzAw4N4/Ho86dO/vsa9u2rTp27CiPx1PvOeHh4YqKivLZHxMT4z3H4/H4FB2Xj18+5k9+dcXxJwbrkgMAbKVGkiPA8yWpqKjIZ6ilod6OuXPn6qWXXmow7oEDBwLI6upB4QEAQDM4nc4mz/F46qmnNHXq1Abb9OzZU263W6dOnfLZf/HiRZ07d67ex4q43W5VVVWppKTEp9ejuLjYe47b7dbOnTt9zrt814s/jytxu91X3C3z7fdpCoZaAAC2YsUCYtHR0YqPj29wCw8PV2JiokpKSpSfn+8999NPP1Vtba0SEhLqjD1w4EC1a9dOubm53n0HDx5UYWGhEhMTJUmJiYn68ssvfYqanJwcOZ1O9enTp8mfIzEx0ed9Lse5/D5NQY8HAMBWzBpqCYbevXtr1KhRmj59upYvX67q6mqlp6drwoQJ3jta/vGPf2jEiBF65513NHjwYLlcLqWlpSkjI0MdO3aU0+nU448/rsTERA0ZMkSSNHLkSPXp00ePPPKIXn75ZXk8Hs2fP1+zZs3yGSK6vFRGeXm5Tp8+rb179yo8PNxbnDzxxBMaPny4Xn31VY0ePVqrV6/Wrl279Oabbzb5M1J4AADQgrz33ntKT0/XiBEjFBYWprFjx2rx4sXe49XV1Tp48KC+/vpr777XXnvN27ayslLJycn6/e9/7z3epk0brV+/XjNnzlRiYqLat2+vKVOm6IUXXvB572/f3ZKfn69Vq1apW7du3rtRhw4dqlWrVmn+/Pl67rnn1KtXL61bt0633357kz+f3+t4tESs4wEAV7dQruNxpwJ7BmCNpN0Kbq6tGT0eAABbCXSohKfTBobJpQAAIGTo8QAA2Ao9Htai8AAA2EqtArurJdhPp23tWlfhcUqt7RNdctrEWFNMinPBpDiS9JVJcQKZLRasWFc+T7HZ/mnSw5q/d7M5cSRJ0SbFaWdSHEmKNynO6ybFkaQHTIpT9wNDm2eLSXHGmBTHzH9T0KK1xl/TAADUi6EWa1F4AABshcLDWhQeAABbYY6HtbidFgAAhAw9HgAAWwm0x4Iej8BQeAAAbIXCw1oMtQAAgJChxwMAYCs1kgJ5Oio9HoGh8AAA2AqFh7UYagEAACFDjwcAwFaYXGotCg8AgK0w1GIthloAAEDI0OMBALCVWgXW4xHIuaDwAADYTKDPaqHwCAyFBwDAVmpE4WEl5ngAAICQaRU9HoZxqf4su2hxIsFSY2KsCy0sjiRVmRSnjUlxzIz1tUlxJJWZFKeNmVPyzfqZC+S/n99l1veTWRdcMu86VZoURzLv3xWT/i0o+/dnu/zveTDR42EthxGKr3KQHT9+XHFxcVanAQAIUFFRkW644YagxL5w4YJ69Oghj8cTcCy3262CggJFRkaakJm9tIrCo7a2VidOnFCHDh3kcNRfx5aVlSkuLk5FRUVyOp0hzDAw5B1aV2ve0tWbO3mHVkvM2zAMnT9/XrGxsQoLC94sgAsXLqiqKvBusfDwcIqOZmoVQy1hYWF+VchOp7PF/LD5g7xD62rNW7p6cyfv0GppebtcrqC/R2RkJAWDxZhcCgAAQobCAwAAhIytCo+IiAgtXLhQERERVqfiF/IOras1b+nqzZ28Q+tqzRutQ6uYXAoAAK4OturxAAAA1qLwAAAAIUPhAQAAQobCAwAAhEyrKzyWLl2q7t27KzIyUgkJCdq5c2eD7deuXav4+HhFRkaqb9++2rhxY4gyvSQzM1N33XWXOnTooM6dOys1NVUHDx5s8JysrCw5HA6fLdQL4ixatOiKHOLj4xs8x+prLUndu3e/Im+Hw6FZs2bV2d7Ka71161bdf//9io2NlcPh0Lp163yOG4ahBQsWqEuXLrrmmmuUlJSkQ4cONRrX358RM/Ourq7WnDlz1LdvX7Vv316xsbGaPHmyTpw40WDM5ny/mZm3JE2dOvWKHEaNGtVoXCuvt6Q6v98dDodeeeWVemOG4nrDvlpV4bFmzRplZGRo4cKF2r17t/r376/k5GSdOnWqzvbbtm3TxIkTlZaWpj179ig1NVWpqanat29fyHLesmWLZs2ape3btysnJ0fV1dUaOXKkKioqGjzP6XTq5MmT3u3YsWMhyvgbt912m08Of/3rX+tt2xKutSR98cUXPjnn5ORIksaNG1fvOVZd64qKCvXv319Lly6t8/jLL7+sxYsXa/ny5dqxY4fat2+v5ORkXbhQ/1O7/P0ZMTvvr7/+Wrt379bzzz+v3bt364MPPtDBgwf1wAMPNBrXn+83s/O+bNSoUT45/OlPf2owptXXW5JPvidPntSKFSvkcDg0duzYBuMG+3rDxoxWZPDgwcasWbO8r2tqaozY2FgjMzOzzvYPP/ywMXr0aJ99CQkJxn/+538GNc+GnDp1ypBkbNmypd42b7/9tuFyuUKXVB0WLlxo9O/fv8ntW+K1NgzDeOKJJ4ybbrrJqK2trfN4S7jWhmEYkowPP/zQ+7q2ttZwu93GK6+84t1XUlJiREREGH/605/qjePvz4jZeddl586dhiTj2LFj9bbx9/stUHXlPWXKFGPMmDF+xWmJ13vMmDHGD3/4wwbbhPp6w15aTY9HVVWV8vPzlZSU5N0XFhampKQk5eXl1XlOXl6eT3tJSk5Orrd9KJSWlkqSOnbs2GC78vJydevWTXFxcRozZoz2798fivR8HDp0SLGxserZs6cmTZqkwsLCetu2xGtdVVWld999V48++miDDxdsCdf6uwoKCuTxeHyuqcvlUkJCQr3XtDk/I6FQWloqh8OhqKioBtv58/0WLJs3b1bnzp116623aubMmTp79my9bVvi9S4uLtaGDRuUlpbWaNuWcL3ROrWawuPMmTOqqalRTEyMz/6YmJh6H4Hs8Xj8ah9stbW1mj17tr7//e/r9ttvr7fdrbfeqhUrVuijjz7Su+++q9raWg0dOlTHjx8PWa4JCQnKyspSdna2li1bpoKCAt199906f/58ne1b2rWWpHXr1qmkpERTp06tt01LuNZ1uXzd/LmmzfkZCbYLFy5ozpw5mjhxYoMPK/P3+y0YRo0apXfeeUe5ubl66aWXtGXLFqWkpKimpqbO9i3xeq9cuVIdOnTQj3/84wbbtYTrjdarVTydtrWYNWuW9u3b1+hYamJiohITE72vhw4dqt69e+uNN97Qiy++GOw0JUkpKSnev/fr108JCQnq1q2b3n///Sb9b6oleOutt5SSkqLY2Nh627SEa91aVVdX6+GHH5ZhGFq2bFmDbVvC99uECRO8f+/bt6/69eunm266SZs3b9aIESNCkkOgVqxYoUmTJjU6QbolXG+0Xq2mx6NTp05q06aNiouLffYXFxfL7XbXeY7b7farfTClp6dr/fr1+uyzz3TDDTf4dW67du10xx136PDhw0HKrnFRUVG65ZZb6s2hJV1rSTp27Jg2bdqkxx57zK/zWsK1luS9bv5c0+b8jATL5aLj2LFjysnJ8fvR7I19v4VCz5491alTp3pzaEnXW5L+8pe/6ODBg35/z0st43qj9Wg1hUd4eLgGDhyo3Nxc777a2lrl5ub6/I/12xITE33aS1JOTk697YPBMAylp6frww8/1KeffqoePXr4HaOmpkZffvmlunTpEoQMm6a8vFxHjhypN4eWcK2/7e2331bnzp01evRov85rCddaknr06CG32+1zTcvKyrRjx456r2lzfkaC4XLRcejQIW3atEnXX3+93zEa+34LhePHj+vs2bP15tBSrvdlb731lgYOHKj+/fv7fW5LuN5oRaye3Wqm1atXGxEREUZWVpbxf//3f8aMGTOMqKgow+PxGIZhGI888ogxd+5cb/vPP//caNu2rfHrX//aOHDggLFw4UKjXbt2xpdffhmynGfOnGm4XC5j8+bNxsmTJ73b119/7W3z3bx/9rOfGZ988olx5MgRIz8/35gwYYIRGRlp7N+/P2R5P/XUU8bmzZuNgoIC4/PPPzeSkpKMTp06GadOnaoz55ZwrS+rqakxbrzxRmPOnDlXHGtJ1/r8+fPGnj17jD179hiSjN/85jfGnj17vHd//OpXvzKioqKMjz76yPjb3/5mjBkzxujRo4fxr3/9yxvjhz/8ofG73/3O+7qxn5Fg511VVWU88MADxg033GDs3bvX53u+srKy3rwb+34Ldt7nz583nn76aSMvL88oKCgwNm3aZNx5551Gr169jAsXLtSbt9XX+7LS0lLj2muvNZYtW1ZnDCuuN+yrVRUehmEYv/vd74wbb7zRCA8PNwYPHmxs377de2z48OHGlClTfNq///77xi233GKEh4cbt912m7Fhw4aQ5iupzu3tt9+uN+/Zs2d7P2NMTIxx3333Gbt37w5p3uPHjze6dOlihIeHG127djXGjx9vHD58uN6cDcP6a33ZJ598YkgyDh48eMWxlnStP/vsszq/Ny7nV1tbazz//PNGTEyMERERYYwYMeKKz9StWzdj4cKFPvsa+hkJdt4FBQX1fs9/9tln9ebd2PdbsPP++uuvjZEjRxrR0dFGu3btjG7duhnTp0+/ooBoadf7sjfeeMO45pprjJKSkjpjWHG9YV8OwzCMoHapAAAA/FurmeMBAABaPgoPAAAQMhQeAAAgZCg8AABAyFB4AACAkKHwAAAAIUPhAQAAQobCAwAAhAyFBwAACBkKDwAAEDIUHgAAIGQoPAAAQMj8/+iwzBBrY9l9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_1_fig=net.attention_1.trainable_part.cpu().detach().numpy()\n",
    "plt.imshow(A_1_fig, cmap='hot')  # 'hot' is a popular colormap for heatmaps\n",
    "plt.colorbar()  # Show color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f70b45ff730>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_1_fig=net.attention_1.non_trainable_part_1.cpu().detach().numpy()\n",
    "plt.imshow(A_1_fig, cmap='hot')  # 'hot' is a popular colormap for heatmaps\n",
    "plt.colorbar()  # Show color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f70b454b250>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_1_fig=net.attention_1.non_trainable_part_2.cpu().detach().numpy()\n",
    "plt.imshow(A_1_fig, cmap='hot')  # 'hot' is a popular colormap for heatmaps\n",
    "plt.colorbar()  # Show color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f70b44237c0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_2_fig=net.attention_2.trainable_part.cpu().detach().numpy()\n",
    "plt.imshow(A_2_fig, cmap='hot')  # 'hot' is a popular colormap for heatmaps\n",
    "plt.colorbar()  # Show color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(20):\n",
    "#     loss=np.empty(X_train_tilde.shape[0])\n",
    "#     for i in range(X_train_tilde.shape[0]):\n",
    "#         temp=net(X_train_tilde[i])[0][-1,:].float()+1e-6\n",
    "#         loss[i]=-torch.matmul(Target_train[i],torch.log(temp))\n",
    "\n",
    "#     Loss=sum(loss)/iter\n",
    "#     print(Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional tensor, but got 2-dimensional tensor for argument #1 'batch1' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pz/p8qzlrz11ndgpdncscl39lg80000gn/T/ipykernel_95419/1398683277.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tilde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tilde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tilde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTarget_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pz/p8qzlrz11ndgpdncscl39lg80000gn/T/ipykernel_95419/1947552867.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_layer_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mcontext_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;31m# print('h_0 shape: ',X.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pz/p8qzlrz11ndgpdncscl39lg80000gn/T/ipykernel_95419/1947552867.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h_0, attn_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# print(total_matrix_A_1.shape, h_0.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_matrix_A_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional tensor, but got 2-dimensional tensor for argument #1 'batch1' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "loss=np.empty(X_train_tilde.shape[0])\n",
    "for i in range(X_train_tilde.shape[0]):\n",
    "    temp=net(X_train_tilde[i])[0][-1,:].float()+1e-6\n",
    "    loss[i]=-torch.matmul(Target_train[i],torch.log(temp))\n",
    "\n",
    "Loss=sum(loss)/iter\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
